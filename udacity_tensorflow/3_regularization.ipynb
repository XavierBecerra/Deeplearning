{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 28, 28) (438047,)\n",
      "Validation set (15586, 28, 28) (15586,)\n",
      "Test set (13645, 28, 28) (13645,)\n"
     ]
    }
   ],
   "source": [
    "#pickle_file = 'notMNIST.pickle'\n",
    "pickle_file = 'notMNIST_noDupNorOvlp.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 784) (438047, 10)\n",
      "Validation set (15586, 784) (15586, 10)\n",
      "Test set (13645, 784) (13645, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1: \n",
    "Logistic regression with SGD\n",
    "\n",
    "Set-up Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "_BETA_REGUL = 5e-4 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  #The question here is: Do we also want to take into account biases fro regularization?\n",
    "  #\"applying weight decay to the bias units usually makes only a small difference to the final network\"\n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regularization = (tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases) )\n",
    "  #l2_regularization = tf.nn.l2_loss(weights)\n",
    "  loss += _BETA_REGUL * l2_regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.104467\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 15.0%\n",
      "Minibatch loss at step 500: 2.347745\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 1000: 1.452825\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 1500: 1.589714\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 2000: 1.175524\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 2500: 0.860617\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 3000: 0.580042\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 78.3%\n",
      "Test accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to assignment 2 (Logistic regression SGD at iteration 3000:\n",
    "\n",
    "*Ass 2 no regu: Minibatch loss at step 3000: 0.578054\n",
    "Minibatch accuracy: 86.7%\n",
    "Validation accuracy: 76.1%\n",
    "Test accuracy: 83.6%*\n",
    "\n",
    "*Last L2 Regul only weights beta 5e-4: Minibatch loss at step 3000: 0.580042\n",
    "Minibatch accuracy: 89.1%\n",
    "Validation accuracy: 78.3%\n",
    "Test accuracy: 85.7%*\n",
    "\n",
    "\n",
    "\n",
    "##### Model 2:  \n",
    "1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "hidden1_nodes = 1024\n",
    "\n",
    "_BETA_REGUL = 5e-4 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h1': tf.Variable( tf.truncated_normal([image_size * image_size, hidden1_nodes]) ),\n",
    "    'out': tf.Variable( tf.truncated_normal([hidden1_nodes, num_labels]) )\n",
    "  }\n",
    "  biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_nodes])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "  \n",
    "  # Create neural network model:\n",
    "  # Hidden fully connected layer with 256 neurons.\n",
    "  layer_1 = tf.add(tf.matmul(tf_train_dataset, weights['h1']) , biases['b1'])\n",
    "  #Output layer applying relu to hiden layer\n",
    "  logits_out = tf.matmul( tf.nn.relu(layer_1), weights['out']) + biases['out']\n",
    "  #Define loss\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_out))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regularization = (tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(biases['b1'])\n",
    "                      + tf.nn.l2_loss(weights['out']) + tf.nn.l2_loss(biases['out']))\n",
    "  #l2_regularization = tf.nn.l2_loss(weights)\n",
    "  loss += _BETA_REGUL * l2_regularization\n",
    "\n",
    "    ##Needed to evaluate test and validation datasets\n",
    "  test_layer_1 = tf.matmul(tf_test_dataset, weights['h1']) + biases['b1']\n",
    "  test_logits_out = tf.matmul( tf.nn.relu(test_layer_1), weights['out']) + biases['out']\n",
    "    \n",
    "  valid_layer_1 = tf.add(tf.matmul(tf_valid_dataset, weights['h1']) , biases['b1'])\n",
    "  valid_logits_out = tf.matmul( tf.nn.relu(valid_layer_1), weights['out']) + biases['out']\n",
    "  \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_out)\n",
    "  valid_prediction = tf.nn.softmax(valid_logits_out)\n",
    "  test_prediction = tf.nn.softmax(test_logits_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Run the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 535.250305\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 25.0%\n",
      "Minibatch loss at step 500: 128.788483\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 1000: 95.687538\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1500: 73.943321\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 2000: 58.023598\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.3%\n",
      "Test accuracy: 85.6%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff25b722fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    losses.append(l)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(range(0, num_steps), losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to assignment 2 (Logistic regression SGD at iteration 3000:\n",
    "\n",
    "*Ass 2 NN: Minibatch loss at step 5000: 4.587700\n",
    "Minibatch accuracy: 78.1%\n",
    "Validation accuracy: 79.6%\n",
    "Test accuracy: 86.6%*\n",
    "\n",
    "*Last L2 Regul only weights and bias beta 5e-4: Minibatch loss at step 5000: 12.971046\n",
    "Minibatch accuracy: 82.0%\n",
    "Validation accuracy: 84.4%\n",
    "Test accuracy: 90.5%\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a few batches we will try by only running a small subset of batches. So batch size is the same, but we will only train with a small subset of batches (no going through the whole training dataset.\n",
    "\n",
    "We will implement this by making the offset a random number between 0 and 10. Only first 10 batches on the data training set are used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s run the model Using the Setup of previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 527.359375\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 36.1%\n",
      "Minibatch loss at step 500: 127.361511\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 1000: 95.485588\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1500: 74.362022\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 2000: 57.911427\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 2500: 45.099991\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 3000: 35.122944\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 3500: 27.352951\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 4000: 21.302094\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 4500: 16.589764\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 5000: 12.920185\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Test accuracy: 83.3%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAEKCAYAAAArTFFlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAG51JREFUeJzt3XmwnHWd7/H356xJTnYSAyLxBA1a4DgRI+K4XGecYRtLxKnrDXdGELlGR7wjd6yZAr2lXudShSsu46CoKLiAKKBcRDFSuI2yJIgskZAFUhBDEpKQ/Zyc5Xv/eH6dPEnOOekk3f07J/15VXX1079++ulv6OTDs3T/vooIzMwarSV3AWbWnBw+ZpaFw8fMsnD4mFkWDh8zy8LhY2ZZOHzMLAuHj5ll4fAxsyzachdQLzNmzIju7u7cZZg1nSVLljwbETMPtt5RGz7d3d0sXrw4dxlmTUfS6mrW82GXmWXh8DGzLBw+ZpaFw8fMsnD4mFkWDh8zy8LhY2ZZNHX43L1sPU9v3pm7DLOmVLfwkXSCpLslLZX0qKQPpPHpkhZJWp7up6VxSfqCpBWSHpJ0amlbF6b1l0u6sFY1XvSN+znjql/VanNmdgjquefTD3wwIk4GTgcukXQycBlwV0TMBe5KjwHOBuam20LgaijCCvgo8GrgNOCjlcCqhZ27B2q1KTM7BHULn4hYGxEPpOVtwB+B44FzgevSatcBb03L5wLXR+EeYKqk44AzgUURsSkiNgOLgLPqVbeZNUZDzvlI6gZeAdwLzIqItempZ4BZafl44KnSy55OY8OND/U+CyUtlrR4w4YNNavfzGqv7uEjaSJwM3BpRGwtPxdF07CaNQ6LiGsiYn5EzJ8586A/qjWzjOoaPpLaKYLnOxFxSxpelw6nSPfr0/ga4ITSy1+QxoYbN7MxrJ5XuwR8HfhjRHy29NRtQOWK1YXAj0rjF6SrXqcDW9Lh2Z3AGZKmpRPNZ6QxMxvD6jmfz2uBdwAPS3owjX0IuBK4SdLFwGrg7em5O4BzgBXATuAigIjYJOnfgPvTeh+PiE11rNvMGqBu4RMRvwE0zNNvGmL9AC4ZZlvXAtfWrjozy62pv+FsZvk4fMwsC4ePmWXh8DGzLBw+ZpaFw8fMsnD4mFkWDh8zy8LhY2ZZOHzMLAuHj5ll4fAxsywcPmaWhcPHzLJw+JhZFvWcyfBaSeslPVIa+56kB9PtycokY5K6Je0qPffl0mteKenh1M/rC2mGRDMb4+o5k+E3gX8Hrq8MRMR/qyxL+gywpbT+yoiYN8R2rgbeTdH54g6Ktjk/qUO9ZtZA9ezb9StgyOlO097L24EbRtpGmmB+ckTck2Y6vJ69fb7MbAzLdc7n9cC6iFheGpsj6feSfinp9WnseIo+XRXD9uwys7GlnoddIzmfffd61gKzI2KjpFcCP5R0yqFuVNJCilbLzJ49uyaFmll9NHzPR1Ib8Dbge5WxiOiNiI1peQmwEjiJoj/XC0ovH7Fnl5sGmo0dOQ67/hp4LCL2HE5JmimpNS2fCMwFVqW+XVslnZ7OE13A3j5fZjaG1fNS+w3A74CXSHo69ekCWMCBJ5rfADyULr3/AHhvqTfX+4CvUfTzWomvdJkdFerZt+v8YcbfOcTYzRRtlYdafzHwspoWZ2bZ+RvOZpaFw8fMsnD4mFkWDh8zy8LhY2ZZOHzMLAuHj5ll4fAxsywcPmaWhcPHzLJw+JhZFg4fM8vC4WNmWTh8zCwLh4+ZZeHwMbMsGt008GOS1pSaA55Teu7y1BhwmaQzS+NnpbEVki6rV71m1lj13PP5JkWDv/1dFRHz0u0OAEknU0yvekp6zX9Iak3zOn8JOBs4GTg/rWtmY1w9p1H9laTuKlc/F7gxInqBJyStAE5Lz62IiFUAkm5M6y6tcblm1mA5zvm8X9JD6bBsWho7HniqtE6lOeBw40OStFDSYkmLN2zYUOu6zayGGh0+VwMvAuZRNAr8TC037r5dZmNHQzuWRsS6yrKkrwK3p4drgBNKq5abAw43bmZjWEP3fCQdV3p4HlC5EnYbsEBSp6Q5FE0D7wPuB+ZKmiOpg+Kk9G2NrNnM6qNuez6paeAbgRmSngY+CrxR0jwggCeB9wBExKOSbqI4kdwPXBIRA2k77wfuBFqBayPi0XrVbGaN0+imgV8fYf0rgCuGGL8DuKOGpZnZKOBvOJtZFg4fM8vC4WNmWTh8zCwLh4+ZZeHwMbMsHD5mloXDx8yycPiYWRYOHzPLwuFjZlk4fMwsC4ePmWXh8DGzLBw+ZpZFo/t2fUrSY2kC+VslTU3j3ZJ2lfp5fbn0mldKejj17fqCJNWrZjNrnEb37VoEvCwiXg48Dlxeem5lqZ/Xe0vjVwPvpphade4Q2zSzMahu4RMRvwI27Tf2s4joTw/voZgQflhpzufJEXFPRARwPfDWetRrZo2V85zPu4CflB7PkfR7Sb+U9Po0djxFr66KEft2mdnY0dDWORWSPkwxUfx30tBaYHZEbJT0SuCHkk45jO0uBBYCzJ49u1blmlkdNHzPR9I7gTcDf58OpYiI3ojYmJaXACuBkyh6dJUPzUbs2+WmgWZjR6P7dp0F/CvwlojYWRqfKak1LZ9IcWJ5VUSsBbZKOj1d5boA+FEjazaz+mh0367LgU5gUbpifk+6svUG4OOS+oBB4L0RUTlZ/T6KK2fjKc4Rlc8TmdkYNSr6dkXEzcDNwzy3GHhZDUszs1HA33A2sywcPmaWhcPHzLJw+JhZFg4fM8vC4WNmWTh8zCwLh4+ZZVFV+Eh6kaTOtPxGSf9UmQjMzOxwVLvnczMwIOnFwDXACcB361ZVA6TftJpZJtWGz2CaBOw84IsR8S/AcfUry8yOdtWGT5+k84ELgdvTWHt9SjKzZlBt+FwEvAa4IiKekDQH+Fb9yjKzo11Vv2qPiKXAPwFImgZMiohP1LMwMzu6VXu16xeSJkuaDjwAfFXSZ+tbmpkdzao97JoSEVuBtwHXR8Srgb+uX1lmdrSrNnzaUhubt7P3hPNBDdM4cLqkRZKWp/tpaVypKeCK1FTw1NJrLkzrL5d0YbXvPxJfaTfLq9rw+ThwJ0Vjv/vTPMvLq3jdNzmwyd9lwF0RMRe4Kz0GOJu9jQEXUjQLJB3qfRR4NXAa8NFKYJnZ2FVV+ETE9yPi5RHxj+nxqoj4uyped0DjQOBc4Lq0fB17mwCeS3FIFxFxDzA17W2dCSyKiE0RsZmi66m7lpqNcdWecH5B6q2+Pt1uljRit9ERzEpdKQCeAWal5eOBp0rrVRoEDjc+VJ0LJS2WtHjDhg2HWZ6ZNUK1h13fAG4Dnp9u/y+NHZHUt6tmZ1/ct8ts7Kg2fGZGxDcioj/dvgkc7r/udelwqtKLfX0aX0Pxm7GKSoPA4cbNbAyrNnw2SvoHSa3p9g/AxsN8z9sofqZBuv9RafyCdNXrdGBLOjy7EzhD0rR0ovmMNGZmY1i1fbveBXwRuIriMOm3wDsP9qJhGgdeCdwk6WJgNcXle4A7gHOAFcBOip90EBGbJP0bcH9a7+OlhoJmNkZV+/OK1cBbymOSLgU+d5DXDdU4EOBNQ6wbwCXDbOda4Npqaq2Wv+ZjlteRzGT4zzWrwsyazpGEj2pWhZk1nSMJHx+5mNlhG/Gcj6RtDB0yAsbXpSIzawojhk9ETGpUIWbWXNw6x8yyaNrwcfcKs7yaNnzMLC+Hj5ll4fAxsywcPmaWhcPHzLJw+JhZFk0bPr7QbpZX04aPmeXV8PCR9BJJD5ZuWyVdKuljktaUxs8pveby1M9rmaQzG12zmdVetTMZ1kxELAPmAUhqpZiP+VaKmQuviohPl9eXdDKwADiFYvL6n0s6KSIGGlq4mdVU7sOuN1E0Ilw9wjrnAjdGRG9EPEExzeppDanOzOomd/gsAG4oPX5/apV8bakradV9u8xs7MgWPpI6KOaF/n4auhp4EcUh2VrgM4exTTcNNBsjcu75nA08EBHrACJiXUQMRMQg8FX2HlpV3bfrUJoG+kftZnnlDJ/zKR1yVRoJJucBj6Tl24AFkjolzQHmAvc1rEozq4uGX+0CkNQF/A3wntLwJyXNo/j+35OV5yLiUUk3AUuBfuASX+kyG/uyhE9E7ACO2W/sHSOsfwVwRb3rMrPGyX21y8yalMPHzLJw+JhZFk0bPuHftZtl1bThY2Z5OXzMLAuHj5ll4fAxsywcPmaWhcPHzLJo2vDxr9rN8mra8DGzvBw+ZpaFw8fMsnD4mFkWDh8zyyLnBPJPSno4NQhcnMamS1okaXm6n5bGJekLqXHgQ5JOzVW3mdVG7j2fv4yIeRExPz2+DLgrIuYCd6XHUEw2PzfdFlJ0ujgi7a3FH/2kWROPdFNmdhhyh8/+zgWuS8vXAW8tjV8fhXuAqftNOH/IWlvES4+dxJwZXUeyGTM7TDnDJ4CfSVoiaWEamxURa9PyM8CstFxV48DD6dvlLxua5ZFlAvnkdRGxRtLzgEWSHis/GREh6ZCiISKuAa4BmD9/vmPFbBTLtucTEWvS/XrgVoomgesqh1Ppfn1averGgWY2NmQJH0ldkiZVloEzKJoE3gZcmFa7EPhRWr4NuCBd9Tod2FI6PDOzMSjXYdcs4FZJlRq+GxE/lXQ/cJOki4HVwNvT+ncA5wArgJ3ARbUqxMdmZnnkahq4CvjzIcY3Am8aYjyAS2pdRwo/M8tgtF1qN7Mm0fTh40vtZnnkvNSe3bPbe2nxkZdZFk0dPhu29bJhW2/uMsyaUtMfdplZHg4fM8vC4WNmWTh8zCwLh4+ZZeHwMbMsHD5mloXDx8yycPiYWRYOHzPLwuEDhH9datZwDQ8fSSdIulvSUkmPSvpAGv+YpDWpj9eDks4pveby1LNrmaQza13Tf67YWOtNmtlB5PhhaT/wwYh4IE2lukTSovTcVRHx6fLKkk4GFgCnAM8Hfi7ppIgYqFVB23v7arUpM6tSw/d8ImJtRDyQlrcBf2SINjgl5wI3RkRvRDxBMZXqabWs6bOLHq/l5sysClnP+UjqBl4B3JuG3p/aIV9baZVMlT27jsTj67bXcnNmVoWcvdonAjcDl0bEVooWyC8C5gFrgc8cxjYPuWmgmeWRq3VOO0XwfCcibgGIiHURMRARg8BX2XtoVXXProi4JiLmR8T8mTNn1u8PYGZHLMfVLgFfB/4YEZ8tjZd7r59H0ccLip5dCyR1SpoDzAXua1S9ZlYfOa52vRZ4B/CwpAfT2IeA8yXNo2il9STwHoCIeFTSTcBSiitll9TySpeZ5dHw8ImI3wBDTdt+xwivuQK4om5FAb39A3S2tdbzLcysxN9wTnb2emfKrJEcPom/62PWWE0dPsd0dexZ/tY9q/npI89krMasuTR1+Hx+wSv2efzeby+h+7If846v30v3ZT9m9cYd+zy/bmsPg4P+EapZLTR108BXnzh9yPFfL38WgP/yqV8M+fy9H3oTMyd2AtDilqdmh0VH63QS8+fPj8WLF1e9/raePn63ciMLv7XksN7vWxefxtznTeJ5kzodSNbUJC2JiPkHXc/hM7yIYOWG7fzHL1ZyywNDfqn6oM75s2NZ+IYXMe+EqUdUi9lY4fCpQfgMJyJ44tkd/HHtNi757gOH/Pq3nXo8tzywhnf+RTeXn/NSf7/IjioOnzqGz0h6+gZ46Okt3Pr7p7nhvqcO/oJh3PDu03nF7KmMa3cw2dji8MkUPiPpHxjkD08/x9aefi76xv1VvWZ6VwfHTh7H86eO49gp4zhuyniOnTyO46bsfTy+wwFlo0e14dPUV7sara21hVe+sLjC9uSVf3vA8z19Azy8Zgtf+/UqXvfiGWzZ1cfaLT08s6WHNc/1sGT1ZjbvPHDWxakT2jl28jhmTurce5t44PKU8e0Uv+s1y8/hM4qMa2/lVd3TeVX30F8BgCKgntnSU4TS1l386bmePY83bO9l1YYdbNjWy+6BwQNe29HawoyJHfuE1DFdnUzv6mB6VwfTujqYPqGDaV3tTO/qYHx7q8PK6sbhM8aMa2+le0YX3TO6hl0nItja08+Gbb3FbXvv3uX0eM1zPTz41BY27ehluO9Ndra1FKE0oRxO7Uzr6uCYrg6mTuhg8vh2pqTb5HFtTB7fTntrU3931ark8DkKSdoTCC9+3sQR1x0cDLb29LFpx24279zNph19bN6xm007d7N5x2427ti95/HTm3eyacdutvb0j7jNCR2tKYzamTy+rbTcviesJo8rxieNa2diZxtdna1M7Gxj4rg273E1CYdPk2tpEVMnFHsx1eobGOS5nX1s3rmbrbv62NrTx5ZdfWzd1Z/u0+M0/qfnenisZxtbdvWx7SDBBdAi6Opoo6sSSuPamdjZSldH256A6uoslrs69j4/rr2VCR1FeI3vKG4T0nJnW4sDbZRx+Ngha29t2XPO6FANDAbbe/r3BNO2nn629/azo7e4Ly/vHRtgR28/z27bWYzv7md7Tz/9h/A7O4kilCrB1N7KhI5KYFXG2hjf0cKEjjbGtbcyrr2FzrYiuDrbWuhsLy23tdLZXlpua0mP967jsBvZmAkfSWcBnwdaga9FxJWZS7LD0NoipkxoZ8qE9n0m5j5UEUFv/yA7evvZ0TvA9t5+dvX1s2v3ILv6Bti5u5+evgF27h5gV98Au3YXt519A/Skscpzz27fvXedPa898IT9oepoGzmcOtpaaG9tob1V6X745Y62Ftpa0uO2FjrSc22te5eLx6Jjz+tL22hrob1FtLaIttZiW22toq2lhdZMPwcaE+EjqRX4EvA3FK1z7pd0W0QszVuZ5SIp7Z20cszIp7UOy+BgsHtgkN6+QXr7B+jtL+57+gb3LPf27//8IL19xfLu/pHX6+krArNvYJD+geK9+gYG6esP+geL1/cNRPF8nWdSkKAtBVN7SwutKZT2BlTx3L+c+VLOetmxNXvfMRE+FJ0sVkTEKgBJN1I0E3T4WF20tIhxLa3pG+btWWuJiD1BVNzKy/s+3t1fCay9y+WAGxgM+geD/hRq/QPBwOAgfYNRPDdQhF95ncr45PG1jYuxEj5DNQ589f4rSVoILASYPXt2YyozqzNJdLSJjraj6ysMR9Wfxn27zMaOsRI+VTcONLOxYayEz/3AXElzJHUACyiaCZrZGDUmzvlERL+k9wN3UlxqvzYiHs1clpkdgTERPgARcQcjNBY0s7FlrBx2mdlRxuFjZlk4fMwsi6N2GlVJG4DVVaw6A3i2zuUcidFeH4z+Gkd7fXB01fjCiDjoF+2O2vCplqTF1cw3m8torw9Gf42jvT5ozhp92GVmWTh8zCwLhw9ck7uAgxjt9cHor3G01wdNWGPTn/Mxszy852NmWTRt+Eg6S9IySSskXdbg975W0npJj5TGpktaJGl5up+WxiXpC6nOhySdWnrNhWn95ZIurGF9J0i6W9JSSY9K+sAorHGcpPsk/SHV+H/S+BxJ96Zavpd+iIykzvR4RXq+u7Sty9P4Mkln1qrGtO1WSb+XdPsore9JSQ9LelDS4jTWmM85IpruRvHj1JXAiUAH8Afg5Aa+/xuAU4FHSmOfBC5Ly5cBn0jL5wA/AQScDtybxqcDq9L9tLQ8rUb1HQecmpYnAY8DJ4+yGgVMTMvtwL3pvW8CFqTxLwP/mJbfB3w5LS8AvpeWT06ffycwJ/29aK3hZ/3PwHeB29Pj0Vbfk8CM/cYa8jk39B/9aLkBrwHuLD2+HLi8wTV07xc+y4Dj0vJxwLK0/BXg/P3XA84HvlIa32e9Gtf6I4r5s0dljcAE4AGK2S2fBdr2/5wpZkR4TVpuS+tp/8++vF4N6noBcBfwV8Dt6f1GTX1pe0OFT0M+52Y97BpqWtbjM9VSMSsi1qblZ4BZaXm4WhvyZ0i7/6+g2LMYVTWmQ5oHgfXAIoq9guciotIcrPx+e2pJz28BjqlzjZ8D/hWotMI4ZpTVBxDAzyQtSdMQQ4M+5zEzpUYziYiQlP0ypKSJwM3ApRGxVaU+VKOhxogYAOZJmgrcCrw0Zz1lkt4MrI+IJZLemLueEbwuItZIeh6wSNJj5Sfr+Tk3657PaJyWdZ2k4wDS/fo0Plytdf0zSGqnCJ7vRMQto7HGioh4Drib4jBmqqTK/1TL77enlvT8FGBjHWt8LfAWSU8CN1Icen1+FNUHQESsSffrKQL8NBr1Odf6+Hss3Cj2+FZRnMCrnHA+pcE1dLPvOZ9Pse9Jvk+m5b9l35N896Xx6cATFCf4pqXl6TWqTcD1wOf2Gx9NNc4Epqbl8cCvgTcD32ffE7rvS8uXsO8J3ZvS8inse0J3FTU8oZve443sPeE8auoDuoBJpeXfAmc16nNu2D+20XajOHP/OMV5gg83+L1vANYCfRTHxxdTHN/fBSwHfl758NIH/aVU58PA/NJ23gWsSLeLaljf6yjOBTwEPJhu54yyGl8O/D7V+AjwkTR+InBfer/vA51pfFx6vCI9f2JpWx9OtS8Dzq7D510On1FTX6rlD+n2aOXfQaM+Z3/D2cyyaNZzPmaWmcPHzLJw+JhZFg4fM8vC4WNmWTh8DABJx6RfNj8o6RlJa0qPO6rcxjckveQg61wi6e9rU/WQ23+bpFHzTWcbni+12wEkfQzYHhGf3m9cFH9nBod84Sgg6dvADyLih7lrsZF5z8dGJOnFaV6f71B8Ee04SddIWpzm0flIad3fSJonqU3Sc5KuTPPt/C79dghJ/1fSpaX1r0zz8iyT9BdpvEvSzel9f5Dea94QtX0qrfOQpE9Iej3FlyGvSnts3ZLmSroz/XDyV5JOSq/9tqSr0/jjks5O438m6f70+ocknVjv/8bNyj8stWq8FLggIiqTTV0WEZvSb5DulvSDiFi632umAL+MiMskfZbiG7BXDrFtRcRpkt4CfITi6/3/E3gmIv5O0p9TTJex74ukWRRBc0pEhKSpEfGcpDso7flIuhv4HxGxUtJrgX8HzkibOQF4FTAX+LmkF1PMq/PpiPiepE6Kb/VaHTh8rBorK8GTnC/pYoq/P8+nmPBq//DZFRE/SctLgNcPs+1bSut0p+XXAZ8AiIg/SHp0iNdtopiq4quSfkwxX84+0q/dTwduLv0iv/x3/qZ0CLlM0lMUIfRb4H9LeiFwS0SsGKZuO0I+7LJq7KgsSJoLfAD4q4h4OfBTit8l7W93aXmA4f9H11vFOgeIiD5gPvBD4K3Aj4dYTcCzETGvdHtZeTMHbja+BZyX6vqppDdUW5MdGoePHarJwDZga5puoaZzCif/CbwdinMwFHtW+5A0CZgcEbcD/4tiwjNSbZMAImIzsFbSeek1LekwruK/qnASxSHYckknRsSKiPg8xd7Uy+vw5zN82GWH7gGKQ6zHgNUUQVFrXwSul7Q0vddSipn9yqYAt6TzMi0UcyVDMWPAVyR9kGKPaAFwdbqC1wF8m+JX3FDMObMYmAgsjIjdkv67pPMpZhz4E/CxOvz5DF9qt1Eonchui4iedJj3M2Bu7J1+tBbv4UvymXnPx0ajicBdKYQEvKeWwWOjg/d8zCwLn3A2sywcPmaWhcPHzLJw+JhZFg4fM8vC4WNmWfx/hJE9f2T5OgQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34ea38c3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_steps = 5001\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the first 10 subsetes of 128 entries in dataset.\n",
    "    offset = batch_size * np.random.choice(10)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    losses.append(l)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(range(0, num_steps), losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, The Minibatch accuracy is 100% (Actually this is like training with a smaller dataset).\n",
    "However, Validation and test accuracy are lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nn.dropout is only applied in train_layer_1. \n",
    "\n",
    "We will apply a probability of 0.5 to keep/dropout an activation (values that go from one layer to the next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "hidden1_nodes = 1024\n",
    "\n",
    "_BETA_REGUL = 5e-4 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h1': tf.Variable( tf.truncated_normal([image_size * image_size, hidden1_nodes]) ),\n",
    "    'out': tf.Variable( tf.truncated_normal([hidden1_nodes, num_labels]) )\n",
    "  }\n",
    "  biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_nodes])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "  \n",
    "  # Create neural network model:\n",
    "  # Hidden fully connected layer with 256 neurons.\n",
    "  train_layer_1 = tf.add(tf.matmul(tf_train_dataset, weights['h1']) , biases['b1'])\n",
    "  train_layer_1 = tf.nn.dropout(train_layer_1, 0.5)\n",
    "  #Output layer applying relu to hiden layer\n",
    "  logits_out = tf.matmul( tf.nn.relu(train_layer_1), weights['out']) + biases['out']\n",
    "  #Define loss\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_out))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regularization = (tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(biases['b1'])\n",
    "                      + tf.nn.l2_loss(weights['out']) + tf.nn.l2_loss(biases['out']))\n",
    "  #l2_regularization = tf.nn.l2_loss(weights)\n",
    "  loss += _BETA_REGUL * l2_regularization\n",
    "\n",
    "    ##Needed to evaluate test and validation datasets\n",
    "  test_layer_1 = tf.matmul(tf_test_dataset, weights['h1']) + biases['b1']\n",
    "  test_logits_out = tf.matmul( tf.nn.relu(test_layer_1), weights['out']) + biases['out']\n",
    "    \n",
    "  valid_layer_1 = tf.add(tf.matmul(tf_valid_dataset, weights['h1']) , biases['b1'])\n",
    "  valid_logits_out = tf.matmul( tf.nn.relu(valid_layer_1), weights['out']) + biases['out']\n",
    "  \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_out)\n",
    "  valid_prediction = tf.nn.softmax(valid_logits_out)\n",
    "  test_prediction = tf.nn.softmax(test_logits_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 647.199707\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 30.8%\n",
      "Minibatch loss at step 500: 141.209442\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1000: 98.687836\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 1500: 78.025322\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 2000: 62.269276\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 2500: 45.906364\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 3000: 34.820324\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 3500: 27.925144\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 4000: 21.562328\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 4500: 16.728735\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 5000: 13.389703\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.4%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3487d42350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    losses.append(l)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(range(0, num_steps), losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout technique seems to improve the model. Test accuray is now aroun 89% when for Problem 1 was around 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Set Up the Model\n",
    "###### First Approach \n",
    "Multilayer Neural Network with 4 hidden layers of same number of neurons each. \n",
    "\n",
    "Weights initialized with deviation square root of (2 / input activations to layer)\n",
    "\n",
    "RELU function for each layer activations.\n",
    "\n",
    "Dropout technique at each layer activations. Only during training.\n",
    "\n",
    "L2 Regulariation including also biases.\n",
    "\n",
    "Dynamic learning rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "num_steps = 5001\n",
    "\n",
    "hidden_layer_nodes = {\n",
    "    'h1' : 1024,\n",
    "    'h2' : 1024,\n",
    "    'h3' : 512,\n",
    "    'h4' : 512 \n",
    "}\n",
    "\n",
    "_BETA_REGUL = 5e-4 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h1': tf.Variable( tf.truncated_normal([image_size * image_size, hidden_layer_nodes['h1'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / (image_size * image_size))) ),\n",
    "    'h2': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h1'], hidden_layer_nodes['h2'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / hidden_layer_nodes['h1'] )) ),\n",
    "    'h3': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h2'], hidden_layer_nodes['h3'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / hidden_layer_nodes['h2'] )) ),\n",
    "    'h4': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h3'], hidden_layer_nodes['h4'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / hidden_layer_nodes['h3'] )) ),\n",
    "    'out': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h4'], num_labels] , \n",
    "                                            stddev= np.sqrt(2.0 / hidden_layer_nodes['h4'] )) ),\n",
    "  }\n",
    "  biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden_layer_nodes['h1'] ])),\n",
    "    'b2': tf.Variable(tf.zeros([hidden_layer_nodes['h2'] ])),\n",
    "    'b3': tf.Variable(tf.zeros([hidden_layer_nodes['h3'] ])),\n",
    "    'b4': tf.Variable(tf.zeros([hidden_layer_nodes['h4'] ])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "  \n",
    "  # Create neural network model:\n",
    "  train_layer_1 = tf.add(tf.matmul(tf_train_dataset, weights['h1']) , biases['b1'])\n",
    "  train_layer_1 = tf.nn.dropout(tf.nn.relu(train_layer_1), 0.5)\n",
    "    \n",
    "  train_layer_2 = tf.add(tf.matmul(train_layer_1, weights['h2']) , biases['b2'])\n",
    "  train_layer_2 = tf.nn.dropout(tf.nn.relu(train_layer_2), 0.5)\n",
    "    \n",
    "  train_layer_3 = tf.add(tf.matmul(train_layer_2, weights['h3']) , biases['b3'])\n",
    "  train_layer_3 = tf.nn.dropout(tf.nn.relu(train_layer_3), 0.5)\n",
    "    \n",
    "  train_layer_4 = tf.add(tf.matmul(train_layer_3, weights['h4']) , biases['b4'])\n",
    "  train_layer_4 = tf.nn.dropout(tf.nn.relu(train_layer_4), 0.5)\n",
    "    \n",
    "  logits_out = tf.add(tf.matmul( train_layer_4, weights['out']) , biases['out'])\n",
    "  #Define loss\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_out))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regul = (tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(biases['b1'])\n",
    "                       + tf.nn.l2_loss(weights['h2']) + tf.nn.l2_loss(biases['b2'])\n",
    "                       + tf.nn.l2_loss(weights['h3']) + tf.nn.l2_loss(biases['b3'])\n",
    "                       + tf.nn.l2_loss(weights['h4']) + tf.nn.l2_loss(biases['b4'])\n",
    "                       + tf.nn.l2_loss(weights['out']) + tf.nn.l2_loss(biases['out']) )\n",
    "    \n",
    "  loss += _BETA_REGUL * l2_regul\n",
    "  \n",
    "  # Optimizer. Exponential decay of learning rate\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, # Base learning rate. Start big to learn fast at the beginning\n",
    "                                             global_step * batch_size,\n",
    "                                             tf_train_labels.shape[0],# Decay step.\n",
    "                                             0.97,                # Decay rate.\n",
    "                                             staircase=True)\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    "  \n",
    "\n",
    "  ##Needed to evaluate test and validation datasets\n",
    "  test_layer_1 = tf.add(tf.matmul(tf_test_dataset, weights['h1']) , biases['b1'])\n",
    "  test_layer_2 = tf.add(tf.matmul(tf.nn.relu(test_layer_1), weights['h2']) , biases['b2'])\n",
    "  test_layer_3 = tf.add(tf.matmul(tf.nn.relu(test_layer_2), weights['h3']) , biases['b3'])\n",
    "  test_layer_4 = tf.add(tf.matmul(tf.nn.relu(test_layer_3), weights['h4']) , biases['b4'])\n",
    "  test_logits_out = tf.matmul( tf.nn.relu(test_layer_4), weights['out']) + biases['out']\n",
    "    \n",
    "  valid_layer_1 = tf.add(tf.matmul(tf_valid_dataset, weights['h1']) , biases['b1'])\n",
    "  valid_layer_2 = tf.add(tf.matmul(tf.nn.relu(valid_layer_1), weights['h2']) , biases['b2'])\n",
    "  valid_layer_3 = tf.add(tf.matmul(tf.nn.relu(valid_layer_2), weights['h3']) , biases['b3'])\n",
    "  valid_layer_4 = tf.add(tf.matmul(tf.nn.relu(valid_layer_3), weights['h4']) , biases['b4'])\n",
    "  valid_logits_out = tf.matmul( tf.nn.relu(valid_layer_4), weights['out']) + biases['out']\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_out)\n",
    "  valid_prediction = tf.nn.softmax(valid_logits_out)\n",
    "  test_prediction = tf.nn.softmax(test_logits_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s run the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Iteration 0 ... step 1 loss 4.23354959488\n",
      "Minibatch loss at step 0: 4.233550\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Iteration 1 ... step 2 loss 6.9909029007\n",
      "Iteration 2 ... step 3 loss 5.19958209991\n",
      "Iteration 3 ... step 4 loss 4.08319854736\n",
      "Iteration 4 ... step 5 loss 3.49902033806\n",
      "Iteration 5 ... step 6 loss 3.52801561356\n",
      "Iteration 6 ... step 7 loss 3.55355930328\n",
      "Iteration 7 ... step 8 loss 3.47751283646\n",
      "Iteration 8 ... step 9 loss 3.42500209808\n",
      "Iteration 9 ... step 10 loss 3.38857245445\n",
      "Iteration 10 ... step 11 loss 3.44027280807\n",
      "Iteration 11 ... step 12 loss 3.41366839409\n",
      "Iteration 12 ... step 13 loss 3.35109472275\n",
      "Iteration 13 ... step 14 loss 3.27803468704\n",
      "Iteration 14 ... step 15 loss 3.33655548096\n",
      "Iteration 15 ... step 16 loss 3.31098842621\n",
      "Iteration 16 ... step 17 loss 3.23417854309\n",
      "Iteration 17 ... step 18 loss 3.12282323837\n",
      "Iteration 18 ... step 19 loss 3.10642838478\n",
      "Iteration 19 ... step 20 loss 3.05146408081\n",
      "Iteration 20 ... step 21 loss 3.0530629158\n",
      "Iteration 21 ... step 22 loss 2.93320441246\n",
      "Iteration 22 ... step 23 loss 2.96480369568\n",
      "Iteration 23 ... step 24 loss 2.98210859299\n",
      "Iteration 24 ... step 25 loss 2.83481383324\n",
      "Iteration 25 ... step 26 loss 2.79028367996\n",
      "Iteration 26 ... step 27 loss 2.88848614693\n",
      "Iteration 27 ... step 28 loss 2.64074254036\n",
      "Iteration 28 ... step 29 loss 2.74150943756\n",
      "Iteration 29 ... step 30 loss 2.63621973991\n",
      "Iteration 30 ... step 31 loss 2.5629196167\n",
      "Iteration 31 ... step 32 loss 2.76138734818\n",
      "Iteration 32 ... step 33 loss 2.67335486412\n",
      "Iteration 33 ... step 34 loss 2.74199151993\n",
      "Iteration 34 ... step 35 loss 2.57437705994\n",
      "Iteration 35 ... step 36 loss 2.63097858429\n",
      "Iteration 36 ... step 37 loss 2.57575583458\n",
      "Iteration 37 ... step 38 loss 2.65930700302\n",
      "Iteration 38 ... step 39 loss 2.52720379829\n",
      "Iteration 39 ... step 40 loss 2.35301351547\n",
      "Iteration 40 ... step 41 loss 2.39043784142\n",
      "Iteration 41 ... step 42 loss 2.43045139313\n",
      "Iteration 42 ... step 43 loss 2.60478138924\n",
      "Iteration 43 ... step 44 loss 2.36357736588\n",
      "Iteration 44 ... step 45 loss 2.50036048889\n",
      "Iteration 45 ... step 46 loss 2.38936042786\n",
      "Iteration 46 ... step 47 loss 2.55973482132\n",
      "Iteration 47 ... step 48 loss 2.47110128403\n",
      "Iteration 48 ... step 49 loss 2.35046672821\n",
      "Iteration 49 ... step 50 loss 2.41313290596\n",
      "Iteration 50 ... step 51 loss 2.29348540306\n",
      "Iteration 51 ... step 52 loss 2.36718463898\n",
      "Iteration 52 ... step 53 loss 2.47357201576\n",
      "Iteration 53 ... step 54 loss 2.55782413483\n",
      "Iteration 54 ... step 55 loss 2.59760761261\n",
      "Iteration 55 ... step 56 loss 2.3390173912\n",
      "Iteration 56 ... step 57 loss 2.33666276932\n",
      "Iteration 57 ... step 58 loss 2.41154313087\n",
      "Iteration 58 ... step 59 loss 2.34315061569\n",
      "Iteration 59 ... step 60 loss 2.41759347916\n",
      "Iteration 60 ... step 61 loss 2.39373254776\n",
      "Iteration 61 ... step 62 loss 2.44657540321\n",
      "Iteration 62 ... step 63 loss 2.57153367996\n",
      "Iteration 63 ... step 64 loss 2.30379772186\n",
      "Iteration 64 ... step 65 loss 2.41728973389\n",
      "Iteration 65 ... step 66 loss 2.35660648346\n",
      "Iteration 66 ... step 67 loss 2.61279439926\n",
      "Iteration 67 ... step 68 loss 2.32166099548\n",
      "Iteration 68 ... step 69 loss 2.51800704002\n",
      "Iteration 69 ... step 70 loss 2.22583270073\n",
      "Iteration 70 ... step 71 loss 2.28274965286\n",
      "Iteration 71 ... step 72 loss 2.44831204414\n",
      "Iteration 72 ... step 73 loss 2.47580480576\n",
      "Iteration 73 ... step 74 loss 2.41549444199\n",
      "Iteration 74 ... step 75 loss 2.21073246002\n",
      "Iteration 75 ... step 76 loss 2.33384037018\n",
      "Iteration 76 ... step 77 loss 2.17850375175\n",
      "Iteration 77 ... step 78 loss 2.28791117668\n",
      "Iteration 78 ... step 79 loss 2.56405115128\n",
      "Iteration 79 ... step 80 loss 2.20124411583\n",
      "Iteration 80 ... step 81 loss 2.29641890526\n",
      "Iteration 81 ... step 82 loss 2.28735518456\n",
      "Iteration 82 ... step 83 loss 2.35194063187\n",
      "Iteration 83 ... step 84 loss 2.20204520226\n",
      "Iteration 84 ... step 85 loss 2.35973787308\n",
      "Iteration 85 ... step 86 loss 2.21633148193\n",
      "Iteration 86 ... step 87 loss 2.28081226349\n",
      "Iteration 87 ... step 88 loss 2.25173568726\n",
      "Iteration 88 ... step 89 loss 2.25720596313\n",
      "Iteration 89 ... step 90 loss 2.59264016151\n",
      "Iteration 90 ... step 91 loss 2.26786923409\n",
      "Iteration 91 ... step 92 loss 2.31795310974\n",
      "Iteration 92 ... step 93 loss 2.17731881142\n",
      "Iteration 93 ... step 94 loss 2.35530877113\n",
      "Iteration 94 ... step 95 loss 2.25771379471\n",
      "Iteration 95 ... step 96 loss 2.41752529144\n",
      "Iteration 96 ... step 97 loss 2.28654408455\n",
      "Iteration 97 ... step 98 loss 2.33489513397\n",
      "Iteration 98 ... step 99 loss 2.24781084061\n",
      "Iteration 99 ... step 100 loss 2.33944606781\n",
      "Iteration 100 ... step 101 loss 2.50447130203\n",
      "Iteration 101 ... step 102 loss 2.34218811989\n",
      "Iteration 102 ... step 103 loss 2.25947999954\n",
      "Iteration 103 ... step 104 loss 2.40766000748\n",
      "Iteration 104 ... step 105 loss 2.16669178009\n",
      "Iteration 105 ... step 106 loss 2.17658138275\n",
      "Iteration 106 ... step 107 loss 2.53920507431\n",
      "Iteration 107 ... step 108 loss 2.23881292343\n",
      "Iteration 108 ... step 109 loss 2.41223955154\n",
      "Iteration 109 ... step 110 loss 2.39147806168\n",
      "Iteration 110 ... step 111 loss 2.33086371422\n",
      "Iteration 111 ... step 112 loss 2.32190132141\n",
      "Iteration 112 ... step 113 loss 2.2422580719\n",
      "Iteration 113 ... step 114 loss 2.33408689499\n",
      "Iteration 114 ... step 115 loss 2.20135498047\n",
      "Iteration 115 ... step 116 loss 2.1492292881\n",
      "Iteration 116 ... step 117 loss 2.51974725723\n",
      "Iteration 117 ... step 118 loss 2.20591044426\n",
      "Iteration 118 ... step 119 loss 2.24177789688\n",
      "Iteration 119 ... step 120 loss 2.4101755619\n",
      "Iteration 120 ... step 121 loss 2.21423244476\n",
      "Iteration 121 ... step 122 loss 2.19168949127\n",
      "Iteration 122 ... step 123 loss 2.37625455856\n",
      "Iteration 123 ... step 124 loss 2.11749529839\n",
      "Iteration 124 ... step 125 loss 2.17511439323\n",
      "Iteration 125 ... step 126 loss 2.17404270172\n",
      "Iteration 126 ... step 127 loss 2.41538143158\n",
      "Iteration 127 ... step 128 loss 2.28110551834\n",
      "Iteration 128 ... step 129 loss 2.1487288475\n",
      "Iteration 129 ... step 130 loss 2.34149289131\n",
      "Iteration 130 ... step 131 loss 2.20186233521\n",
      "Iteration 131 ... step 132 loss 2.42781209946\n",
      "Iteration 132 ... step 133 loss 2.49371767044\n",
      "Iteration 133 ... step 134 loss 2.35534763336\n",
      "Iteration 134 ... step 135 loss 2.32076358795\n",
      "Iteration 135 ... step 136 loss 2.25673913956\n",
      "Iteration 136 ... step 137 loss 2.37943339348\n",
      "Iteration 137 ... step 138 loss 2.36818122864\n",
      "Iteration 138 ... step 139 loss 2.3427157402\n",
      "Iteration 139 ... step 140 loss 2.26145505905\n",
      "Iteration 140 ... step 141 loss 2.28717136383\n",
      "Iteration 141 ... step 142 loss 2.25403618813\n",
      "Iteration 142 ... step 143 loss 2.27023553848\n",
      "Iteration 143 ... step 144 loss 2.14474415779\n",
      "Iteration 144 ... step 145 loss 2.38376712799\n",
      "Iteration 145 ... step 146 loss 2.19921922684\n",
      "Iteration 146 ... step 147 loss 2.36764502525\n",
      "Iteration 147 ... step 148 loss 2.19817829132\n",
      "Iteration 148 ... step 149 loss 2.12759304047\n",
      "Iteration 149 ... step 150 loss 2.46143937111\n",
      "Iteration 150 ... step 151 loss 2.29115653038\n",
      "Iteration 151 ... step 152 loss 2.37632274628\n",
      "Iteration 152 ... step 153 loss 2.11633682251\n",
      "Iteration 153 ... step 154 loss 2.10832262039\n",
      "Iteration 154 ... step 155 loss 2.30352926254\n",
      "Iteration 155 ... step 156 loss 2.17389154434\n",
      "Iteration 156 ... step 157 loss 2.38537836075\n",
      "Iteration 157 ... step 158 loss 2.31540441513\n",
      "Iteration 158 ... step 159 loss 2.28693509102\n",
      "Iteration 159 ... step 160 loss 2.25092983246\n",
      "Iteration 160 ... step 161 loss 2.36608982086\n",
      "Iteration 161 ... step 162 loss 2.3009428978\n",
      "Iteration 162 ... step 163 loss 2.296230793\n",
      "Iteration 163 ... step 164 loss 2.10003066063\n",
      "Iteration 164 ... step 165 loss 2.17543387413\n",
      "Iteration 165 ... step 166 loss 2.18471956253\n",
      "Iteration 166 ... step 167 loss 2.19141530991\n",
      "Iteration 167 ... step 168 loss 2.30663943291\n",
      "Iteration 168 ... step 169 loss 2.20119047165\n",
      "Iteration 169 ... step 170 loss 2.10947227478\n",
      "Iteration 170 ... step 171 loss 2.14841341972\n",
      "Iteration 171 ... step 172 loss 2.14844870567\n",
      "Iteration 172 ... step 173 loss 2.22752475739\n",
      "Iteration 173 ... step 174 loss 2.27684211731\n",
      "Iteration 174 ... step 175 loss 2.34928154945\n",
      "Iteration 175 ... step 176 loss 2.22640991211\n",
      "Iteration 176 ... step 177 loss 2.24633026123\n",
      "Iteration 177 ... step 178 loss 2.30348682404\n",
      "Iteration 178 ... step 179 loss 2.09510755539\n",
      "Iteration 179 ... step 180 loss 2.20094203949\n",
      "Iteration 180 ... step 181 loss 2.0457649231\n",
      "Iteration 181 ... step 182 loss 2.37624621391\n",
      "Iteration 182 ... step 183 loss 2.17862319946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 183 ... step 184 loss 2.07374048233\n",
      "Iteration 184 ... step 185 loss 2.25898694992\n",
      "Iteration 185 ... step 186 loss 2.08748817444\n",
      "Iteration 186 ... step 187 loss 2.14061641693\n",
      "Iteration 187 ... step 188 loss 2.21252989769\n",
      "Iteration 188 ... step 189 loss 2.3218626976\n",
      "Iteration 189 ... step 190 loss 2.3483338356\n",
      "Iteration 190 ... step 191 loss 2.33314323425\n",
      "Iteration 191 ... step 192 loss 2.24686551094\n",
      "Iteration 192 ... step 193 loss 2.18998670578\n",
      "Iteration 193 ... step 194 loss 2.20352292061\n",
      "Iteration 194 ... step 195 loss 2.40285038948\n",
      "Iteration 195 ... step 196 loss 2.12371706963\n",
      "Iteration 196 ... step 197 loss 2.15352058411\n",
      "Iteration 197 ... step 198 loss 2.35987305641\n",
      "Iteration 198 ... step 199 loss 2.25082826614\n",
      "Iteration 199 ... step 200 loss 2.22130584717\n",
      "Iteration 200 ... step 201 loss 2.35059714317\n",
      "Iteration 201 ... step 202 loss 2.17579174042\n",
      "Iteration 202 ... step 203 loss 2.3226621151\n",
      "Iteration 203 ... step 204 loss 2.19273519516\n",
      "Iteration 204 ... step 205 loss 2.34204220772\n",
      "Iteration 205 ... step 206 loss 2.33745193481\n",
      "Iteration 206 ... step 207 loss 2.13936114311\n",
      "Iteration 207 ... step 208 loss 2.42262268066\n",
      "Iteration 208 ... step 209 loss 2.20980715752\n",
      "Iteration 209 ... step 210 loss 2.25262069702\n",
      "Iteration 210 ... step 211 loss 2.24970960617\n",
      "Iteration 211 ... step 212 loss 2.36320114136\n",
      "Iteration 212 ... step 213 loss 2.24983024597\n",
      "Iteration 213 ... step 214 loss 2.11003065109\n",
      "Iteration 214 ... step 215 loss 2.22854995728\n",
      "Iteration 215 ... step 216 loss 2.33176803589\n",
      "Iteration 216 ... step 217 loss 2.18766069412\n",
      "Iteration 217 ... step 218 loss 2.33059740067\n",
      "Iteration 218 ... step 219 loss 2.3601641655\n",
      "Iteration 219 ... step 220 loss 2.41592359543\n",
      "Iteration 220 ... step 221 loss 2.21577978134\n",
      "Iteration 221 ... step 222 loss 2.29084706306\n",
      "Iteration 222 ... step 223 loss 2.35409641266\n",
      "Iteration 223 ... step 224 loss 2.10812783241\n",
      "Iteration 224 ... step 225 loss 2.24143743515\n",
      "Iteration 225 ... step 226 loss 2.4285569191\n",
      "Iteration 226 ... step 227 loss 2.24362254143\n",
      "Iteration 227 ... step 228 loss 2.36046218872\n",
      "Iteration 228 ... step 229 loss 2.3261590004\n",
      "Iteration 229 ... step 230 loss 2.18887495995\n",
      "Iteration 230 ... step 231 loss 2.25707602501\n",
      "Iteration 231 ... step 232 loss 2.26287603378\n",
      "Iteration 232 ... step 233 loss 2.27241945267\n",
      "Iteration 233 ... step 234 loss 2.29821896553\n",
      "Iteration 234 ... step 235 loss 2.37281608582\n",
      "Iteration 235 ... step 236 loss 2.30098438263\n",
      "Iteration 236 ... step 237 loss 2.50138044357\n",
      "Iteration 237 ... step 238 loss 2.24166059494\n",
      "Iteration 238 ... step 239 loss 2.29541778564\n",
      "Iteration 239 ... step 240 loss 2.11366724968\n",
      "Iteration 240 ... step 241 loss 2.12287330627\n",
      "Iteration 241 ... step 242 loss 2.42157506943\n",
      "Iteration 242 ... step 243 loss 2.16730880737\n",
      "Iteration 243 ... step 244 loss 2.10989427567\n",
      "Iteration 244 ... step 245 loss 2.65841984749\n",
      "Iteration 245 ... step 246 loss 2.15866994858\n",
      "Iteration 246 ... step 247 loss 2.2226960659\n",
      "Iteration 247 ... step 248 loss 2.39042425156\n",
      "Iteration 248 ... step 249 loss 2.55005431175\n",
      "Iteration 249 ... step 250 loss 2.06223344803\n",
      "Iteration 250 ... step 251 loss 2.27303647995\n",
      "Iteration 251 ... step 252 loss 2.14866638184\n",
      "Iteration 252 ... step 253 loss 2.29344844818\n",
      "Iteration 253 ... step 254 loss 2.01598310471\n",
      "Iteration 254 ... step 255 loss 2.32501339912\n",
      "Iteration 255 ... step 256 loss 2.31752443314\n",
      "Iteration 256 ... step 257 loss 2.20430660248\n",
      "Iteration 257 ... step 258 loss 2.33061122894\n",
      "Iteration 258 ... step 259 loss 2.22959685326\n",
      "Iteration 259 ... step 260 loss 2.13342881203\n",
      "Iteration 260 ... step 261 loss 2.22184085846\n",
      "Iteration 261 ... step 262 loss 2.32130742073\n",
      "Iteration 262 ... step 263 loss 2.22635316849\n",
      "Iteration 263 ... step 264 loss 2.22257184982\n",
      "Iteration 264 ... step 265 loss 2.2973985672\n",
      "Iteration 265 ... step 266 loss 2.14211821556\n",
      "Iteration 266 ... step 267 loss 2.24880981445\n",
      "Iteration 267 ... step 268 loss 2.2616276741\n",
      "Iteration 268 ... step 269 loss 2.20009469986\n",
      "Iteration 269 ... step 270 loss 2.27269363403\n",
      "Iteration 270 ... step 271 loss 2.2329454422\n",
      "Iteration 271 ... step 272 loss 2.22074580193\n",
      "Iteration 272 ... step 273 loss 2.21351242065\n",
      "Iteration 273 ... step 274 loss 2.25246620178\n",
      "Iteration 274 ... step 275 loss 2.14385938644\n",
      "Iteration 275 ... step 276 loss 2.29890799522\n",
      "Iteration 276 ... step 277 loss 2.28986763954\n",
      "Iteration 277 ... step 278 loss 2.3014857769\n",
      "Iteration 278 ... step 279 loss 2.38302326202\n",
      "Iteration 279 ... step 280 loss 2.21298742294\n",
      "Iteration 280 ... step 281 loss 2.19383478165\n",
      "Iteration 281 ... step 282 loss 2.37192702293\n",
      "Iteration 282 ... step 283 loss 2.26255130768\n",
      "Iteration 283 ... step 284 loss 2.13540434837\n",
      "Iteration 284 ... step 285 loss 2.25880718231\n",
      "Iteration 285 ... step 286 loss 2.13658428192\n",
      "Iteration 286 ... step 287 loss 2.31630754471\n",
      "Iteration 287 ... step 288 loss 2.2047829628\n",
      "Iteration 288 ... step 289 loss 2.19245433807\n",
      "Iteration 289 ... step 290 loss 2.21351838112\n",
      "Iteration 290 ... step 291 loss 2.28757572174\n",
      "Iteration 291 ... step 292 loss 2.00210595131\n",
      "Iteration 292 ... step 293 loss 2.18787336349\n",
      "Iteration 293 ... step 294 loss 2.31879520416\n",
      "Iteration 294 ... step 295 loss 2.27903866768\n",
      "Iteration 295 ... step 296 loss 2.65970468521\n",
      "Iteration 296 ... step 297 loss 2.2929148674\n",
      "Iteration 297 ... step 298 loss 2.20958375931\n",
      "Iteration 298 ... step 299 loss 2.35309815407\n",
      "Iteration 299 ... step 300 loss 2.26742935181\n",
      "Iteration 300 ... step 301 loss 2.48757219315\n",
      "Iteration 301 ... step 302 loss 2.33631157875\n",
      "Iteration 302 ... step 303 loss 2.24331903458\n",
      "Iteration 303 ... step 304 loss 2.33509683609\n",
      "Iteration 304 ... step 305 loss 2.33428192139\n",
      "Iteration 305 ... step 306 loss 2.30072641373\n",
      "Iteration 306 ... step 307 loss 2.50375127792\n",
      "Iteration 307 ... step 308 loss 2.34499979019\n",
      "Iteration 308 ... step 309 loss 2.41839575768\n",
      "Iteration 309 ... step 310 loss 2.30282688141\n",
      "Iteration 310 ... step 311 loss 2.31334686279\n",
      "Iteration 311 ... step 312 loss 2.14893198013\n",
      "Iteration 312 ... step 313 loss 2.25821828842\n",
      "Iteration 313 ... step 314 loss 2.33449554443\n",
      "Iteration 314 ... step 315 loss 2.24485564232\n",
      "Iteration 315 ... step 316 loss 2.35937166214\n",
      "Iteration 316 ... step 317 loss 2.07033371925\n",
      "Iteration 317 ... step 318 loss 2.16045236588\n",
      "Iteration 318 ... step 319 loss 2.28058195114\n",
      "Iteration 319 ... step 320 loss 2.34330320358\n",
      "Iteration 320 ... step 321 loss 2.15465927124\n",
      "Iteration 321 ... step 322 loss 2.03268718719\n",
      "Iteration 322 ... step 323 loss 2.40041422844\n",
      "Iteration 323 ... step 324 loss 2.44478702545\n",
      "Iteration 324 ... step 325 loss 2.29625749588\n",
      "Iteration 325 ... step 326 loss 2.36917471886\n",
      "Iteration 326 ... step 327 loss 2.16672420502\n",
      "Iteration 327 ... step 328 loss 2.29159927368\n",
      "Iteration 328 ... step 329 loss 2.21770811081\n",
      "Iteration 329 ... step 330 loss 2.39904117584\n",
      "Iteration 330 ... step 331 loss 2.33422756195\n",
      "Iteration 331 ... step 332 loss 2.3275179863\n",
      "Iteration 332 ... step 333 loss 2.44159889221\n",
      "Iteration 333 ... step 334 loss 2.15784311295\n",
      "Iteration 334 ... step 335 loss 2.27749967575\n",
      "Iteration 335 ... step 336 loss 2.14766836166\n",
      "Iteration 336 ... step 337 loss 2.19378805161\n",
      "Iteration 337 ... step 338 loss 2.14379453659\n",
      "Iteration 338 ... step 339 loss 2.25510835648\n",
      "Iteration 339 ... step 340 loss 2.12990283966\n",
      "Iteration 340 ... step 341 loss 2.19214868546\n",
      "Iteration 341 ... step 342 loss 2.26118564606\n",
      "Iteration 342 ... step 343 loss 2.33046412468\n",
      "Iteration 343 ... step 344 loss 2.1564142704\n",
      "Iteration 344 ... step 345 loss 2.37035417557\n",
      "Iteration 345 ... step 346 loss 2.05310297012\n",
      "Iteration 346 ... step 347 loss 2.25786113739\n",
      "Iteration 347 ... step 348 loss 2.29258179665\n",
      "Iteration 348 ... step 349 loss 2.30506014824\n",
      "Iteration 349 ... step 350 loss 2.32406330109\n",
      "Iteration 350 ... step 351 loss 2.30833673477\n",
      "Iteration 351 ... step 352 loss 2.06722402573\n",
      "Iteration 352 ... step 353 loss 2.26193928719\n",
      "Iteration 353 ... step 354 loss 2.25959205627\n",
      "Iteration 354 ... step 355 loss 2.46805667877\n",
      "Iteration 355 ... step 356 loss 2.26192903519\n",
      "Iteration 356 ... step 357 loss 2.22352409363\n",
      "Iteration 357 ... step 358 loss 2.26089406013\n",
      "Iteration 358 ... step 359 loss 2.32249093056\n",
      "Iteration 359 ... step 360 loss 2.37586450577\n",
      "Iteration 360 ... step 361 loss 2.37644863129\n",
      "Iteration 361 ... step 362 loss 2.50533604622\n",
      "Iteration 362 ... step 363 loss 2.21094107628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 363 ... step 364 loss 2.18031167984\n",
      "Iteration 364 ... step 365 loss 2.29192638397\n",
      "Iteration 365 ... step 366 loss 2.24346065521\n",
      "Iteration 366 ... step 367 loss 2.35952758789\n",
      "Iteration 367 ... step 368 loss 2.18191003799\n",
      "Iteration 368 ... step 369 loss 2.21057033539\n",
      "Iteration 369 ... step 370 loss 2.34536314011\n",
      "Iteration 370 ... step 371 loss 2.39218378067\n",
      "Iteration 371 ... step 372 loss 2.33766508102\n",
      "Iteration 372 ... step 373 loss 2.12057089806\n",
      "Iteration 373 ... step 374 loss 2.2064602375\n",
      "Iteration 374 ... step 375 loss 2.30845975876\n",
      "Iteration 375 ... step 376 loss 2.24492788315\n",
      "Iteration 376 ... step 377 loss 2.16488695145\n",
      "Iteration 377 ... step 378 loss 2.33229160309\n",
      "Iteration 378 ... step 379 loss 2.28449821472\n",
      "Iteration 379 ... step 380 loss 2.44454097748\n",
      "Iteration 380 ... step 381 loss 2.52373409271\n",
      "Iteration 381 ... step 382 loss 2.38977622986\n",
      "Iteration 382 ... step 383 loss 2.39851093292\n",
      "Iteration 383 ... step 384 loss 2.3828253746\n",
      "Iteration 384 ... step 385 loss 2.28671860695\n",
      "Iteration 385 ... step 386 loss 2.20717668533\n",
      "Iteration 386 ... step 387 loss 2.47463846207\n",
      "Iteration 387 ... step 388 loss 2.17950248718\n",
      "Iteration 388 ... step 389 loss 2.25688838959\n",
      "Iteration 389 ... step 390 loss 2.35587692261\n",
      "Iteration 390 ... step 391 loss 2.12893629074\n",
      "Iteration 391 ... step 392 loss 2.16948747635\n",
      "Iteration 392 ... step 393 loss 2.08209514618\n",
      "Iteration 393 ... step 394 loss 2.30801892281\n",
      "Iteration 394 ... step 395 loss 2.29943704605\n",
      "Iteration 395 ... step 396 loss 2.21577215195\n",
      "Iteration 396 ... step 397 loss 2.15209722519\n",
      "Iteration 397 ... step 398 loss 2.25498008728\n",
      "Iteration 398 ... step 399 loss 2.61050033569\n",
      "Iteration 399 ... step 400 loss 2.38027667999\n",
      "Iteration 400 ... step 401 loss 2.13983225822\n",
      "Iteration 401 ... step 402 loss 2.17774009705\n",
      "Iteration 402 ... step 403 loss 2.19606876373\n",
      "Iteration 403 ... step 404 loss 2.32071852684\n",
      "Iteration 404 ... step 405 loss 2.26846528053\n",
      "Iteration 405 ... step 406 loss 2.20770597458\n",
      "Iteration 406 ... step 407 loss 2.10718107224\n",
      "Iteration 407 ... step 408 loss 2.14182186127\n",
      "Iteration 408 ... step 409 loss 2.30203533173\n",
      "Iteration 409 ... step 410 loss 2.27989816666\n",
      "Iteration 410 ... step 411 loss 2.13464426994\n",
      "Iteration 411 ... step 412 loss 2.27258348465\n",
      "Iteration 412 ... step 413 loss 2.33155584335\n",
      "Iteration 413 ... step 414 loss 2.15528917313\n",
      "Iteration 414 ... step 415 loss 2.17900800705\n",
      "Iteration 415 ... step 416 loss 2.43078589439\n",
      "Iteration 416 ... step 417 loss 2.07600593567\n",
      "Iteration 417 ... step 418 loss 2.32342863083\n",
      "Iteration 418 ... step 419 loss 2.29654121399\n",
      "Iteration 419 ... step 420 loss 2.20799541473\n",
      "Iteration 420 ... step 421 loss 2.41881322861\n",
      "Iteration 421 ... step 422 loss 2.43462610245\n",
      "Iteration 422 ... step 423 loss 2.11583852768\n",
      "Iteration 423 ... step 424 loss 2.33754563332\n",
      "Iteration 424 ... step 425 loss 2.31596517563\n",
      "Iteration 425 ... step 426 loss 2.28654384613\n",
      "Iteration 426 ... step 427 loss 2.20385217667\n",
      "Iteration 427 ... step 428 loss 2.2765455246\n",
      "Iteration 428 ... step 429 loss 2.14304423332\n",
      "Iteration 429 ... step 430 loss 2.12285327911\n",
      "Iteration 430 ... step 431 loss 2.39162969589\n",
      "Iteration 431 ... step 432 loss 2.36750364304\n",
      "Iteration 432 ... step 433 loss 2.24585771561\n",
      "Iteration 433 ... step 434 loss 2.21496295929\n",
      "Iteration 434 ... step 435 loss 2.30584406853\n",
      "Iteration 435 ... step 436 loss 2.2433590889\n",
      "Iteration 436 ... step 437 loss 2.28062272072\n",
      "Iteration 437 ... step 438 loss 2.18730854988\n",
      "Iteration 438 ... step 439 loss 2.50642251968\n",
      "Iteration 439 ... step 440 loss 2.08247995377\n",
      "Iteration 440 ... step 441 loss 2.23441100121\n",
      "Iteration 441 ... step 442 loss 2.23355960846\n",
      "Iteration 442 ... step 443 loss 2.27192020416\n",
      "Iteration 443 ... step 444 loss 2.25154304504\n",
      "Iteration 444 ... step 445 loss 2.44234132767\n",
      "Iteration 445 ... step 446 loss 2.19021606445\n",
      "Iteration 446 ... step 447 loss 2.13469052315\n",
      "Iteration 447 ... step 448 loss 2.39376401901\n",
      "Iteration 448 ... step 449 loss 2.153652668\n",
      "Iteration 449 ... step 450 loss 2.27269077301\n",
      "Iteration 450 ... step 451 loss 2.2104434967\n",
      "Iteration 451 ... step 452 loss 2.17486381531\n",
      "Iteration 452 ... step 453 loss 2.44685935974\n",
      "Iteration 453 ... step 454 loss 2.18832135201\n",
      "Iteration 454 ... step 455 loss 2.27479434013\n",
      "Iteration 455 ... step 456 loss 2.4046330452\n",
      "Iteration 456 ... step 457 loss 2.09394931793\n",
      "Iteration 457 ... step 458 loss 2.23878097534\n",
      "Iteration 458 ... step 459 loss 2.21998643875\n",
      "Iteration 459 ... step 460 loss 2.14851641655\n",
      "Iteration 460 ... step 461 loss 2.45662307739\n",
      "Iteration 461 ... step 462 loss 2.20861434937\n",
      "Iteration 462 ... step 463 loss 2.1538901329\n",
      "Iteration 463 ... step 464 loss 2.31318807602\n",
      "Iteration 464 ... step 465 loss 2.29992437363\n",
      "Iteration 465 ... step 466 loss 2.2757229805\n",
      "Iteration 466 ... step 467 loss 2.41538286209\n",
      "Iteration 467 ... step 468 loss 2.20616960526\n",
      "Iteration 468 ... step 469 loss 2.22909832001\n",
      "Iteration 469 ... step 470 loss 2.26197457314\n",
      "Iteration 470 ... step 471 loss 2.20452404022\n",
      "Iteration 471 ... step 472 loss 2.50191998482\n",
      "Iteration 472 ... step 473 loss 2.30368709564\n",
      "Iteration 473 ... step 474 loss 2.30812072754\n",
      "Iteration 474 ... step 475 loss 2.38521194458\n",
      "Iteration 475 ... step 476 loss 2.27626609802\n",
      "Iteration 476 ... step 477 loss 2.136906147\n",
      "Iteration 477 ... step 478 loss 2.27297878265\n",
      "Iteration 478 ... step 479 loss 2.44432973862\n",
      "Iteration 479 ... step 480 loss 2.20915412903\n",
      "Iteration 480 ... step 481 loss 2.10992622375\n",
      "Iteration 481 ... step 482 loss 2.36909246445\n",
      "Iteration 482 ... step 483 loss 2.31198406219\n",
      "Iteration 483 ... step 484 loss 2.4292383194\n",
      "Iteration 484 ... step 485 loss 2.05231428146\n",
      "Iteration 485 ... step 486 loss 2.12451505661\n",
      "Iteration 486 ... step 487 loss 2.36734724045\n",
      "Iteration 487 ... step 488 loss 2.20172834396\n",
      "Iteration 488 ... step 489 loss 2.2747964859\n",
      "Iteration 489 ... step 490 loss 2.28462338448\n",
      "Iteration 490 ... step 491 loss 2.22175931931\n",
      "Iteration 491 ... step 492 loss 2.0839548111\n",
      "Iteration 492 ... step 493 loss 2.32541847229\n",
      "Iteration 493 ... step 494 loss 2.13725805283\n",
      "Iteration 494 ... step 495 loss 2.30222249031\n",
      "Iteration 495 ... step 496 loss 2.16822171211\n",
      "Iteration 496 ... step 497 loss 2.20436239243\n",
      "Iteration 497 ... step 498 loss 2.01121687889\n",
      "Iteration 498 ... step 499 loss 2.34842824936\n",
      "Iteration 499 ... step 500 loss 2.44059157372\n",
      "Iteration 500 ... step 501 loss 2.12162947655\n",
      "Minibatch loss at step 500: 2.121629\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 501 ... step 502 loss 2.28072524071\n",
      "Iteration 502 ... step 503 loss 2.39466238022\n",
      "Iteration 503 ... step 504 loss 2.40832948685\n",
      "Iteration 504 ... step 505 loss 2.42943096161\n",
      "Iteration 505 ... step 506 loss 2.33274912834\n",
      "Iteration 506 ... step 507 loss 2.16161513329\n",
      "Iteration 507 ... step 508 loss 2.22894287109\n",
      "Iteration 508 ... step 509 loss 2.39508914948\n",
      "Iteration 509 ... step 510 loss 2.36330366135\n",
      "Iteration 510 ... step 511 loss 2.29183673859\n",
      "Iteration 511 ... step 512 loss 2.13146400452\n",
      "Iteration 512 ... step 513 loss 2.1340045929\n",
      "Iteration 513 ... step 514 loss 2.13941001892\n",
      "Iteration 514 ... step 515 loss 2.1427590847\n",
      "Iteration 515 ... step 516 loss 2.18145346642\n",
      "Iteration 516 ... step 517 loss 2.36137294769\n",
      "Iteration 517 ... step 518 loss 2.27166318893\n",
      "Iteration 518 ... step 519 loss 2.42542076111\n",
      "Iteration 519 ... step 520 loss 2.07390737534\n",
      "Iteration 520 ... step 521 loss 2.01573848724\n",
      "Iteration 521 ... step 522 loss 2.27147555351\n",
      "Iteration 522 ... step 523 loss 2.17698955536\n",
      "Iteration 523 ... step 524 loss 2.40575933456\n",
      "Iteration 524 ... step 525 loss 2.20875263214\n",
      "Iteration 525 ... step 526 loss 2.19295072556\n",
      "Iteration 526 ... step 527 loss 2.13838768005\n",
      "Iteration 527 ... step 528 loss 2.3508143425\n",
      "Iteration 528 ... step 529 loss 2.29334807396\n",
      "Iteration 529 ... step 530 loss 2.16402721405\n",
      "Iteration 530 ... step 531 loss 2.28521251678\n",
      "Iteration 531 ... step 532 loss 2.31461954117\n",
      "Iteration 532 ... step 533 loss 2.33454418182\n",
      "Iteration 533 ... step 534 loss 2.16497397423\n",
      "Iteration 534 ... step 535 loss 2.26213026047\n",
      "Iteration 535 ... step 536 loss 2.15402889252\n",
      "Iteration 536 ... step 537 loss 2.24480438232\n",
      "Iteration 537 ... step 538 loss 2.28913831711\n",
      "Iteration 538 ... step 539 loss 2.35115718842\n",
      "Iteration 539 ... step 540 loss 2.34713411331\n",
      "Iteration 540 ... step 541 loss 2.13469791412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 541 ... step 542 loss 2.44231319427\n",
      "Iteration 542 ... step 543 loss 2.18038678169\n",
      "Iteration 543 ... step 544 loss 2.03113412857\n",
      "Iteration 544 ... step 545 loss 2.28576374054\n",
      "Iteration 545 ... step 546 loss 2.37753295898\n",
      "Iteration 546 ... step 547 loss 2.16230916977\n",
      "Iteration 547 ... step 548 loss 2.12164115906\n",
      "Iteration 548 ... step 549 loss 2.47044610977\n",
      "Iteration 549 ... step 550 loss 2.29103326797\n",
      "Iteration 550 ... step 551 loss 2.26095247269\n",
      "Iteration 551 ... step 552 loss 2.26705932617\n",
      "Iteration 552 ... step 553 loss 2.27928209305\n",
      "Iteration 553 ... step 554 loss 2.17314600945\n",
      "Iteration 554 ... step 555 loss 2.38998508453\n",
      "Iteration 555 ... step 556 loss 2.20842909813\n",
      "Iteration 556 ... step 557 loss 2.28488063812\n",
      "Iteration 557 ... step 558 loss 2.22193932533\n",
      "Iteration 558 ... step 559 loss 2.29133462906\n",
      "Iteration 559 ... step 560 loss 2.3838596344\n",
      "Iteration 560 ... step 561 loss 2.13636469841\n",
      "Iteration 561 ... step 562 loss 2.30360794067\n",
      "Iteration 562 ... step 563 loss 2.18749713898\n",
      "Iteration 563 ... step 564 loss 2.23271012306\n",
      "Iteration 564 ... step 565 loss 2.24933958054\n",
      "Iteration 565 ... step 566 loss 2.21144509315\n",
      "Iteration 566 ... step 567 loss 2.26362895966\n",
      "Iteration 567 ... step 568 loss 2.37564849854\n",
      "Iteration 568 ... step 569 loss 2.33729505539\n",
      "Iteration 569 ... step 570 loss 2.16979837418\n",
      "Iteration 570 ... step 571 loss 2.18341350555\n",
      "Iteration 571 ... step 572 loss 2.23763132095\n",
      "Iteration 572 ... step 573 loss 2.11473846436\n",
      "Iteration 573 ... step 574 loss 2.33957338333\n",
      "Iteration 574 ... step 575 loss 2.24547672272\n",
      "Iteration 575 ... step 576 loss 2.28416061401\n",
      "Iteration 576 ... step 577 loss 2.44194078445\n",
      "Iteration 577 ... step 578 loss 2.1390209198\n",
      "Iteration 578 ... step 579 loss 2.32647037506\n",
      "Iteration 579 ... step 580 loss 2.13008117676\n",
      "Iteration 580 ... step 581 loss 2.16867446899\n",
      "Iteration 581 ... step 582 loss 2.11142253876\n",
      "Iteration 582 ... step 583 loss 2.47364425659\n",
      "Iteration 583 ... step 584 loss 2.28894042969\n",
      "Iteration 584 ... step 585 loss 2.09743762016\n",
      "Iteration 585 ... step 586 loss 2.08731651306\n",
      "Iteration 586 ... step 587 loss 2.43592190742\n",
      "Iteration 587 ... step 588 loss 2.28461408615\n",
      "Iteration 588 ... step 589 loss 2.14842891693\n",
      "Iteration 589 ... step 590 loss 2.3293864727\n",
      "Iteration 590 ... step 591 loss 2.05892658234\n",
      "Iteration 591 ... step 592 loss 2.18192052841\n",
      "Iteration 592 ... step 593 loss 2.27002763748\n",
      "Iteration 593 ... step 594 loss 2.53720426559\n",
      "Iteration 594 ... step 595 loss 2.1451048851\n",
      "Iteration 595 ... step 596 loss 2.31506991386\n",
      "Iteration 596 ... step 597 loss 2.39449930191\n",
      "Iteration 597 ... step 598 loss 2.20382547379\n",
      "Iteration 598 ... step 599 loss 2.28523731232\n",
      "Iteration 599 ... step 600 loss 2.21357965469\n",
      "Iteration 600 ... step 601 loss 2.24803495407\n",
      "Iteration 601 ... step 602 loss 2.21382808685\n",
      "Iteration 602 ... step 603 loss 2.21948575974\n",
      "Iteration 603 ... step 604 loss 2.19875431061\n",
      "Iteration 604 ... step 605 loss 2.13538217545\n",
      "Iteration 605 ... step 606 loss 2.32215023041\n",
      "Iteration 606 ... step 607 loss 2.4046216011\n",
      "Iteration 607 ... step 608 loss 2.32353162766\n",
      "Iteration 608 ... step 609 loss 2.23211812973\n",
      "Iteration 609 ... step 610 loss 2.22344875336\n",
      "Iteration 610 ... step 611 loss 2.29660749435\n",
      "Iteration 611 ... step 612 loss 2.42647838593\n",
      "Iteration 612 ... step 613 loss 2.07954597473\n",
      "Iteration 613 ... step 614 loss 2.25603914261\n",
      "Iteration 614 ... step 615 loss 2.1673078537\n",
      "Iteration 615 ... step 616 loss 2.2992837429\n",
      "Iteration 616 ... step 617 loss 2.26869010925\n",
      "Iteration 617 ... step 618 loss 2.20369291306\n",
      "Iteration 618 ... step 619 loss 2.3206820488\n",
      "Iteration 619 ... step 620 loss 2.37726593018\n",
      "Iteration 620 ... step 621 loss 2.31703472137\n",
      "Iteration 621 ... step 622 loss 2.2322602272\n",
      "Iteration 622 ... step 623 loss 2.24226570129\n",
      "Iteration 623 ... step 624 loss 2.24272942543\n",
      "Iteration 624 ... step 625 loss 2.39323091507\n",
      "Iteration 625 ... step 626 loss 2.22729182243\n",
      "Iteration 626 ... step 627 loss 2.24721479416\n",
      "Iteration 627 ... step 628 loss 2.09616088867\n",
      "Iteration 628 ... step 629 loss 2.23297166824\n",
      "Iteration 629 ... step 630 loss 2.24129295349\n",
      "Iteration 630 ... step 631 loss 2.27012395859\n",
      "Iteration 631 ... step 632 loss 2.24869227409\n",
      "Iteration 632 ... step 633 loss 2.21581792831\n",
      "Iteration 633 ... step 634 loss 2.15035295486\n",
      "Iteration 634 ... step 635 loss 2.21908473969\n",
      "Iteration 635 ... step 636 loss 2.29482412338\n",
      "Iteration 636 ... step 637 loss 2.31532430649\n",
      "Iteration 637 ... step 638 loss 2.31823921204\n",
      "Iteration 638 ... step 639 loss 2.16355800629\n",
      "Iteration 639 ... step 640 loss 2.24908399582\n",
      "Iteration 640 ... step 641 loss 2.43090343475\n",
      "Iteration 641 ... step 642 loss 2.1611905098\n",
      "Iteration 642 ... step 643 loss 2.44890832901\n",
      "Iteration 643 ... step 644 loss 2.33389616013\n",
      "Iteration 644 ... step 645 loss 2.31765937805\n",
      "Iteration 645 ... step 646 loss 2.36361670494\n",
      "Iteration 646 ... step 647 loss 2.35462474823\n",
      "Iteration 647 ... step 648 loss 2.09005188942\n",
      "Iteration 648 ... step 649 loss 2.36977529526\n",
      "Iteration 649 ... step 650 loss 2.22353315353\n",
      "Iteration 650 ... step 651 loss 2.14212417603\n",
      "Iteration 651 ... step 652 loss 2.33855438232\n",
      "Iteration 652 ... step 653 loss 2.17116117477\n",
      "Iteration 653 ... step 654 loss 2.53159570694\n",
      "Iteration 654 ... step 655 loss 2.12089538574\n",
      "Iteration 655 ... step 656 loss 2.26951217651\n",
      "Iteration 656 ... step 657 loss 2.60492563248\n",
      "Iteration 657 ... step 658 loss 2.40946054459\n",
      "Iteration 658 ... step 659 loss 2.27315616608\n",
      "Iteration 659 ... step 660 loss 2.19065332413\n",
      "Iteration 660 ... step 661 loss 2.24915599823\n",
      "Iteration 661 ... step 662 loss 2.46554756165\n",
      "Iteration 662 ... step 663 loss 2.2338385582\n",
      "Iteration 663 ... step 664 loss 2.26442527771\n",
      "Iteration 664 ... step 665 loss 2.15513801575\n",
      "Iteration 665 ... step 666 loss 2.4149646759\n",
      "Iteration 666 ... step 667 loss 2.08302783966\n",
      "Iteration 667 ... step 668 loss 2.09703493118\n",
      "Iteration 668 ... step 669 loss 2.19487380981\n",
      "Iteration 669 ... step 670 loss 2.35741758347\n",
      "Iteration 670 ... step 671 loss 2.19454526901\n",
      "Iteration 671 ... step 672 loss 2.36525774002\n",
      "Iteration 672 ... step 673 loss 2.37673997879\n",
      "Iteration 673 ... step 674 loss 2.26895523071\n",
      "Iteration 674 ... step 675 loss 2.33375787735\n",
      "Iteration 675 ... step 676 loss 2.0293238163\n",
      "Iteration 676 ... step 677 loss 2.31717395782\n",
      "Iteration 677 ... step 678 loss 2.44042253494\n",
      "Iteration 678 ... step 679 loss 2.33555316925\n",
      "Iteration 679 ... step 680 loss 2.46500873566\n",
      "Iteration 680 ... step 681 loss 2.27086353302\n",
      "Iteration 681 ... step 682 loss 2.2647819519\n",
      "Iteration 682 ... step 683 loss 2.20375061035\n",
      "Iteration 683 ... step 684 loss 2.24158287048\n",
      "Iteration 684 ... step 685 loss 2.34186887741\n",
      "Iteration 685 ... step 686 loss 2.1439037323\n",
      "Iteration 686 ... step 687 loss 2.13032484055\n",
      "Iteration 687 ... step 688 loss 2.18608236313\n",
      "Iteration 688 ... step 689 loss 2.15856313705\n",
      "Iteration 689 ... step 690 loss 2.22709941864\n",
      "Iteration 690 ... step 691 loss 2.18301200867\n",
      "Iteration 691 ... step 692 loss 2.14341545105\n",
      "Iteration 692 ... step 693 loss 2.12050151825\n",
      "Iteration 693 ... step 694 loss 2.30460500717\n",
      "Iteration 694 ... step 695 loss 2.26407408714\n",
      "Iteration 695 ... step 696 loss 2.40241670609\n",
      "Iteration 696 ... step 697 loss 2.13859128952\n",
      "Iteration 697 ... step 698 loss 2.33859443665\n",
      "Iteration 698 ... step 699 loss 2.25591278076\n",
      "Iteration 699 ... step 700 loss 2.14135050774\n",
      "Iteration 700 ... step 701 loss 2.32836914062\n",
      "Iteration 701 ... step 702 loss 2.13574743271\n",
      "Iteration 702 ... step 703 loss 2.29001307487\n",
      "Iteration 703 ... step 704 loss 2.48772144318\n",
      "Iteration 704 ... step 705 loss 2.25865054131\n",
      "Iteration 705 ... step 706 loss 2.28583288193\n",
      "Iteration 706 ... step 707 loss 2.17805027962\n",
      "Iteration 707 ... step 708 loss 2.37892603874\n",
      "Iteration 708 ... step 709 loss 2.26558017731\n",
      "Iteration 709 ... step 710 loss 2.18543815613\n",
      "Iteration 710 ... step 711 loss 2.19764709473\n",
      "Iteration 711 ... step 712 loss 2.23490858078\n",
      "Iteration 712 ... step 713 loss 2.17545819283\n",
      "Iteration 713 ... step 714 loss 2.30311346054\n",
      "Iteration 714 ... step 715 loss 2.36126470566\n",
      "Iteration 715 ... step 716 loss 2.27683973312\n",
      "Iteration 716 ... step 717 loss 2.17918252945\n",
      "Iteration 717 ... step 718 loss 2.32161784172\n",
      "Iteration 718 ... step 719 loss 2.15209913254\n",
      "Iteration 719 ... step 720 loss 2.3568239212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 720 ... step 721 loss 2.06093096733\n",
      "Iteration 721 ... step 722 loss 2.15952515602\n",
      "Iteration 722 ... step 723 loss 2.03179311752\n",
      "Iteration 723 ... step 724 loss 2.37573862076\n",
      "Iteration 724 ... step 725 loss 2.45591115952\n",
      "Iteration 725 ... step 726 loss 2.39694547653\n",
      "Iteration 726 ... step 727 loss 1.95225548744\n",
      "Iteration 727 ... step 728 loss 2.12410354614\n",
      "Iteration 728 ... step 729 loss 2.34713363647\n",
      "Iteration 729 ... step 730 loss 2.23948144913\n",
      "Iteration 730 ... step 731 loss 2.2108271122\n",
      "Iteration 731 ... step 732 loss 2.23899364471\n",
      "Iteration 732 ... step 733 loss 2.51820516586\n",
      "Iteration 733 ... step 734 loss 2.28916573524\n",
      "Iteration 734 ... step 735 loss 2.36692523956\n",
      "Iteration 735 ... step 736 loss 2.14150524139\n",
      "Iteration 736 ... step 737 loss 2.30721569061\n",
      "Iteration 737 ... step 738 loss 2.30821323395\n",
      "Iteration 738 ... step 739 loss 2.13242125511\n",
      "Iteration 739 ... step 740 loss 2.24373888969\n",
      "Iteration 740 ... step 741 loss 2.24763059616\n",
      "Iteration 741 ... step 742 loss 2.32565951347\n",
      "Iteration 742 ... step 743 loss 2.19588375092\n",
      "Iteration 743 ... step 744 loss 2.17803239822\n",
      "Iteration 744 ... step 745 loss 2.33589553833\n",
      "Iteration 745 ... step 746 loss 2.41023921967\n",
      "Iteration 746 ... step 747 loss 2.32066249847\n",
      "Iteration 747 ... step 748 loss 2.40468883514\n",
      "Iteration 748 ... step 749 loss 2.30645370483\n",
      "Iteration 749 ... step 750 loss 2.41360378265\n",
      "Iteration 750 ... step 751 loss 2.14756774902\n",
      "Iteration 751 ... step 752 loss 2.04573941231\n",
      "Iteration 752 ... step 753 loss 2.56532812119\n",
      "Iteration 753 ... step 754 loss 2.4471077919\n",
      "Iteration 754 ... step 755 loss 2.34527635574\n",
      "Iteration 755 ... step 756 loss 2.24979925156\n",
      "Iteration 756 ... step 757 loss 2.45692586899\n",
      "Iteration 757 ... step 758 loss 2.0949947834\n",
      "Iteration 758 ... step 759 loss 2.10462856293\n",
      "Iteration 759 ... step 760 loss 2.34043359756\n",
      "Iteration 760 ... step 761 loss 2.25619506836\n",
      "Iteration 761 ... step 762 loss 2.21630048752\n",
      "Iteration 762 ... step 763 loss 2.20756101608\n",
      "Iteration 763 ... step 764 loss 2.33494663239\n",
      "Iteration 764 ... step 765 loss 2.16397118568\n",
      "Iteration 765 ... step 766 loss 2.28094816208\n",
      "Iteration 766 ... step 767 loss 2.11852121353\n",
      "Iteration 767 ... step 768 loss 2.27288818359\n",
      "Iteration 768 ... step 769 loss 2.13634681702\n",
      "Iteration 769 ... step 770 loss 2.07096767426\n",
      "Iteration 770 ... step 771 loss 2.10482478142\n",
      "Iteration 771 ... step 772 loss 2.30594849586\n",
      "Iteration 772 ... step 773 loss 2.19657802582\n",
      "Iteration 773 ... step 774 loss 2.18549919128\n",
      "Iteration 774 ... step 775 loss 2.16455173492\n",
      "Iteration 775 ... step 776 loss 2.35340833664\n",
      "Iteration 776 ... step 777 loss 2.15668344498\n",
      "Iteration 777 ... step 778 loss 2.20926094055\n",
      "Iteration 778 ... step 779 loss 2.30527734756\n",
      "Iteration 779 ... step 780 loss 2.36950111389\n",
      "Iteration 780 ... step 781 loss 2.10020232201\n",
      "Iteration 781 ... step 782 loss 2.23589468002\n",
      "Iteration 782 ... step 783 loss 2.2005200386\n",
      "Iteration 783 ... step 784 loss 2.20591878891\n",
      "Iteration 784 ... step 785 loss 2.21636915207\n",
      "Iteration 785 ... step 786 loss 2.31548643112\n",
      "Iteration 786 ... step 787 loss 2.2323307991\n",
      "Iteration 787 ... step 788 loss 2.02785539627\n",
      "Iteration 788 ... step 789 loss 2.44548416138\n",
      "Iteration 789 ... step 790 loss 2.48368597031\n",
      "Iteration 790 ... step 791 loss 2.20472049713\n",
      "Iteration 791 ... step 792 loss 2.51568889618\n",
      "Iteration 792 ... step 793 loss 2.35928297043\n",
      "Iteration 793 ... step 794 loss 2.30309605598\n",
      "Iteration 794 ... step 795 loss 2.18235445023\n",
      "Iteration 795 ... step 796 loss 2.23621273041\n",
      "Iteration 796 ... step 797 loss 2.35951137543\n",
      "Iteration 797 ... step 798 loss 2.4051566124\n",
      "Iteration 798 ... step 799 loss 2.10233664513\n",
      "Iteration 799 ... step 800 loss 2.29136443138\n",
      "Iteration 800 ... step 801 loss 2.18682527542\n",
      "Iteration 801 ... step 802 loss 2.23490476608\n",
      "Iteration 802 ... step 803 loss 2.48675203323\n",
      "Iteration 803 ... step 804 loss 2.13011169434\n",
      "Iteration 804 ... step 805 loss 2.40576386452\n",
      "Iteration 805 ... step 806 loss 2.28257036209\n",
      "Iteration 806 ... step 807 loss 2.17930960655\n",
      "Iteration 807 ... step 808 loss 2.17214584351\n",
      "Iteration 808 ... step 809 loss 2.02340602875\n",
      "Iteration 809 ... step 810 loss 2.17661142349\n",
      "Iteration 810 ... step 811 loss 2.40619802475\n",
      "Iteration 811 ... step 812 loss 2.18898868561\n",
      "Iteration 812 ... step 813 loss 2.27533245087\n",
      "Iteration 813 ... step 814 loss 2.19665408134\n",
      "Iteration 814 ... step 815 loss 2.23474693298\n",
      "Iteration 815 ... step 816 loss 2.23224401474\n",
      "Iteration 816 ... step 817 loss 2.31725502014\n",
      "Iteration 817 ... step 818 loss 2.16234230995\n",
      "Iteration 818 ... step 819 loss 2.14225625992\n",
      "Iteration 819 ... step 820 loss 2.23485088348\n",
      "Iteration 820 ... step 821 loss 2.23801708221\n",
      "Iteration 821 ... step 822 loss 2.33345222473\n",
      "Iteration 822 ... step 823 loss 2.38478708267\n",
      "Iteration 823 ... step 824 loss 2.2472743988\n",
      "Iteration 824 ... step 825 loss 2.17337012291\n",
      "Iteration 825 ... step 826 loss 2.10339903831\n",
      "Iteration 826 ... step 827 loss 2.27160930634\n",
      "Iteration 827 ... step 828 loss 2.24851989746\n",
      "Iteration 828 ... step 829 loss 2.0907971859\n",
      "Iteration 829 ... step 830 loss 2.23612833023\n",
      "Iteration 830 ... step 831 loss 2.18493270874\n",
      "Iteration 831 ... step 832 loss 2.1480884552\n",
      "Iteration 832 ... step 833 loss 2.288022995\n",
      "Iteration 833 ... step 834 loss 2.04520654678\n",
      "Iteration 834 ... step 835 loss 2.25937008858\n",
      "Iteration 835 ... step 836 loss 2.31310319901\n",
      "Iteration 836 ... step 837 loss 2.34576153755\n",
      "Iteration 837 ... step 838 loss 2.32462644577\n",
      "Iteration 838 ... step 839 loss 2.22194361687\n",
      "Iteration 839 ... step 840 loss 2.36995577812\n",
      "Iteration 840 ... step 841 loss 2.25895810127\n",
      "Iteration 841 ... step 842 loss 2.2809472084\n",
      "Iteration 842 ... step 843 loss 2.21133899689\n",
      "Iteration 843 ... step 844 loss 2.22004938126\n",
      "Iteration 844 ... step 845 loss 2.16380023956\n",
      "Iteration 845 ... step 846 loss 2.03682208061\n",
      "Iteration 846 ... step 847 loss 2.12684869766\n",
      "Iteration 847 ... step 848 loss 2.13026237488\n",
      "Iteration 848 ... step 849 loss 2.26053476334\n",
      "Iteration 849 ... step 850 loss 2.32149934769\n",
      "Iteration 850 ... step 851 loss 2.13813591003\n",
      "Iteration 851 ... step 852 loss 2.27511286736\n",
      "Iteration 852 ... step 853 loss 2.20326900482\n",
      "Iteration 853 ... step 854 loss 2.18388366699\n",
      "Iteration 854 ... step 855 loss 2.21512651443\n",
      "Iteration 855 ... step 856 loss 2.15308785439\n",
      "Iteration 856 ... step 857 loss 2.17532920837\n",
      "Iteration 857 ... step 858 loss 2.2828578949\n",
      "Iteration 858 ... step 859 loss 2.34083127975\n",
      "Iteration 859 ... step 860 loss 2.47858953476\n",
      "Iteration 860 ... step 861 loss 2.19434857368\n",
      "Iteration 861 ... step 862 loss 2.32074737549\n",
      "Iteration 862 ... step 863 loss 2.17207551003\n",
      "Iteration 863 ... step 864 loss 2.35354328156\n",
      "Iteration 864 ... step 865 loss 2.1366045475\n",
      "Iteration 865 ... step 866 loss 2.44358778\n",
      "Iteration 866 ... step 867 loss 2.30139827728\n",
      "Iteration 867 ... step 868 loss 2.1477894783\n",
      "Iteration 868 ... step 869 loss 2.20891904831\n",
      "Iteration 869 ... step 870 loss 2.22534513474\n",
      "Iteration 870 ... step 871 loss 2.44451117516\n",
      "Iteration 871 ... step 872 loss 2.23163175583\n",
      "Iteration 872 ... step 873 loss 2.49166536331\n",
      "Iteration 873 ... step 874 loss 2.19817900658\n",
      "Iteration 874 ... step 875 loss 2.25935268402\n",
      "Iteration 875 ... step 876 loss 2.24781608582\n",
      "Iteration 876 ... step 877 loss 2.27316474915\n",
      "Iteration 877 ... step 878 loss 2.15947914124\n",
      "Iteration 878 ... step 879 loss 2.21430444717\n",
      "Iteration 879 ... step 880 loss 1.98063504696\n",
      "Iteration 880 ... step 881 loss 2.30398130417\n",
      "Iteration 881 ... step 882 loss 2.33033800125\n",
      "Iteration 882 ... step 883 loss 2.06615328789\n",
      "Iteration 883 ... step 884 loss 2.28006982803\n",
      "Iteration 884 ... step 885 loss 2.21144008636\n",
      "Iteration 885 ... step 886 loss 2.26970052719\n",
      "Iteration 886 ... step 887 loss 2.34524202347\n",
      "Iteration 887 ... step 888 loss 2.25422525406\n",
      "Iteration 888 ... step 889 loss 2.23257064819\n",
      "Iteration 889 ... step 890 loss 2.19241857529\n",
      "Iteration 890 ... step 891 loss 2.17747712135\n",
      "Iteration 891 ... step 892 loss 2.34411430359\n",
      "Iteration 892 ... step 893 loss 2.34344863892\n",
      "Iteration 893 ... step 894 loss 2.08294153214\n",
      "Iteration 894 ... step 895 loss 2.3554110527\n",
      "Iteration 895 ... step 896 loss 2.29301214218\n",
      "Iteration 896 ... step 897 loss 2.06933069229\n",
      "Iteration 897 ... step 898 loss 2.31292200089\n",
      "Iteration 898 ... step 899 loss 2.19467830658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 899 ... step 900 loss 2.04568910599\n",
      "Iteration 900 ... step 901 loss 2.13445568085\n",
      "Iteration 901 ... step 902 loss 2.33545875549\n",
      "Iteration 902 ... step 903 loss 2.31231307983\n",
      "Iteration 903 ... step 904 loss 2.25813484192\n",
      "Iteration 904 ... step 905 loss 2.32230424881\n",
      "Iteration 905 ... step 906 loss 2.3983335495\n",
      "Iteration 906 ... step 907 loss 2.21435832977\n",
      "Iteration 907 ... step 908 loss 2.17196178436\n",
      "Iteration 908 ... step 909 loss 2.29711508751\n",
      "Iteration 909 ... step 910 loss 2.24006700516\n",
      "Iteration 910 ... step 911 loss 2.44505929947\n",
      "Iteration 911 ... step 912 loss 2.1603076458\n",
      "Iteration 912 ... step 913 loss 2.3604388237\n",
      "Iteration 913 ... step 914 loss 2.20299959183\n",
      "Iteration 914 ... step 915 loss 2.21742486954\n",
      "Iteration 915 ... step 916 loss 2.17852473259\n",
      "Iteration 916 ... step 917 loss 2.31054830551\n",
      "Iteration 917 ... step 918 loss 2.21205425262\n",
      "Iteration 918 ... step 919 loss 2.17439007759\n",
      "Iteration 919 ... step 920 loss 2.20536708832\n",
      "Iteration 920 ... step 921 loss 2.02005052567\n",
      "Iteration 921 ... step 922 loss 2.21775197983\n",
      "Iteration 922 ... step 923 loss 2.14438319206\n",
      "Iteration 923 ... step 924 loss 2.25208044052\n",
      "Iteration 924 ... step 925 loss 2.24841928482\n",
      "Iteration 925 ... step 926 loss 2.21744060516\n",
      "Iteration 926 ... step 927 loss 2.20220994949\n",
      "Iteration 927 ... step 928 loss 2.15753936768\n",
      "Iteration 928 ... step 929 loss 2.1352558136\n",
      "Iteration 929 ... step 930 loss 2.20237350464\n",
      "Iteration 930 ... step 931 loss 2.27822804451\n",
      "Iteration 931 ... step 932 loss 2.19899082184\n",
      "Iteration 932 ... step 933 loss 2.30972766876\n",
      "Iteration 933 ... step 934 loss 2.19518995285\n",
      "Iteration 934 ... step 935 loss 2.26586031914\n",
      "Iteration 935 ... step 936 loss 2.15880918503\n",
      "Iteration 936 ... step 937 loss 2.02774572372\n",
      "Iteration 937 ... step 938 loss 2.24644613266\n",
      "Iteration 938 ... step 939 loss 2.36234378815\n",
      "Iteration 939 ... step 940 loss 2.30030250549\n",
      "Iteration 940 ... step 941 loss 2.30353879929\n",
      "Iteration 941 ... step 942 loss 2.11908102036\n",
      "Iteration 942 ... step 943 loss 2.4710791111\n",
      "Iteration 943 ... step 944 loss 2.20372867584\n",
      "Iteration 944 ... step 945 loss 2.41698646545\n",
      "Iteration 945 ... step 946 loss 2.29519581795\n",
      "Iteration 946 ... step 947 loss 2.32615613937\n",
      "Iteration 947 ... step 948 loss 2.24303293228\n",
      "Iteration 948 ... step 949 loss 2.48986268044\n",
      "Iteration 949 ... step 950 loss 2.1401450634\n",
      "Iteration 950 ... step 951 loss 2.41356158257\n",
      "Iteration 951 ... step 952 loss 2.30715274811\n",
      "Iteration 952 ... step 953 loss 2.37182974815\n",
      "Iteration 953 ... step 954 loss 2.21306180954\n",
      "Iteration 954 ... step 955 loss 2.02526521683\n",
      "Iteration 955 ... step 956 loss 2.32788944244\n",
      "Iteration 956 ... step 957 loss 2.25129032135\n",
      "Iteration 957 ... step 958 loss 2.41169118881\n",
      "Iteration 958 ... step 959 loss 2.39305686951\n",
      "Iteration 959 ... step 960 loss 2.22676515579\n",
      "Iteration 960 ... step 961 loss 2.19290709496\n",
      "Iteration 961 ... step 962 loss 2.23566269875\n",
      "Iteration 962 ... step 963 loss 2.06082296371\n",
      "Iteration 963 ... step 964 loss 2.54964470863\n",
      "Iteration 964 ... step 965 loss 2.31766319275\n",
      "Iteration 965 ... step 966 loss 2.26985359192\n",
      "Iteration 966 ... step 967 loss 2.20044088364\n",
      "Iteration 967 ... step 968 loss 2.33936929703\n",
      "Iteration 968 ... step 969 loss 2.45213985443\n",
      "Iteration 969 ... step 970 loss 2.30212450027\n",
      "Iteration 970 ... step 971 loss 2.44149589539\n",
      "Iteration 971 ... step 972 loss 2.16777682304\n",
      "Iteration 972 ... step 973 loss 2.22209024429\n",
      "Iteration 973 ... step 974 loss 2.30570411682\n",
      "Iteration 974 ... step 975 loss 2.11924242973\n",
      "Iteration 975 ... step 976 loss 2.42443418503\n",
      "Iteration 976 ... step 977 loss 2.48427391052\n",
      "Iteration 977 ... step 978 loss 2.35453104973\n",
      "Iteration 978 ... step 979 loss 2.34816598892\n",
      "Iteration 979 ... step 980 loss 2.2052500248\n",
      "Iteration 980 ... step 981 loss 2.23263669014\n",
      "Iteration 981 ... step 982 loss 2.23309898376\n",
      "Iteration 982 ... step 983 loss 2.2004070282\n",
      "Iteration 983 ... step 984 loss 2.43954372406\n",
      "Iteration 984 ... step 985 loss 2.35920476913\n",
      "Iteration 985 ... step 986 loss 2.23213863373\n",
      "Iteration 986 ... step 987 loss 2.26025056839\n",
      "Iteration 987 ... step 988 loss 2.11371636391\n",
      "Iteration 988 ... step 989 loss 2.23381614685\n",
      "Iteration 989 ... step 990 loss 2.25017404556\n",
      "Iteration 990 ... step 991 loss 2.41355276108\n",
      "Iteration 991 ... step 992 loss 2.08981657028\n",
      "Iteration 992 ... step 993 loss 2.22717952728\n",
      "Iteration 993 ... step 994 loss 2.21296644211\n",
      "Iteration 994 ... step 995 loss 2.02061843872\n",
      "Iteration 995 ... step 996 loss 2.25934457779\n",
      "Iteration 996 ... step 997 loss 2.3265080452\n",
      "Iteration 997 ... step 998 loss 2.2142124176\n",
      "Iteration 998 ... step 999 loss 2.34615945816\n",
      "Iteration 999 ... step 1000 loss 2.56982588768\n",
      "Iteration 1000 ... step 1001 loss 2.01582980156\n",
      "Minibatch loss at step 1000: 2.015830\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 1001 ... step 1002 loss 2.15686511993\n",
      "Iteration 1002 ... step 1003 loss 2.32428646088\n",
      "Iteration 1003 ... step 1004 loss 2.38118863106\n",
      "Iteration 1004 ... step 1005 loss 2.33592605591\n",
      "Iteration 1005 ... step 1006 loss 2.31586694717\n",
      "Iteration 1006 ... step 1007 loss 2.2450606823\n",
      "Iteration 1007 ... step 1008 loss 2.286236763\n",
      "Iteration 1008 ... step 1009 loss 2.23875284195\n",
      "Iteration 1009 ... step 1010 loss 2.02305793762\n",
      "Iteration 1010 ... step 1011 loss 2.30997753143\n",
      "Iteration 1011 ... step 1012 loss 2.07252693176\n",
      "Iteration 1012 ... step 1013 loss 2.28887891769\n",
      "Iteration 1013 ... step 1014 loss 2.34621953964\n",
      "Iteration 1014 ... step 1015 loss 2.19875097275\n",
      "Iteration 1015 ... step 1016 loss 2.25453519821\n",
      "Iteration 1016 ... step 1017 loss 2.22386598587\n",
      "Iteration 1017 ... step 1018 loss 2.1692481041\n",
      "Iteration 1018 ... step 1019 loss 2.13344454765\n",
      "Iteration 1019 ... step 1020 loss 2.31929516792\n",
      "Iteration 1020 ... step 1021 loss 2.24409556389\n",
      "Iteration 1021 ... step 1022 loss 2.27858400345\n",
      "Iteration 1022 ... step 1023 loss 2.18587350845\n",
      "Iteration 1023 ... step 1024 loss 2.22772789001\n",
      "Iteration 1024 ... step 1025 loss 2.21134710312\n",
      "Iteration 1025 ... step 1026 loss 2.17495083809\n",
      "Iteration 1026 ... step 1027 loss 2.35718727112\n",
      "Iteration 1027 ... step 1028 loss 2.35139989853\n",
      "Iteration 1028 ... step 1029 loss 2.144572258\n",
      "Iteration 1029 ... step 1030 loss 2.23550271988\n",
      "Iteration 1030 ... step 1031 loss 2.20476722717\n",
      "Iteration 1031 ... step 1032 loss 2.25563097\n",
      "Iteration 1032 ... step 1033 loss 2.24354839325\n",
      "Iteration 1033 ... step 1034 loss 2.05762243271\n",
      "Iteration 1034 ... step 1035 loss 2.31190776825\n",
      "Iteration 1035 ... step 1036 loss 2.06677055359\n",
      "Iteration 1036 ... step 1037 loss 2.38223028183\n",
      "Iteration 1037 ... step 1038 loss 2.25078964233\n",
      "Iteration 1038 ... step 1039 loss 1.94314575195\n",
      "Iteration 1039 ... step 1040 loss 2.39499235153\n",
      "Iteration 1040 ... step 1041 loss 2.15487718582\n",
      "Iteration 1041 ... step 1042 loss 2.39662075043\n",
      "Iteration 1042 ... step 1043 loss 2.32779121399\n",
      "Iteration 1043 ... step 1044 loss 2.14321804047\n",
      "Iteration 1044 ... step 1045 loss 2.22713756561\n",
      "Iteration 1045 ... step 1046 loss 2.29242324829\n",
      "Iteration 1046 ... step 1047 loss 2.1215057373\n",
      "Iteration 1047 ... step 1048 loss 2.24890232086\n",
      "Iteration 1048 ... step 1049 loss 2.1207690239\n",
      "Iteration 1049 ... step 1050 loss 2.11678886414\n",
      "Iteration 1050 ... step 1051 loss 2.30478954315\n",
      "Iteration 1051 ... step 1052 loss 2.3984131813\n",
      "Iteration 1052 ... step 1053 loss 2.54135465622\n",
      "Iteration 1053 ... step 1054 loss 1.98143482208\n",
      "Iteration 1054 ... step 1055 loss 2.35488653183\n",
      "Iteration 1055 ... step 1056 loss 2.46946334839\n",
      "Iteration 1056 ... step 1057 loss 2.16565465927\n",
      "Iteration 1057 ... step 1058 loss 2.48210525513\n",
      "Iteration 1058 ... step 1059 loss 2.23935365677\n",
      "Iteration 1059 ... step 1060 loss 2.35856747627\n",
      "Iteration 1060 ... step 1061 loss 2.23618984222\n",
      "Iteration 1061 ... step 1062 loss 2.08809995651\n",
      "Iteration 1062 ... step 1063 loss 2.36906242371\n",
      "Iteration 1063 ... step 1064 loss 2.2101688385\n",
      "Iteration 1064 ... step 1065 loss 2.25624799728\n",
      "Iteration 1065 ... step 1066 loss 2.29739975929\n",
      "Iteration 1066 ... step 1067 loss 2.21516561508\n",
      "Iteration 1067 ... step 1068 loss 2.44691801071\n",
      "Iteration 1068 ... step 1069 loss 2.3444890976\n",
      "Iteration 1069 ... step 1070 loss 2.2733540535\n",
      "Iteration 1070 ... step 1071 loss 2.11458134651\n",
      "Iteration 1071 ... step 1072 loss 2.18020606041\n",
      "Iteration 1072 ... step 1073 loss 2.43077230453\n",
      "Iteration 1073 ... step 1074 loss 2.28880500793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1074 ... step 1075 loss 2.16362166405\n",
      "Iteration 1075 ... step 1076 loss 2.29614210129\n",
      "Iteration 1076 ... step 1077 loss 2.32750320435\n",
      "Iteration 1077 ... step 1078 loss 2.38884854317\n",
      "Iteration 1078 ... step 1079 loss 2.29060935974\n",
      "Iteration 1079 ... step 1080 loss 2.32146453857\n",
      "Iteration 1080 ... step 1081 loss 2.26704359055\n",
      "Iteration 1081 ... step 1082 loss 2.20735144615\n",
      "Iteration 1082 ... step 1083 loss 2.22499537468\n",
      "Iteration 1083 ... step 1084 loss 2.34059095383\n",
      "Iteration 1084 ... step 1085 loss 2.29912376404\n",
      "Iteration 1085 ... step 1086 loss 2.10935020447\n",
      "Iteration 1086 ... step 1087 loss 2.33197307587\n",
      "Iteration 1087 ... step 1088 loss 2.26337909698\n",
      "Iteration 1088 ... step 1089 loss 2.36064100266\n",
      "Iteration 1089 ... step 1090 loss 2.43482017517\n",
      "Iteration 1090 ... step 1091 loss 2.42672252655\n",
      "Iteration 1091 ... step 1092 loss 2.35392618179\n",
      "Iteration 1092 ... step 1093 loss 2.1119351387\n",
      "Iteration 1093 ... step 1094 loss 2.13679170609\n",
      "Iteration 1094 ... step 1095 loss 2.24467086792\n",
      "Iteration 1095 ... step 1096 loss 2.23334574699\n",
      "Iteration 1096 ... step 1097 loss 2.37976074219\n",
      "Iteration 1097 ... step 1098 loss 2.15706205368\n",
      "Iteration 1098 ... step 1099 loss 2.50904941559\n",
      "Iteration 1099 ... step 1100 loss 2.11752820015\n",
      "Iteration 1100 ... step 1101 loss 2.26510190964\n",
      "Iteration 1101 ... step 1102 loss 2.26201796532\n",
      "Iteration 1102 ... step 1103 loss 2.38939428329\n",
      "Iteration 1103 ... step 1104 loss 2.26667833328\n",
      "Iteration 1104 ... step 1105 loss 2.31170034409\n",
      "Iteration 1105 ... step 1106 loss 2.48845386505\n",
      "Iteration 1106 ... step 1107 loss 2.15585136414\n",
      "Iteration 1107 ... step 1108 loss 2.48774003983\n",
      "Iteration 1108 ... step 1109 loss 2.20445585251\n",
      "Iteration 1109 ... step 1110 loss 2.23974084854\n",
      "Iteration 1110 ... step 1111 loss 2.40200042725\n",
      "Iteration 1111 ... step 1112 loss 2.24038314819\n",
      "Iteration 1112 ... step 1113 loss 2.15969562531\n",
      "Iteration 1113 ... step 1114 loss 2.50917077065\n",
      "Iteration 1114 ... step 1115 loss 2.47442722321\n",
      "Iteration 1115 ... step 1116 loss 2.18684530258\n",
      "Iteration 1116 ... step 1117 loss 2.33011007309\n",
      "Iteration 1117 ... step 1118 loss 2.33271598816\n",
      "Iteration 1118 ... step 1119 loss 2.19424343109\n",
      "Iteration 1119 ... step 1120 loss 2.40622282028\n",
      "Iteration 1120 ... step 1121 loss 2.16504120827\n",
      "Iteration 1121 ... step 1122 loss 2.40880489349\n",
      "Iteration 1122 ... step 1123 loss 2.2321600914\n",
      "Iteration 1123 ... step 1124 loss 2.24761629105\n",
      "Iteration 1124 ... step 1125 loss 2.03682303429\n",
      "Iteration 1125 ... step 1126 loss 2.29036474228\n",
      "Iteration 1126 ... step 1127 loss 2.13334178925\n",
      "Iteration 1127 ... step 1128 loss 2.24089193344\n",
      "Iteration 1128 ... step 1129 loss 2.28908395767\n",
      "Iteration 1129 ... step 1130 loss 2.26303005219\n",
      "Iteration 1130 ... step 1131 loss 2.1583571434\n",
      "Iteration 1131 ... step 1132 loss 2.27137994766\n",
      "Iteration 1132 ... step 1133 loss 2.3023109436\n",
      "Iteration 1133 ... step 1134 loss 2.1912624836\n",
      "Iteration 1134 ... step 1135 loss 2.19432735443\n",
      "Iteration 1135 ... step 1136 loss 2.41511750221\n",
      "Iteration 1136 ... step 1137 loss 2.24517059326\n",
      "Iteration 1137 ... step 1138 loss 2.2409696579\n",
      "Iteration 1138 ... step 1139 loss 2.2187025547\n",
      "Iteration 1139 ... step 1140 loss 2.25589513779\n",
      "Iteration 1140 ... step 1141 loss 2.12194871902\n",
      "Iteration 1141 ... step 1142 loss 2.21609306335\n",
      "Iteration 1142 ... step 1143 loss 2.27505874634\n",
      "Iteration 1143 ... step 1144 loss 2.25729751587\n",
      "Iteration 1144 ... step 1145 loss 2.26829624176\n",
      "Iteration 1145 ... step 1146 loss 2.23670864105\n",
      "Iteration 1146 ... step 1147 loss 2.17790198326\n",
      "Iteration 1147 ... step 1148 loss 2.13051319122\n",
      "Iteration 1148 ... step 1149 loss 2.24012470245\n",
      "Iteration 1149 ... step 1150 loss 2.20576810837\n",
      "Iteration 1150 ... step 1151 loss 2.08735537529\n",
      "Iteration 1151 ... step 1152 loss 2.17090988159\n",
      "Iteration 1152 ... step 1153 loss 2.20511436462\n",
      "Iteration 1153 ... step 1154 loss 2.35880327225\n",
      "Iteration 1154 ... step 1155 loss 2.13900470734\n",
      "Iteration 1155 ... step 1156 loss 2.304479599\n",
      "Iteration 1156 ... step 1157 loss 2.14570307732\n",
      "Iteration 1157 ... step 1158 loss 2.33460712433\n",
      "Iteration 1158 ... step 1159 loss 2.22707700729\n",
      "Iteration 1159 ... step 1160 loss 2.05205821991\n",
      "Iteration 1160 ... step 1161 loss 2.40235471725\n",
      "Iteration 1161 ... step 1162 loss 2.13541507721\n",
      "Iteration 1162 ... step 1163 loss 2.39549398422\n",
      "Iteration 1163 ... step 1164 loss 2.40228462219\n",
      "Iteration 1164 ... step 1165 loss 2.20876646042\n",
      "Iteration 1165 ... step 1166 loss 2.15470576286\n",
      "Iteration 1166 ... step 1167 loss 2.36898732185\n",
      "Iteration 1167 ... step 1168 loss 2.23764801025\n",
      "Iteration 1168 ... step 1169 loss 2.2325387001\n",
      "Iteration 1169 ... step 1170 loss 2.19336509705\n",
      "Iteration 1170 ... step 1171 loss 2.198584795\n",
      "Iteration 1171 ... step 1172 loss 2.13990592957\n",
      "Iteration 1172 ... step 1173 loss 2.21104669571\n",
      "Iteration 1173 ... step 1174 loss 2.20268821716\n",
      "Iteration 1174 ... step 1175 loss 2.3108830452\n",
      "Iteration 1175 ... step 1176 loss 2.18468570709\n",
      "Iteration 1176 ... step 1177 loss 2.23512935638\n",
      "Iteration 1177 ... step 1178 loss 2.32435750961\n",
      "Iteration 1178 ... step 1179 loss 2.24678277969\n",
      "Iteration 1179 ... step 1180 loss 2.07609891891\n",
      "Iteration 1180 ... step 1181 loss 2.33548402786\n",
      "Iteration 1181 ... step 1182 loss 2.21215200424\n",
      "Iteration 1182 ... step 1183 loss 2.56205177307\n",
      "Iteration 1183 ... step 1184 loss 2.2711391449\n",
      "Iteration 1184 ... step 1185 loss 2.13033914566\n",
      "Iteration 1185 ... step 1186 loss 2.21630239487\n",
      "Iteration 1186 ... step 1187 loss 2.39562940598\n",
      "Iteration 1187 ... step 1188 loss 2.2496471405\n",
      "Iteration 1188 ... step 1189 loss 2.37531709671\n",
      "Iteration 1189 ... step 1190 loss 2.22942209244\n",
      "Iteration 1190 ... step 1191 loss 2.34091329575\n",
      "Iteration 1191 ... step 1192 loss 2.23441910744\n",
      "Iteration 1192 ... step 1193 loss 2.47687768936\n",
      "Iteration 1193 ... step 1194 loss 2.07347917557\n",
      "Iteration 1194 ... step 1195 loss 2.48786592484\n",
      "Iteration 1195 ... step 1196 loss 2.13288116455\n",
      "Iteration 1196 ... step 1197 loss 2.05027604103\n",
      "Iteration 1197 ... step 1198 loss 2.26544809341\n",
      "Iteration 1198 ... step 1199 loss 2.09693861008\n",
      "Iteration 1199 ... step 1200 loss 2.18123817444\n",
      "Iteration 1200 ... step 1201 loss 2.19579410553\n",
      "Iteration 1201 ... step 1202 loss 2.24556970596\n",
      "Iteration 1202 ... step 1203 loss 2.33750724792\n",
      "Iteration 1203 ... step 1204 loss 2.31767630577\n",
      "Iteration 1204 ... step 1205 loss 2.36771011353\n",
      "Iteration 1205 ... step 1206 loss 2.49887657166\n",
      "Iteration 1206 ... step 1207 loss 2.13735961914\n",
      "Iteration 1207 ... step 1208 loss 2.19886398315\n",
      "Iteration 1208 ... step 1209 loss 2.19882750511\n",
      "Iteration 1209 ... step 1210 loss 2.19651269913\n",
      "Iteration 1210 ... step 1211 loss 2.19601941109\n",
      "Iteration 1211 ... step 1212 loss 2.26077461243\n",
      "Iteration 1212 ... step 1213 loss 2.25711035728\n",
      "Iteration 1213 ... step 1214 loss 2.49066400528\n",
      "Iteration 1214 ... step 1215 loss 2.16324901581\n",
      "Iteration 1215 ... step 1216 loss 2.32079219818\n",
      "Iteration 1216 ... step 1217 loss 2.13864731789\n",
      "Iteration 1217 ... step 1218 loss 2.28031635284\n",
      "Iteration 1218 ... step 1219 loss 2.31564807892\n",
      "Iteration 1219 ... step 1220 loss 2.25657081604\n",
      "Iteration 1220 ... step 1221 loss 2.46742916107\n",
      "Iteration 1221 ... step 1222 loss 2.47525072098\n",
      "Iteration 1222 ... step 1223 loss 2.16172885895\n",
      "Iteration 1223 ... step 1224 loss 2.09449458122\n",
      "Iteration 1224 ... step 1225 loss 2.316655159\n",
      "Iteration 1225 ... step 1226 loss 2.4675078392\n",
      "Iteration 1226 ... step 1227 loss 2.32153940201\n",
      "Iteration 1227 ... step 1228 loss 2.02660369873\n",
      "Iteration 1228 ... step 1229 loss 2.19441175461\n",
      "Iteration 1229 ... step 1230 loss 2.23410892487\n",
      "Iteration 1230 ... step 1231 loss 2.24204969406\n",
      "Iteration 1231 ... step 1232 loss 2.20499849319\n",
      "Iteration 1232 ... step 1233 loss 2.10620594025\n",
      "Iteration 1233 ... step 1234 loss 2.16658616066\n",
      "Iteration 1234 ... step 1235 loss 2.17832493782\n",
      "Iteration 1235 ... step 1236 loss 2.42936086655\n",
      "Iteration 1236 ... step 1237 loss 2.32212114334\n",
      "Iteration 1237 ... step 1238 loss 2.1942949295\n",
      "Iteration 1238 ... step 1239 loss 2.09342861176\n",
      "Iteration 1239 ... step 1240 loss 2.17327356339\n",
      "Iteration 1240 ... step 1241 loss 2.22883963585\n",
      "Iteration 1241 ... step 1242 loss 2.26505661011\n",
      "Iteration 1242 ... step 1243 loss 2.21072006226\n",
      "Iteration 1243 ... step 1244 loss 2.21266222\n",
      "Iteration 1244 ... step 1245 loss 2.10074901581\n",
      "Iteration 1245 ... step 1246 loss 2.19539737701\n",
      "Iteration 1246 ... step 1247 loss 2.42279863358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1247 ... step 1248 loss 2.19613647461\n",
      "Iteration 1248 ... step 1249 loss 2.55565452576\n",
      "Iteration 1249 ... step 1250 loss 2.18450784683\n",
      "Iteration 1250 ... step 1251 loss 2.32734012604\n",
      "Iteration 1251 ... step 1252 loss 2.13210391998\n",
      "Iteration 1252 ... step 1253 loss 2.27593016624\n",
      "Iteration 1253 ... step 1254 loss 2.30617713928\n",
      "Iteration 1254 ... step 1255 loss 2.31864285469\n",
      "Iteration 1255 ... step 1256 loss 2.2869925499\n",
      "Iteration 1256 ... step 1257 loss 2.48539733887\n",
      "Iteration 1257 ... step 1258 loss 2.39488577843\n",
      "Iteration 1258 ... step 1259 loss 2.23161220551\n",
      "Iteration 1259 ... step 1260 loss 2.29228973389\n",
      "Iteration 1260 ... step 1261 loss 2.01138043404\n",
      "Iteration 1261 ... step 1262 loss 1.98119986057\n",
      "Iteration 1262 ... step 1263 loss 2.31374192238\n",
      "Iteration 1263 ... step 1264 loss 2.29148817062\n",
      "Iteration 1264 ... step 1265 loss 2.40335702896\n",
      "Iteration 1265 ... step 1266 loss 2.36938476562\n",
      "Iteration 1266 ... step 1267 loss 2.24142670631\n",
      "Iteration 1267 ... step 1268 loss 2.32318305969\n",
      "Iteration 1268 ... step 1269 loss 2.28364372253\n",
      "Iteration 1269 ... step 1270 loss 2.30948019028\n",
      "Iteration 1270 ... step 1271 loss 2.24819421768\n",
      "Iteration 1271 ... step 1272 loss 2.2390499115\n",
      "Iteration 1272 ... step 1273 loss 2.23563432693\n",
      "Iteration 1273 ... step 1274 loss 2.25109100342\n",
      "Iteration 1274 ... step 1275 loss 2.25431632996\n",
      "Iteration 1275 ... step 1276 loss 2.24088191986\n",
      "Iteration 1276 ... step 1277 loss 2.29613399506\n",
      "Iteration 1277 ... step 1278 loss 2.18526721001\n",
      "Iteration 1278 ... step 1279 loss 2.33050251007\n",
      "Iteration 1279 ... step 1280 loss 2.27290678024\n",
      "Iteration 1280 ... step 1281 loss 2.17665457726\n",
      "Iteration 1281 ... step 1282 loss 2.1520614624\n",
      "Iteration 1282 ... step 1283 loss 2.34791898727\n",
      "Iteration 1283 ... step 1284 loss 2.40158414841\n",
      "Iteration 1284 ... step 1285 loss 2.43724632263\n",
      "Iteration 1285 ... step 1286 loss 2.23455929756\n",
      "Iteration 1286 ... step 1287 loss 2.22202110291\n",
      "Iteration 1287 ... step 1288 loss 2.42311859131\n",
      "Iteration 1288 ... step 1289 loss 2.23090696335\n",
      "Iteration 1289 ... step 1290 loss 2.01598882675\n",
      "Iteration 1290 ... step 1291 loss 2.25956392288\n",
      "Iteration 1291 ... step 1292 loss 2.15146565437\n",
      "Iteration 1292 ... step 1293 loss 2.53438043594\n",
      "Iteration 1293 ... step 1294 loss 2.36473917961\n",
      "Iteration 1294 ... step 1295 loss 2.13612914085\n",
      "Iteration 1295 ... step 1296 loss 2.42339634895\n",
      "Iteration 1296 ... step 1297 loss 2.26053524017\n",
      "Iteration 1297 ... step 1298 loss 2.06453084946\n",
      "Iteration 1298 ... step 1299 loss 2.41295051575\n",
      "Iteration 1299 ... step 1300 loss 2.28795003891\n",
      "Iteration 1300 ... step 1301 loss 2.18786525726\n",
      "Iteration 1301 ... step 1302 loss 2.21936821938\n",
      "Iteration 1302 ... step 1303 loss 2.19804048538\n",
      "Iteration 1303 ... step 1304 loss 2.24401712418\n",
      "Iteration 1304 ... step 1305 loss 2.20314502716\n",
      "Iteration 1305 ... step 1306 loss 2.26028084755\n",
      "Iteration 1306 ... step 1307 loss 2.19533872604\n",
      "Iteration 1307 ... step 1308 loss 2.27563667297\n",
      "Iteration 1308 ... step 1309 loss 2.55683708191\n",
      "Iteration 1309 ... step 1310 loss 1.97791051865\n",
      "Iteration 1310 ... step 1311 loss 2.09618902206\n",
      "Iteration 1311 ... step 1312 loss 2.34715056419\n",
      "Iteration 1312 ... step 1313 loss 2.24289798737\n",
      "Iteration 1313 ... step 1314 loss 2.28620028496\n",
      "Iteration 1314 ... step 1315 loss 2.28879284859\n",
      "Iteration 1315 ... step 1316 loss 2.26817941666\n",
      "Iteration 1316 ... step 1317 loss 2.26001691818\n",
      "Iteration 1317 ... step 1318 loss 1.98742771149\n",
      "Iteration 1318 ... step 1319 loss 2.47257900238\n",
      "Iteration 1319 ... step 1320 loss 2.1873497963\n",
      "Iteration 1320 ... step 1321 loss 2.23578166962\n",
      "Iteration 1321 ... step 1322 loss 2.2790555954\n",
      "Iteration 1322 ... step 1323 loss 2.20158863068\n",
      "Iteration 1323 ... step 1324 loss 2.34480381012\n",
      "Iteration 1324 ... step 1325 loss 2.24715995789\n",
      "Iteration 1325 ... step 1326 loss 2.20236206055\n",
      "Iteration 1326 ... step 1327 loss 2.38085484505\n",
      "Iteration 1327 ... step 1328 loss 2.23441076279\n",
      "Iteration 1328 ... step 1329 loss 2.19434690475\n",
      "Iteration 1329 ... step 1330 loss 2.25618076324\n",
      "Iteration 1330 ... step 1331 loss 2.33094406128\n",
      "Iteration 1331 ... step 1332 loss 2.1044318676\n",
      "Iteration 1332 ... step 1333 loss 2.15369701385\n",
      "Iteration 1333 ... step 1334 loss 2.10482311249\n",
      "Iteration 1334 ... step 1335 loss 2.11052370071\n",
      "Iteration 1335 ... step 1336 loss 2.1727411747\n",
      "Iteration 1336 ... step 1337 loss 2.13619303703\n",
      "Iteration 1337 ... step 1338 loss 2.38780450821\n",
      "Iteration 1338 ... step 1339 loss 2.2214589119\n",
      "Iteration 1339 ... step 1340 loss 2.22973394394\n",
      "Iteration 1340 ... step 1341 loss 2.26505041122\n",
      "Iteration 1341 ... step 1342 loss 2.29430246353\n",
      "Iteration 1342 ... step 1343 loss 2.55526566505\n",
      "Iteration 1343 ... step 1344 loss 2.35355019569\n",
      "Iteration 1344 ... step 1345 loss 2.13875722885\n",
      "Iteration 1345 ... step 1346 loss 2.11872458458\n",
      "Iteration 1346 ... step 1347 loss 2.15894174576\n",
      "Iteration 1347 ... step 1348 loss 2.24614048004\n",
      "Iteration 1348 ... step 1349 loss 2.10955953598\n",
      "Iteration 1349 ... step 1350 loss 2.22578811646\n",
      "Iteration 1350 ... step 1351 loss 2.14964985847\n",
      "Iteration 1351 ... step 1352 loss 2.1846203804\n",
      "Iteration 1352 ... step 1353 loss 2.28731155396\n",
      "Iteration 1353 ... step 1354 loss 2.20382642746\n",
      "Iteration 1354 ... step 1355 loss 2.37407517433\n",
      "Iteration 1355 ... step 1356 loss 2.29769086838\n",
      "Iteration 1356 ... step 1357 loss 2.41654348373\n",
      "Iteration 1357 ... step 1358 loss 2.2877163887\n",
      "Iteration 1358 ... step 1359 loss 2.29023647308\n",
      "Iteration 1359 ... step 1360 loss 2.40496969223\n",
      "Iteration 1360 ... step 1361 loss 2.29549837112\n",
      "Iteration 1361 ... step 1362 loss 2.26750659943\n",
      "Iteration 1362 ... step 1363 loss 2.35162496567\n",
      "Iteration 1363 ... step 1364 loss 1.97956097126\n",
      "Iteration 1364 ... step 1365 loss 2.24936485291\n",
      "Iteration 1365 ... step 1366 loss 2.23125934601\n",
      "Iteration 1366 ... step 1367 loss 2.36069917679\n",
      "Iteration 1367 ... step 1368 loss 2.17552137375\n",
      "Iteration 1368 ... step 1369 loss 2.25921058655\n",
      "Iteration 1369 ... step 1370 loss 2.33187818527\n",
      "Iteration 1370 ... step 1371 loss 2.20362949371\n",
      "Iteration 1371 ... step 1372 loss 2.31365680695\n",
      "Iteration 1372 ... step 1373 loss 2.27826690674\n",
      "Iteration 1373 ... step 1374 loss 2.38570785522\n",
      "Iteration 1374 ... step 1375 loss 2.20138883591\n",
      "Iteration 1375 ... step 1376 loss 2.1822385788\n",
      "Iteration 1376 ... step 1377 loss 2.17062497139\n",
      "Iteration 1377 ... step 1378 loss 2.169724226\n",
      "Iteration 1378 ... step 1379 loss 2.27951669693\n",
      "Iteration 1379 ... step 1380 loss 2.34569644928\n",
      "Iteration 1380 ... step 1381 loss 2.48353624344\n",
      "Iteration 1381 ... step 1382 loss 2.14663791656\n",
      "Iteration 1382 ... step 1383 loss 2.26672744751\n",
      "Iteration 1383 ... step 1384 loss 2.29953479767\n",
      "Iteration 1384 ... step 1385 loss 2.35917901993\n",
      "Iteration 1385 ... step 1386 loss 2.36118221283\n",
      "Iteration 1386 ... step 1387 loss 2.1528646946\n",
      "Iteration 1387 ... step 1388 loss 2.08941555023\n",
      "Iteration 1388 ... step 1389 loss 2.37685275078\n",
      "Iteration 1389 ... step 1390 loss 2.16293334961\n",
      "Iteration 1390 ... step 1391 loss 2.33127307892\n",
      "Iteration 1391 ... step 1392 loss 2.04786300659\n",
      "Iteration 1392 ... step 1393 loss 2.15978240967\n",
      "Iteration 1393 ... step 1394 loss 2.34517765045\n",
      "Iteration 1394 ... step 1395 loss 2.32692718506\n",
      "Iteration 1395 ... step 1396 loss 2.15166211128\n",
      "Iteration 1396 ... step 1397 loss 2.38855886459\n",
      "Iteration 1397 ... step 1398 loss 2.16691303253\n",
      "Iteration 1398 ... step 1399 loss 2.35126495361\n",
      "Iteration 1399 ... step 1400 loss 2.16970133781\n",
      "Iteration 1400 ... step 1401 loss 2.25074267387\n",
      "Iteration 1401 ... step 1402 loss 2.25868701935\n",
      "Iteration 1402 ... step 1403 loss 2.06913518906\n",
      "Iteration 1403 ... step 1404 loss 2.34054088593\n",
      "Iteration 1404 ... step 1405 loss 2.22851085663\n",
      "Iteration 1405 ... step 1406 loss 2.27202820778\n",
      "Iteration 1406 ... step 1407 loss 2.42297482491\n",
      "Iteration 1407 ... step 1408 loss 2.20118188858\n",
      "Iteration 1408 ... step 1409 loss 2.25865840912\n",
      "Iteration 1409 ... step 1410 loss 2.31184148788\n",
      "Iteration 1410 ... step 1411 loss 2.14870047569\n",
      "Iteration 1411 ... step 1412 loss 2.11062955856\n",
      "Iteration 1412 ... step 1413 loss 2.26566410065\n",
      "Iteration 1413 ... step 1414 loss 2.39211559296\n",
      "Iteration 1414 ... step 1415 loss 2.26056146622\n",
      "Iteration 1415 ... step 1416 loss 2.09966421127\n",
      "Iteration 1416 ... step 1417 loss 2.27657818794\n",
      "Iteration 1417 ... step 1418 loss 2.38652944565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1418 ... step 1419 loss 2.11975479126\n",
      "Iteration 1419 ... step 1420 loss 2.25233650208\n",
      "Iteration 1420 ... step 1421 loss 2.33711385727\n",
      "Iteration 1421 ... step 1422 loss 2.24698734283\n",
      "Iteration 1422 ... step 1423 loss 2.37077260017\n",
      "Iteration 1423 ... step 1424 loss 2.22569274902\n",
      "Iteration 1424 ... step 1425 loss 2.0754981041\n",
      "Iteration 1425 ... step 1426 loss 2.11193442345\n",
      "Iteration 1426 ... step 1427 loss 2.40028595924\n",
      "Iteration 1427 ... step 1428 loss 2.0773434639\n",
      "Iteration 1428 ... step 1429 loss 2.09838628769\n",
      "Iteration 1429 ... step 1430 loss 2.10109472275\n",
      "Iteration 1430 ... step 1431 loss 2.18511724472\n",
      "Iteration 1431 ... step 1432 loss 2.01866579056\n",
      "Iteration 1432 ... step 1433 loss 2.22542619705\n",
      "Iteration 1433 ... step 1434 loss 2.16615915298\n",
      "Iteration 1434 ... step 1435 loss 2.17537975311\n",
      "Iteration 1435 ... step 1436 loss 2.05661821365\n",
      "Iteration 1436 ... step 1437 loss 2.17289590836\n",
      "Iteration 1437 ... step 1438 loss 2.31391096115\n",
      "Iteration 1438 ... step 1439 loss 2.27020740509\n",
      "Iteration 1439 ... step 1440 loss 2.16638278961\n",
      "Iteration 1440 ... step 1441 loss 2.27804899216\n",
      "Iteration 1441 ... step 1442 loss 2.16135263443\n",
      "Iteration 1442 ... step 1443 loss 2.30090761185\n",
      "Iteration 1443 ... step 1444 loss 2.19089508057\n",
      "Iteration 1444 ... step 1445 loss 2.35892486572\n",
      "Iteration 1445 ... step 1446 loss 2.07644534111\n",
      "Iteration 1446 ... step 1447 loss 2.23703551292\n",
      "Iteration 1447 ... step 1448 loss 2.31475114822\n",
      "Iteration 1448 ... step 1449 loss 2.11722683907\n",
      "Iteration 1449 ... step 1450 loss 2.41133403778\n",
      "Iteration 1450 ... step 1451 loss 2.07487845421\n",
      "Iteration 1451 ... step 1452 loss 2.28411340714\n",
      "Iteration 1452 ... step 1453 loss 2.21940755844\n",
      "Iteration 1453 ... step 1454 loss 2.21722412109\n",
      "Iteration 1454 ... step 1455 loss 2.17783856392\n",
      "Iteration 1455 ... step 1456 loss 2.26884460449\n",
      "Iteration 1456 ... step 1457 loss 2.18477416039\n",
      "Iteration 1457 ... step 1458 loss 2.12891435623\n",
      "Iteration 1458 ... step 1459 loss 2.22196388245\n",
      "Iteration 1459 ... step 1460 loss 2.19057059288\n",
      "Iteration 1460 ... step 1461 loss 2.27637624741\n",
      "Iteration 1461 ... step 1462 loss 2.34337234497\n",
      "Iteration 1462 ... step 1463 loss 2.11958575249\n",
      "Iteration 1463 ... step 1464 loss 2.06109523773\n",
      "Iteration 1464 ... step 1465 loss 2.20875406265\n",
      "Iteration 1465 ... step 1466 loss 2.33332419395\n",
      "Iteration 1466 ... step 1467 loss 2.01004695892\n",
      "Iteration 1467 ... step 1468 loss 2.12201547623\n",
      "Iteration 1468 ... step 1469 loss 2.3488972187\n",
      "Iteration 1469 ... step 1470 loss 2.09619379044\n",
      "Iteration 1470 ... step 1471 loss 2.40826129913\n",
      "Iteration 1471 ... step 1472 loss 2.26246738434\n",
      "Iteration 1472 ... step 1473 loss 2.23224782944\n",
      "Iteration 1473 ... step 1474 loss 2.09714746475\n",
      "Iteration 1474 ... step 1475 loss 2.35135006905\n",
      "Iteration 1475 ... step 1476 loss 2.2580704689\n",
      "Iteration 1476 ... step 1477 loss 2.22721910477\n",
      "Iteration 1477 ... step 1478 loss 2.20687150955\n",
      "Iteration 1478 ... step 1479 loss 2.32995176315\n",
      "Iteration 1479 ... step 1480 loss 2.30602741241\n",
      "Iteration 1480 ... step 1481 loss 2.21874761581\n",
      "Iteration 1481 ... step 1482 loss 2.2154853344\n",
      "Iteration 1482 ... step 1483 loss 2.24624013901\n",
      "Iteration 1483 ... step 1484 loss 2.15592002869\n",
      "Iteration 1484 ... step 1485 loss 2.22965049744\n",
      "Iteration 1485 ... step 1486 loss 2.35087060928\n",
      "Iteration 1486 ... step 1487 loss 2.21559715271\n",
      "Iteration 1487 ... step 1488 loss 2.28277921677\n",
      "Iteration 1488 ... step 1489 loss 2.21070098877\n",
      "Iteration 1489 ... step 1490 loss 2.12907266617\n",
      "Iteration 1490 ... step 1491 loss 2.17989015579\n",
      "Iteration 1491 ... step 1492 loss 2.28263545036\n",
      "Iteration 1492 ... step 1493 loss 2.20096206665\n",
      "Iteration 1493 ... step 1494 loss 2.31966376305\n",
      "Iteration 1494 ... step 1495 loss 2.07687973976\n",
      "Iteration 1495 ... step 1496 loss 2.13910722733\n",
      "Iteration 1496 ... step 1497 loss 2.36866545677\n",
      "Iteration 1497 ... step 1498 loss 2.30929040909\n",
      "Iteration 1498 ... step 1499 loss 2.19719076157\n",
      "Iteration 1499 ... step 1500 loss 2.29638719559\n",
      "Iteration 1500 ... step 1501 loss 2.4562420845\n",
      "Minibatch loss at step 1500: 2.456242\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 1501 ... step 1502 loss 2.11320877075\n",
      "Iteration 1502 ... step 1503 loss 2.06689453125\n",
      "Iteration 1503 ... step 1504 loss 2.31037473679\n",
      "Iteration 1504 ... step 1505 loss 2.09576225281\n",
      "Iteration 1505 ... step 1506 loss 2.06256771088\n",
      "Iteration 1506 ... step 1507 loss 2.31491947174\n",
      "Iteration 1507 ... step 1508 loss 2.44223976135\n",
      "Iteration 1508 ... step 1509 loss 2.22952795029\n",
      "Iteration 1509 ... step 1510 loss 2.16510438919\n",
      "Iteration 1510 ... step 1511 loss 2.32047271729\n",
      "Iteration 1511 ... step 1512 loss 2.09157085419\n",
      "Iteration 1512 ... step 1513 loss 2.25717496872\n",
      "Iteration 1513 ... step 1514 loss 2.32339382172\n",
      "Iteration 1514 ... step 1515 loss 2.36792421341\n",
      "Iteration 1515 ... step 1516 loss 2.071600914\n",
      "Iteration 1516 ... step 1517 loss 2.39742183685\n",
      "Iteration 1517 ... step 1518 loss 2.33770632744\n",
      "Iteration 1518 ... step 1519 loss 2.36338233948\n",
      "Iteration 1519 ... step 1520 loss 2.3362159729\n",
      "Iteration 1520 ... step 1521 loss 2.06708741188\n",
      "Iteration 1521 ... step 1522 loss 2.2254114151\n",
      "Iteration 1522 ... step 1523 loss 2.10202074051\n",
      "Iteration 1523 ... step 1524 loss 2.05114269257\n",
      "Iteration 1524 ... step 1525 loss 2.2567050457\n",
      "Iteration 1525 ... step 1526 loss 2.01609802246\n",
      "Iteration 1526 ... step 1527 loss 2.02071547508\n",
      "Iteration 1527 ... step 1528 loss 2.3418879509\n",
      "Iteration 1528 ... step 1529 loss 2.3194565773\n",
      "Iteration 1529 ... step 1530 loss 2.2917881012\n",
      "Iteration 1530 ... step 1531 loss 2.41678667068\n",
      "Iteration 1531 ... step 1532 loss 2.25168991089\n",
      "Iteration 1532 ... step 1533 loss 2.25996565819\n",
      "Iteration 1533 ... step 1534 loss 2.54740667343\n",
      "Iteration 1534 ... step 1535 loss 2.23579025269\n",
      "Iteration 1535 ... step 1536 loss 2.16518688202\n",
      "Iteration 1536 ... step 1537 loss 2.43403792381\n",
      "Iteration 1537 ... step 1538 loss 2.00173568726\n",
      "Iteration 1538 ... step 1539 loss 2.11953687668\n",
      "Iteration 1539 ... step 1540 loss 2.17702817917\n",
      "Iteration 1540 ... step 1541 loss 2.04010224342\n",
      "Iteration 1541 ... step 1542 loss 2.20261621475\n",
      "Iteration 1542 ... step 1543 loss 2.09030342102\n",
      "Iteration 1543 ... step 1544 loss 2.28929185867\n",
      "Iteration 1544 ... step 1545 loss 2.13446736336\n",
      "Iteration 1545 ... step 1546 loss 2.11522436142\n",
      "Iteration 1546 ... step 1547 loss 2.30474662781\n",
      "Iteration 1547 ... step 1548 loss 2.18258404732\n",
      "Iteration 1548 ... step 1549 loss 2.09179639816\n",
      "Iteration 1549 ... step 1550 loss 2.18570566177\n",
      "Iteration 1550 ... step 1551 loss 2.36677074432\n",
      "Iteration 1551 ... step 1552 loss 2.47171449661\n",
      "Iteration 1552 ... step 1553 loss 2.09109830856\n",
      "Iteration 1553 ... step 1554 loss 2.32430696487\n",
      "Iteration 1554 ... step 1555 loss 2.16848468781\n",
      "Iteration 1555 ... step 1556 loss 2.243932724\n",
      "Iteration 1556 ... step 1557 loss 2.31863999367\n",
      "Iteration 1557 ... step 1558 loss 2.22548627853\n",
      "Iteration 1558 ... step 1559 loss 2.28505587578\n",
      "Iteration 1559 ... step 1560 loss 2.4324901104\n",
      "Iteration 1560 ... step 1561 loss 2.13045692444\n",
      "Iteration 1561 ... step 1562 loss 2.38612174988\n",
      "Iteration 1562 ... step 1563 loss 2.14853096008\n",
      "Iteration 1563 ... step 1564 loss 2.19226312637\n",
      "Iteration 1564 ... step 1565 loss 2.09235143661\n",
      "Iteration 1565 ... step 1566 loss 2.13350725174\n",
      "Iteration 1566 ... step 1567 loss 2.2350487709\n",
      "Iteration 1567 ... step 1568 loss 2.36572313309\n",
      "Iteration 1568 ... step 1569 loss 2.08963871002\n",
      "Iteration 1569 ... step 1570 loss 2.18737959862\n",
      "Iteration 1570 ... step 1571 loss 2.30863761902\n",
      "Iteration 1571 ... step 1572 loss 2.46862602234\n",
      "Iteration 1572 ... step 1573 loss 2.21685934067\n",
      "Iteration 1573 ... step 1574 loss 2.24234724045\n",
      "Iteration 1574 ... step 1575 loss 2.20513439178\n",
      "Iteration 1575 ... step 1576 loss 2.15456843376\n",
      "Iteration 1576 ... step 1577 loss 2.16820430756\n",
      "Iteration 1577 ... step 1578 loss 2.06162285805\n",
      "Iteration 1578 ... step 1579 loss 2.29320669174\n",
      "Iteration 1579 ... step 1580 loss 2.25799846649\n",
      "Iteration 1580 ... step 1581 loss 2.22542595863\n",
      "Iteration 1581 ... step 1582 loss 2.44016504288\n",
      "Iteration 1582 ... step 1583 loss 2.27520799637\n",
      "Iteration 1583 ... step 1584 loss 2.3956091404\n",
      "Iteration 1584 ... step 1585 loss 2.21655225754\n",
      "Iteration 1585 ... step 1586 loss 2.05660796165\n",
      "Iteration 1586 ... step 1587 loss 2.26177358627\n",
      "Iteration 1587 ... step 1588 loss 2.17588162422\n",
      "Iteration 1588 ... step 1589 loss 2.16770362854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1589 ... step 1590 loss 2.15720319748\n",
      "Iteration 1590 ... step 1591 loss 2.34645318985\n",
      "Iteration 1591 ... step 1592 loss 1.97025740147\n",
      "Iteration 1592 ... step 1593 loss 2.27169656754\n",
      "Iteration 1593 ... step 1594 loss 2.26496100426\n",
      "Iteration 1594 ... step 1595 loss 2.19305634499\n",
      "Iteration 1595 ... step 1596 loss 2.08966159821\n",
      "Iteration 1596 ... step 1597 loss 2.17739224434\n",
      "Iteration 1597 ... step 1598 loss 2.41609859467\n",
      "Iteration 1598 ... step 1599 loss 2.08195757866\n",
      "Iteration 1599 ... step 1600 loss 2.32801055908\n",
      "Iteration 1600 ... step 1601 loss 2.19943904877\n",
      "Iteration 1601 ... step 1602 loss 2.44313454628\n",
      "Iteration 1602 ... step 1603 loss 2.3197426796\n",
      "Iteration 1603 ... step 1604 loss 2.21550440788\n",
      "Iteration 1604 ... step 1605 loss 2.29719400406\n",
      "Iteration 1605 ... step 1606 loss 2.38955330849\n",
      "Iteration 1606 ... step 1607 loss 2.24503850937\n",
      "Iteration 1607 ... step 1608 loss 2.22900867462\n",
      "Iteration 1608 ... step 1609 loss 2.05180549622\n",
      "Iteration 1609 ... step 1610 loss 2.09487128258\n",
      "Iteration 1610 ... step 1611 loss 2.18641853333\n",
      "Iteration 1611 ... step 1612 loss 2.08228302002\n",
      "Iteration 1612 ... step 1613 loss 2.40569543839\n",
      "Iteration 1613 ... step 1614 loss 2.39183282852\n",
      "Iteration 1614 ... step 1615 loss 2.10891771317\n",
      "Iteration 1615 ... step 1616 loss 2.19468927383\n",
      "Iteration 1616 ... step 1617 loss 2.37546682358\n",
      "Iteration 1617 ... step 1618 loss 2.35447359085\n",
      "Iteration 1618 ... step 1619 loss 2.25356030464\n",
      "Iteration 1619 ... step 1620 loss 2.2205786705\n",
      "Iteration 1620 ... step 1621 loss 2.25044488907\n",
      "Iteration 1621 ... step 1622 loss 2.16647863388\n",
      "Iteration 1622 ... step 1623 loss 2.23770356178\n",
      "Iteration 1623 ... step 1624 loss 2.11545467377\n",
      "Iteration 1624 ... step 1625 loss 2.30817651749\n",
      "Iteration 1625 ... step 1626 loss 2.34324026108\n",
      "Iteration 1626 ... step 1627 loss 2.28160858154\n",
      "Iteration 1627 ... step 1628 loss 2.35040950775\n",
      "Iteration 1628 ... step 1629 loss 2.29332351685\n",
      "Iteration 1629 ... step 1630 loss 2.22744870186\n",
      "Iteration 1630 ... step 1631 loss 2.12420248985\n",
      "Iteration 1631 ... step 1632 loss 2.30718708038\n",
      "Iteration 1632 ... step 1633 loss 2.19922447205\n",
      "Iteration 1633 ... step 1634 loss 2.21940565109\n",
      "Iteration 1634 ... step 1635 loss 2.19422769547\n",
      "Iteration 1635 ... step 1636 loss 2.40226745605\n",
      "Iteration 1636 ... step 1637 loss 2.14781546593\n",
      "Iteration 1637 ... step 1638 loss 2.19750547409\n",
      "Iteration 1638 ... step 1639 loss 2.21016931534\n",
      "Iteration 1639 ... step 1640 loss 2.43673372269\n",
      "Iteration 1640 ... step 1641 loss 2.21185350418\n",
      "Iteration 1641 ... step 1642 loss 2.18819379807\n",
      "Iteration 1642 ... step 1643 loss 2.19398069382\n",
      "Iteration 1643 ... step 1644 loss 2.46411991119\n",
      "Iteration 1644 ... step 1645 loss 2.46578454971\n",
      "Iteration 1645 ... step 1646 loss 2.12605929375\n",
      "Iteration 1646 ... step 1647 loss 2.17640376091\n",
      "Iteration 1647 ... step 1648 loss 2.30034208298\n",
      "Iteration 1648 ... step 1649 loss 2.3840470314\n",
      "Iteration 1649 ... step 1650 loss 2.12487053871\n",
      "Iteration 1650 ... step 1651 loss 2.20971822739\n",
      "Iteration 1651 ... step 1652 loss 2.21898031235\n",
      "Iteration 1652 ... step 1653 loss 2.18745136261\n",
      "Iteration 1653 ... step 1654 loss 2.40470075607\n",
      "Iteration 1654 ... step 1655 loss 2.12106466293\n",
      "Iteration 1655 ... step 1656 loss 2.24893522263\n",
      "Iteration 1656 ... step 1657 loss 2.18506526947\n",
      "Iteration 1657 ... step 1658 loss 2.1956923008\n",
      "Iteration 1658 ... step 1659 loss 2.22600913048\n",
      "Iteration 1659 ... step 1660 loss 2.36089229584\n",
      "Iteration 1660 ... step 1661 loss 2.3502407074\n",
      "Iteration 1661 ... step 1662 loss 2.23272490501\n",
      "Iteration 1662 ... step 1663 loss 2.25972127914\n",
      "Iteration 1663 ... step 1664 loss 2.29826164246\n",
      "Iteration 1664 ... step 1665 loss 2.11126327515\n",
      "Iteration 1665 ... step 1666 loss 2.34759616852\n",
      "Iteration 1666 ... step 1667 loss 2.15593385696\n",
      "Iteration 1667 ... step 1668 loss 2.32655668259\n",
      "Iteration 1668 ... step 1669 loss 2.54803419113\n",
      "Iteration 1669 ... step 1670 loss 2.06927585602\n",
      "Iteration 1670 ... step 1671 loss 2.37318229675\n",
      "Iteration 1671 ... step 1672 loss 2.23198914528\n",
      "Iteration 1672 ... step 1673 loss 2.22974586487\n",
      "Iteration 1673 ... step 1674 loss 2.28823375702\n",
      "Iteration 1674 ... step 1675 loss 2.27901792526\n",
      "Iteration 1675 ... step 1676 loss 2.23456192017\n",
      "Iteration 1676 ... step 1677 loss 2.2855091095\n",
      "Iteration 1677 ... step 1678 loss 2.18120217323\n",
      "Iteration 1678 ... step 1679 loss 2.19192290306\n",
      "Iteration 1679 ... step 1680 loss 2.23679971695\n",
      "Iteration 1680 ... step 1681 loss 2.20302009583\n",
      "Iteration 1681 ... step 1682 loss 2.17139601707\n",
      "Iteration 1682 ... step 1683 loss 2.35375452042\n",
      "Iteration 1683 ... step 1684 loss 2.20877599716\n",
      "Iteration 1684 ... step 1685 loss 2.13027715683\n",
      "Iteration 1685 ... step 1686 loss 2.36088037491\n",
      "Iteration 1686 ... step 1687 loss 2.35049200058\n",
      "Iteration 1687 ... step 1688 loss 2.37456178665\n",
      "Iteration 1688 ... step 1689 loss 2.04135966301\n",
      "Iteration 1689 ... step 1690 loss 2.23838758469\n",
      "Iteration 1690 ... step 1691 loss 2.23095989227\n",
      "Iteration 1691 ... step 1692 loss 2.41241765022\n",
      "Iteration 1692 ... step 1693 loss 2.11467766762\n",
      "Iteration 1693 ... step 1694 loss 2.24713373184\n",
      "Iteration 1694 ... step 1695 loss 2.10000753403\n",
      "Iteration 1695 ... step 1696 loss 2.10848021507\n",
      "Iteration 1696 ... step 1697 loss 2.32685852051\n",
      "Iteration 1697 ... step 1698 loss 2.12336921692\n",
      "Iteration 1698 ... step 1699 loss 2.3402132988\n",
      "Iteration 1699 ... step 1700 loss 2.31564712524\n",
      "Iteration 1700 ... step 1701 loss 2.18264055252\n",
      "Iteration 1701 ... step 1702 loss 2.09586524963\n",
      "Iteration 1702 ... step 1703 loss 2.20801830292\n",
      "Iteration 1703 ... step 1704 loss 2.15612840652\n",
      "Iteration 1704 ... step 1705 loss 2.1637597084\n",
      "Iteration 1705 ... step 1706 loss 2.12289595604\n",
      "Iteration 1706 ... step 1707 loss 2.1748790741\n",
      "Iteration 1707 ... step 1708 loss 2.33183288574\n",
      "Iteration 1708 ... step 1709 loss 2.31198596954\n",
      "Iteration 1709 ... step 1710 loss 2.19907903671\n",
      "Iteration 1710 ... step 1711 loss 2.192735672\n",
      "Iteration 1711 ... step 1712 loss 2.33551311493\n",
      "Iteration 1712 ... step 1713 loss 2.14907312393\n",
      "Iteration 1713 ... step 1714 loss 2.40101718903\n",
      "Iteration 1714 ... step 1715 loss 2.16772913933\n",
      "Iteration 1715 ... step 1716 loss 2.37332820892\n",
      "Iteration 1716 ... step 1717 loss 2.32861351967\n",
      "Iteration 1717 ... step 1718 loss 2.33576393127\n",
      "Iteration 1718 ... step 1719 loss 2.21014547348\n",
      "Iteration 1719 ... step 1720 loss 2.16422200203\n",
      "Iteration 1720 ... step 1721 loss 2.12873005867\n",
      "Iteration 1721 ... step 1722 loss 2.18078351021\n",
      "Iteration 1722 ... step 1723 loss 2.25007152557\n",
      "Iteration 1723 ... step 1724 loss 2.32705974579\n",
      "Iteration 1724 ... step 1725 loss 2.26167583466\n",
      "Iteration 1725 ... step 1726 loss 2.31559371948\n",
      "Iteration 1726 ... step 1727 loss 2.15047883987\n",
      "Iteration 1727 ... step 1728 loss 2.26842164993\n",
      "Iteration 1728 ... step 1729 loss 2.2248544693\n",
      "Iteration 1729 ... step 1730 loss 2.06325984001\n",
      "Iteration 1730 ... step 1731 loss 2.18436670303\n",
      "Iteration 1731 ... step 1732 loss 2.31202220917\n",
      "Iteration 1732 ... step 1733 loss 2.26186299324\n",
      "Iteration 1733 ... step 1734 loss 2.34385919571\n",
      "Iteration 1734 ... step 1735 loss 2.07649850845\n",
      "Iteration 1735 ... step 1736 loss 2.23016643524\n",
      "Iteration 1736 ... step 1737 loss 2.21266889572\n",
      "Iteration 1737 ... step 1738 loss 2.08865237236\n",
      "Iteration 1738 ... step 1739 loss 2.17575001717\n",
      "Iteration 1739 ... step 1740 loss 2.07223939896\n",
      "Iteration 1740 ... step 1741 loss 2.29145479202\n",
      "Iteration 1741 ... step 1742 loss 2.20150899887\n",
      "Iteration 1742 ... step 1743 loss 2.15403032303\n",
      "Iteration 1743 ... step 1744 loss 2.4302444458\n",
      "Iteration 1744 ... step 1745 loss 2.14819145203\n",
      "Iteration 1745 ... step 1746 loss 2.36386823654\n",
      "Iteration 1746 ... step 1747 loss 2.19132471085\n",
      "Iteration 1747 ... step 1748 loss 2.28777718544\n",
      "Iteration 1748 ... step 1749 loss 2.21296691895\n",
      "Iteration 1749 ... step 1750 loss 2.30820083618\n",
      "Iteration 1750 ... step 1751 loss 2.07616376877\n",
      "Iteration 1751 ... step 1752 loss 2.32818555832\n",
      "Iteration 1752 ... step 1753 loss 2.30173492432\n",
      "Iteration 1753 ... step 1754 loss 2.23485183716\n",
      "Iteration 1754 ... step 1755 loss 2.10763502121\n",
      "Iteration 1755 ... step 1756 loss 2.18744421005\n",
      "Iteration 1756 ... step 1757 loss 2.25130224228\n",
      "Iteration 1757 ... step 1758 loss 2.25092720985\n",
      "Iteration 1758 ... step 1759 loss 2.19725894928\n",
      "Iteration 1759 ... step 1760 loss 2.33409786224\n",
      "Iteration 1760 ... step 1761 loss 2.24357414246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1761 ... step 1762 loss 2.32683205605\n",
      "Iteration 1762 ... step 1763 loss 2.12243080139\n",
      "Iteration 1763 ... step 1764 loss 2.30359983444\n",
      "Iteration 1764 ... step 1765 loss 1.97739386559\n",
      "Iteration 1765 ... step 1766 loss 2.210501194\n",
      "Iteration 1766 ... step 1767 loss 2.08643507957\n",
      "Iteration 1767 ... step 1768 loss 2.14822292328\n",
      "Iteration 1768 ... step 1769 loss 2.02947402\n",
      "Iteration 1769 ... step 1770 loss 2.20211935043\n",
      "Iteration 1770 ... step 1771 loss 2.33597946167\n",
      "Iteration 1771 ... step 1772 loss 2.12094497681\n",
      "Iteration 1772 ... step 1773 loss 2.17528247833\n",
      "Iteration 1773 ... step 1774 loss 2.15475821495\n",
      "Iteration 1774 ... step 1775 loss 2.2176861763\n",
      "Iteration 1775 ... step 1776 loss 2.2089715004\n",
      "Iteration 1776 ... step 1777 loss 2.38246774673\n",
      "Iteration 1777 ... step 1778 loss 2.48072767258\n",
      "Iteration 1778 ... step 1779 loss 2.07015252113\n",
      "Iteration 1779 ... step 1780 loss 2.30237388611\n",
      "Iteration 1780 ... step 1781 loss 2.17362236977\n",
      "Iteration 1781 ... step 1782 loss 2.23944497108\n",
      "Iteration 1782 ... step 1783 loss 2.17633295059\n",
      "Iteration 1783 ... step 1784 loss 2.16521644592\n",
      "Iteration 1784 ... step 1785 loss 2.351790905\n",
      "Iteration 1785 ... step 1786 loss 2.32528448105\n",
      "Iteration 1786 ... step 1787 loss 2.32709646225\n",
      "Iteration 1787 ... step 1788 loss 2.20317935944\n",
      "Iteration 1788 ... step 1789 loss 2.41516971588\n",
      "Iteration 1789 ... step 1790 loss 2.12645959854\n",
      "Iteration 1790 ... step 1791 loss 2.15590047836\n",
      "Iteration 1791 ... step 1792 loss 2.33622694016\n",
      "Iteration 1792 ... step 1793 loss 2.38508605957\n",
      "Iteration 1793 ... step 1794 loss 2.42233800888\n",
      "Iteration 1794 ... step 1795 loss 2.230946064\n",
      "Iteration 1795 ... step 1796 loss 2.182117939\n",
      "Iteration 1796 ... step 1797 loss 2.38142585754\n",
      "Iteration 1797 ... step 1798 loss 2.12350058556\n",
      "Iteration 1798 ... step 1799 loss 2.27696323395\n",
      "Iteration 1799 ... step 1800 loss 2.13971543312\n",
      "Iteration 1800 ... step 1801 loss 2.21729135513\n",
      "Iteration 1801 ... step 1802 loss 1.91894304752\n",
      "Iteration 1802 ... step 1803 loss 2.23760080338\n",
      "Iteration 1803 ... step 1804 loss 2.5357568264\n",
      "Iteration 1804 ... step 1805 loss 2.17542457581\n",
      "Iteration 1805 ... step 1806 loss 2.2062830925\n",
      "Iteration 1806 ... step 1807 loss 1.94197976589\n",
      "Iteration 1807 ... step 1808 loss 2.21805667877\n",
      "Iteration 1808 ... step 1809 loss 2.24588298798\n",
      "Iteration 1809 ... step 1810 loss 2.32844614983\n",
      "Iteration 1810 ... step 1811 loss 2.10594654083\n",
      "Iteration 1811 ... step 1812 loss 2.05077004433\n",
      "Iteration 1812 ... step 1813 loss 2.32311224937\n",
      "Iteration 1813 ... step 1814 loss 2.23404693604\n",
      "Iteration 1814 ... step 1815 loss 2.15425491333\n",
      "Iteration 1815 ... step 1816 loss 2.28130102158\n",
      "Iteration 1816 ... step 1817 loss 2.195333004\n",
      "Iteration 1817 ... step 1818 loss 2.31423282623\n",
      "Iteration 1818 ... step 1819 loss 2.25695633888\n",
      "Iteration 1819 ... step 1820 loss 1.93958592415\n",
      "Iteration 1820 ... step 1821 loss 2.4165096283\n",
      "Iteration 1821 ... step 1822 loss 2.16772317886\n",
      "Iteration 1822 ... step 1823 loss 2.13074827194\n",
      "Iteration 1823 ... step 1824 loss 2.08521461487\n",
      "Iteration 1824 ... step 1825 loss 2.3037571907\n",
      "Iteration 1825 ... step 1826 loss 2.08609127998\n",
      "Iteration 1826 ... step 1827 loss 2.23412632942\n",
      "Iteration 1827 ... step 1828 loss 2.35194587708\n",
      "Iteration 1828 ... step 1829 loss 2.28727889061\n",
      "Iteration 1829 ... step 1830 loss 2.33437299728\n",
      "Iteration 1830 ... step 1831 loss 2.32867336273\n",
      "Iteration 1831 ... step 1832 loss 2.18488788605\n",
      "Iteration 1832 ... step 1833 loss 2.07599520683\n",
      "Iteration 1833 ... step 1834 loss 2.21007728577\n",
      "Iteration 1834 ... step 1835 loss 2.17697668076\n",
      "Iteration 1835 ... step 1836 loss 2.15692901611\n",
      "Iteration 1836 ... step 1837 loss 2.37030076981\n",
      "Iteration 1837 ... step 1838 loss 2.42471432686\n",
      "Iteration 1838 ... step 1839 loss 2.29948091507\n",
      "Iteration 1839 ... step 1840 loss 2.26879882812\n",
      "Iteration 1840 ... step 1841 loss 2.28812551498\n",
      "Iteration 1841 ... step 1842 loss 2.45126581192\n",
      "Iteration 1842 ... step 1843 loss 2.25910949707\n",
      "Iteration 1843 ... step 1844 loss 2.41571235657\n",
      "Iteration 1844 ... step 1845 loss 2.18175601959\n",
      "Iteration 1845 ... step 1846 loss 2.2629160881\n",
      "Iteration 1846 ... step 1847 loss 2.35701322556\n",
      "Iteration 1847 ... step 1848 loss 2.16189980507\n",
      "Iteration 1848 ... step 1849 loss 2.12418937683\n",
      "Iteration 1849 ... step 1850 loss 2.26204967499\n",
      "Iteration 1850 ... step 1851 loss 2.24870610237\n",
      "Iteration 1851 ... step 1852 loss 2.3385643959\n",
      "Iteration 1852 ... step 1853 loss 2.12995386124\n",
      "Iteration 1853 ... step 1854 loss 2.21297645569\n",
      "Iteration 1854 ... step 1855 loss 2.17371201515\n",
      "Iteration 1855 ... step 1856 loss 2.27712011337\n",
      "Iteration 1856 ... step 1857 loss 2.36149311066\n",
      "Iteration 1857 ... step 1858 loss 2.09340834618\n",
      "Iteration 1858 ... step 1859 loss 2.10578727722\n",
      "Iteration 1859 ... step 1860 loss 2.18870306015\n",
      "Iteration 1860 ... step 1861 loss 2.15843081474\n",
      "Iteration 1861 ... step 1862 loss 2.34191656113\n",
      "Iteration 1862 ... step 1863 loss 2.14275574684\n",
      "Iteration 1863 ... step 1864 loss 2.29546189308\n",
      "Iteration 1864 ... step 1865 loss 2.21712446213\n",
      "Iteration 1865 ... step 1866 loss 2.19528627396\n",
      "Iteration 1866 ... step 1867 loss 2.14542150497\n",
      "Iteration 1867 ... step 1868 loss 2.11015152931\n",
      "Iteration 1868 ... step 1869 loss 2.21710252762\n",
      "Iteration 1869 ... step 1870 loss 2.09569716454\n",
      "Iteration 1870 ... step 1871 loss 2.08092689514\n",
      "Iteration 1871 ... step 1872 loss 2.31800174713\n",
      "Iteration 1872 ... step 1873 loss 2.03032684326\n",
      "Iteration 1873 ... step 1874 loss 2.25889801979\n",
      "Iteration 1874 ... step 1875 loss 2.20547866821\n",
      "Iteration 1875 ... step 1876 loss 2.11061000824\n",
      "Iteration 1876 ... step 1877 loss 2.39951705933\n",
      "Iteration 1877 ... step 1878 loss 2.17144966125\n",
      "Iteration 1878 ... step 1879 loss 2.24952554703\n",
      "Iteration 1879 ... step 1880 loss 2.22301197052\n",
      "Iteration 1880 ... step 1881 loss 2.13270139694\n",
      "Iteration 1881 ... step 1882 loss 2.36414337158\n",
      "Iteration 1882 ... step 1883 loss 2.06769371033\n",
      "Iteration 1883 ... step 1884 loss 2.28356266022\n",
      "Iteration 1884 ... step 1885 loss 2.31045103073\n",
      "Iteration 1885 ... step 1886 loss 2.34057569504\n",
      "Iteration 1886 ... step 1887 loss 2.25935935974\n",
      "Iteration 1887 ... step 1888 loss 2.11691570282\n",
      "Iteration 1888 ... step 1889 loss 2.19298553467\n",
      "Iteration 1889 ... step 1890 loss 2.17007708549\n",
      "Iteration 1890 ... step 1891 loss 2.11399269104\n",
      "Iteration 1891 ... step 1892 loss 2.06791257858\n",
      "Iteration 1892 ... step 1893 loss 2.36313199997\n",
      "Iteration 1893 ... step 1894 loss 2.34886264801\n",
      "Iteration 1894 ... step 1895 loss 2.31041526794\n",
      "Iteration 1895 ... step 1896 loss 2.22093772888\n",
      "Iteration 1896 ... step 1897 loss 2.18719148636\n",
      "Iteration 1897 ... step 1898 loss 2.20184612274\n",
      "Iteration 1898 ... step 1899 loss 2.1655421257\n",
      "Iteration 1899 ... step 1900 loss 2.23800516129\n",
      "Iteration 1900 ... step 1901 loss 2.37300348282\n",
      "Iteration 1901 ... step 1902 loss 2.24766635895\n",
      "Iteration 1902 ... step 1903 loss 2.27273201942\n",
      "Iteration 1903 ... step 1904 loss 2.12901163101\n",
      "Iteration 1904 ... step 1905 loss 2.34702825546\n",
      "Iteration 1905 ... step 1906 loss 2.28194999695\n",
      "Iteration 1906 ... step 1907 loss 2.26788139343\n",
      "Iteration 1907 ... step 1908 loss 2.22752833366\n",
      "Iteration 1908 ... step 1909 loss 2.11954927444\n",
      "Iteration 1909 ... step 1910 loss 2.21132063866\n",
      "Iteration 1910 ... step 1911 loss 2.29932641983\n",
      "Iteration 1911 ... step 1912 loss 2.14640188217\n",
      "Iteration 1912 ... step 1913 loss 2.19127750397\n",
      "Iteration 1913 ... step 1914 loss 2.40224552155\n",
      "Iteration 1914 ... step 1915 loss 2.19347858429\n",
      "Iteration 1915 ... step 1916 loss 2.11729097366\n",
      "Iteration 1916 ... step 1917 loss 2.28221321106\n",
      "Iteration 1917 ... step 1918 loss 2.1758377552\n",
      "Iteration 1918 ... step 1919 loss 2.2855014801\n",
      "Iteration 1919 ... step 1920 loss 2.20436668396\n",
      "Iteration 1920 ... step 1921 loss 2.16152000427\n",
      "Iteration 1921 ... step 1922 loss 2.0771420002\n",
      "Iteration 1922 ... step 1923 loss 2.18771290779\n",
      "Iteration 1923 ... step 1924 loss 2.24533247948\n",
      "Iteration 1924 ... step 1925 loss 2.1925201416\n",
      "Iteration 1925 ... step 1926 loss 2.39388513565\n",
      "Iteration 1926 ... step 1927 loss 2.1129322052\n",
      "Iteration 1927 ... step 1928 loss 2.07443237305\n",
      "Iteration 1928 ... step 1929 loss 2.25245523453\n",
      "Iteration 1929 ... step 1930 loss 2.08343577385\n",
      "Iteration 1930 ... step 1931 loss 2.07238674164\n",
      "Iteration 1931 ... step 1932 loss 2.26002311707\n",
      "Iteration 1932 ... step 1933 loss 2.1641330719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1933 ... step 1934 loss 2.29720449448\n",
      "Iteration 1934 ... step 1935 loss 2.47972416878\n",
      "Iteration 1935 ... step 1936 loss 2.19899368286\n",
      "Iteration 1936 ... step 1937 loss 2.11017155647\n",
      "Iteration 1937 ... step 1938 loss 2.36135292053\n",
      "Iteration 1938 ... step 1939 loss 2.39602994919\n",
      "Iteration 1939 ... step 1940 loss 2.20328712463\n",
      "Iteration 1940 ... step 1941 loss 2.2251534462\n",
      "Iteration 1941 ... step 1942 loss 2.16582298279\n",
      "Iteration 1942 ... step 1943 loss 2.31855273247\n",
      "Iteration 1943 ... step 1944 loss 2.40898227692\n",
      "Iteration 1944 ... step 1945 loss 2.28827190399\n",
      "Iteration 1945 ... step 1946 loss 2.13406682014\n",
      "Iteration 1946 ... step 1947 loss 2.05763053894\n",
      "Iteration 1947 ... step 1948 loss 2.29706382751\n",
      "Iteration 1948 ... step 1949 loss 2.17802596092\n",
      "Iteration 1949 ... step 1950 loss 2.2258849144\n",
      "Iteration 1950 ... step 1951 loss 1.99730944633\n",
      "Iteration 1951 ... step 1952 loss 1.99646043777\n",
      "Iteration 1952 ... step 1953 loss 2.27727842331\n",
      "Iteration 1953 ... step 1954 loss 2.25331830978\n",
      "Iteration 1954 ... step 1955 loss 2.2117562294\n",
      "Iteration 1955 ... step 1956 loss 2.18214917183\n",
      "Iteration 1956 ... step 1957 loss 2.27474308014\n",
      "Iteration 1957 ... step 1958 loss 2.456407547\n",
      "Iteration 1958 ... step 1959 loss 2.08869051933\n",
      "Iteration 1959 ... step 1960 loss 2.37318944931\n",
      "Iteration 1960 ... step 1961 loss 2.08692717552\n",
      "Iteration 1961 ... step 1962 loss 2.2254691124\n",
      "Iteration 1962 ... step 1963 loss 2.10492753983\n",
      "Iteration 1963 ... step 1964 loss 2.28692674637\n",
      "Iteration 1964 ... step 1965 loss 2.2722735405\n",
      "Iteration 1965 ... step 1966 loss 2.17947673798\n",
      "Iteration 1966 ... step 1967 loss 2.2632727623\n",
      "Iteration 1967 ... step 1968 loss 2.25331687927\n",
      "Iteration 1968 ... step 1969 loss 2.16238594055\n",
      "Iteration 1969 ... step 1970 loss 2.07729697227\n",
      "Iteration 1970 ... step 1971 loss 2.28959035873\n",
      "Iteration 1971 ... step 1972 loss 2.32997179031\n",
      "Iteration 1972 ... step 1973 loss 2.31273317337\n",
      "Iteration 1973 ... step 1974 loss 2.22675943375\n",
      "Iteration 1974 ... step 1975 loss 2.0651679039\n",
      "Iteration 1975 ... step 1976 loss 2.09920072556\n",
      "Iteration 1976 ... step 1977 loss 2.25068044662\n",
      "Iteration 1977 ... step 1978 loss 2.16130495071\n",
      "Iteration 1978 ... step 1979 loss 2.31683421135\n",
      "Iteration 1979 ... step 1980 loss 2.10107922554\n",
      "Iteration 1980 ... step 1981 loss 2.0911450386\n",
      "Iteration 1981 ... step 1982 loss 2.16432571411\n",
      "Iteration 1982 ... step 1983 loss 2.1378595829\n",
      "Iteration 1983 ... step 1984 loss 2.15465116501\n",
      "Iteration 1984 ... step 1985 loss 2.20105648041\n",
      "Iteration 1985 ... step 1986 loss 2.22229957581\n",
      "Iteration 1986 ... step 1987 loss 2.35556745529\n",
      "Iteration 1987 ... step 1988 loss 2.25767564774\n",
      "Iteration 1988 ... step 1989 loss 2.16631603241\n",
      "Iteration 1989 ... step 1990 loss 2.08918905258\n",
      "Iteration 1990 ... step 1991 loss 2.30703449249\n",
      "Iteration 1991 ... step 1992 loss 2.15902900696\n",
      "Iteration 1992 ... step 1993 loss 2.21823072433\n",
      "Iteration 1993 ... step 1994 loss 2.06819725037\n",
      "Iteration 1994 ... step 1995 loss 2.23637914658\n",
      "Iteration 1995 ... step 1996 loss 2.28933548927\n",
      "Iteration 1996 ... step 1997 loss 2.20430326462\n",
      "Iteration 1997 ... step 1998 loss 2.25131940842\n",
      "Iteration 1998 ... step 1999 loss 2.16580247879\n",
      "Iteration 1999 ... step 2000 loss 2.27816724777\n",
      "Iteration 2000 ... step 2001 loss 2.32652902603\n",
      "Minibatch loss at step 2000: 2.326529\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 2001 ... step 2002 loss 2.39113068581\n",
      "Iteration 2002 ... step 2003 loss 2.48512983322\n",
      "Iteration 2003 ... step 2004 loss 2.3065290451\n",
      "Iteration 2004 ... step 2005 loss 2.2702999115\n",
      "Iteration 2005 ... step 2006 loss 2.2396774292\n",
      "Iteration 2006 ... step 2007 loss 2.27670717239\n",
      "Iteration 2007 ... step 2008 loss 2.05541181564\n",
      "Iteration 2008 ... step 2009 loss 2.2492685318\n",
      "Iteration 2009 ... step 2010 loss 2.37568712234\n",
      "Iteration 2010 ... step 2011 loss 2.149310112\n",
      "Iteration 2011 ... step 2012 loss 2.27123880386\n",
      "Iteration 2012 ... step 2013 loss 2.10171556473\n",
      "Iteration 2013 ... step 2014 loss 2.23131608963\n",
      "Iteration 2014 ... step 2015 loss 2.10079193115\n",
      "Iteration 2015 ... step 2016 loss 2.29768610001\n",
      "Iteration 2016 ... step 2017 loss 2.23111057281\n",
      "Iteration 2017 ... step 2018 loss 2.14021158218\n",
      "Iteration 2018 ... step 2019 loss 2.10273885727\n",
      "Iteration 2019 ... step 2020 loss 2.39406013489\n",
      "Iteration 2020 ... step 2021 loss 2.31863927841\n",
      "Iteration 2021 ... step 2022 loss 2.39755177498\n",
      "Iteration 2022 ... step 2023 loss 2.24952363968\n",
      "Iteration 2023 ... step 2024 loss 2.08439159393\n",
      "Iteration 2024 ... step 2025 loss 2.1935005188\n",
      "Iteration 2025 ... step 2026 loss 2.27804994583\n",
      "Iteration 2026 ... step 2027 loss 2.24465942383\n",
      "Iteration 2027 ... step 2028 loss 2.36978054047\n",
      "Iteration 2028 ... step 2029 loss 2.35733127594\n",
      "Iteration 2029 ... step 2030 loss 2.1484246254\n",
      "Iteration 2030 ... step 2031 loss 2.42231178284\n",
      "Iteration 2031 ... step 2032 loss 2.38406038284\n",
      "Iteration 2032 ... step 2033 loss 2.2754817009\n",
      "Iteration 2033 ... step 2034 loss 2.30295085907\n",
      "Iteration 2034 ... step 2035 loss 2.09659433365\n",
      "Iteration 2035 ... step 2036 loss 2.18969488144\n",
      "Iteration 2036 ... step 2037 loss 2.28334760666\n",
      "Iteration 2037 ... step 2038 loss 2.10949039459\n",
      "Iteration 2038 ... step 2039 loss 2.28139686584\n",
      "Iteration 2039 ... step 2040 loss 2.35397386551\n",
      "Iteration 2040 ... step 2041 loss 2.14730644226\n",
      "Iteration 2041 ... step 2042 loss 2.12041163445\n",
      "Iteration 2042 ... step 2043 loss 2.36680030823\n",
      "Iteration 2043 ... step 2044 loss 2.15001583099\n",
      "Iteration 2044 ... step 2045 loss 2.06264090538\n",
      "Iteration 2045 ... step 2046 loss 2.43043899536\n",
      "Iteration 2046 ... step 2047 loss 2.1987285614\n",
      "Iteration 2047 ... step 2048 loss 2.29467773438\n",
      "Iteration 2048 ... step 2049 loss 2.36441707611\n",
      "Iteration 2049 ... step 2050 loss 2.2884554863\n",
      "Iteration 2050 ... step 2051 loss 2.21969747543\n",
      "Iteration 2051 ... step 2052 loss 2.28588628769\n",
      "Iteration 2052 ... step 2053 loss 2.02876281738\n",
      "Iteration 2053 ... step 2054 loss 2.27844047546\n",
      "Iteration 2054 ... step 2055 loss 2.28318309784\n",
      "Iteration 2055 ... step 2056 loss 2.25845766068\n",
      "Iteration 2056 ... step 2057 loss 2.2837061882\n",
      "Iteration 2057 ... step 2058 loss 2.20391654968\n",
      "Iteration 2058 ... step 2059 loss 2.22299909592\n",
      "Iteration 2059 ... step 2060 loss 2.19673681259\n",
      "Iteration 2060 ... step 2061 loss 2.09799981117\n",
      "Iteration 2061 ... step 2062 loss 2.32345104218\n",
      "Iteration 2062 ... step 2063 loss 2.14015126228\n",
      "Iteration 2063 ... step 2064 loss 2.05419683456\n",
      "Iteration 2064 ... step 2065 loss 2.20972442627\n",
      "Iteration 2065 ... step 2066 loss 2.32755947113\n",
      "Iteration 2066 ... step 2067 loss 2.30124378204\n",
      "Iteration 2067 ... step 2068 loss 2.22563052177\n",
      "Iteration 2068 ... step 2069 loss 2.2478685379\n",
      "Iteration 2069 ... step 2070 loss 2.15458583832\n",
      "Iteration 2070 ... step 2071 loss 2.22919416428\n",
      "Iteration 2071 ... step 2072 loss 2.32264184952\n",
      "Iteration 2072 ... step 2073 loss 2.29987812042\n",
      "Iteration 2073 ... step 2074 loss 2.32490301132\n",
      "Iteration 2074 ... step 2075 loss 2.2829990387\n",
      "Iteration 2075 ... step 2076 loss 2.27309346199\n",
      "Iteration 2076 ... step 2077 loss 2.28715991974\n",
      "Iteration 2077 ... step 2078 loss 2.20743322372\n",
      "Iteration 2078 ... step 2079 loss 2.25625395775\n",
      "Iteration 2079 ... step 2080 loss 2.00759935379\n",
      "Iteration 2080 ... step 2081 loss 2.10228633881\n",
      "Iteration 2081 ... step 2082 loss 2.23261404037\n",
      "Iteration 2082 ... step 2083 loss 2.15194606781\n",
      "Iteration 2083 ... step 2084 loss 2.30014276505\n",
      "Iteration 2084 ... step 2085 loss 2.1933889389\n",
      "Iteration 2085 ... step 2086 loss 1.99198234081\n",
      "Iteration 2086 ... step 2087 loss 2.2885556221\n",
      "Iteration 2087 ... step 2088 loss 2.23755669594\n",
      "Iteration 2088 ... step 2089 loss 2.23874473572\n",
      "Iteration 2089 ... step 2090 loss 2.10435390472\n",
      "Iteration 2090 ... step 2091 loss 2.15936326981\n",
      "Iteration 2091 ... step 2092 loss 2.34671473503\n",
      "Iteration 2092 ... step 2093 loss 2.2330057621\n",
      "Iteration 2093 ... step 2094 loss 2.01401925087\n",
      "Iteration 2094 ... step 2095 loss 2.25375986099\n",
      "Iteration 2095 ... step 2096 loss 2.15697622299\n",
      "Iteration 2096 ... step 2097 loss 2.21798610687\n",
      "Iteration 2097 ... step 2098 loss 2.02940535545\n",
      "Iteration 2098 ... step 2099 loss 2.26063966751\n",
      "Iteration 2099 ... step 2100 loss 2.45334720612\n",
      "Iteration 2100 ... step 2101 loss 2.16324400902\n",
      "Iteration 2101 ... step 2102 loss 2.20465612411\n",
      "Iteration 2102 ... step 2103 loss 2.18233776093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2103 ... step 2104 loss 2.232609272\n",
      "Iteration 2104 ... step 2105 loss 2.17280173302\n",
      "Iteration 2105 ... step 2106 loss 2.18861341476\n",
      "Iteration 2106 ... step 2107 loss 2.11566233635\n",
      "Iteration 2107 ... step 2108 loss 2.28121805191\n",
      "Iteration 2108 ... step 2109 loss 2.1528646946\n",
      "Iteration 2109 ... step 2110 loss 2.31488656998\n",
      "Iteration 2110 ... step 2111 loss 2.08280658722\n",
      "Iteration 2111 ... step 2112 loss 2.02409410477\n",
      "Iteration 2112 ... step 2113 loss 2.10273241997\n",
      "Iteration 2113 ... step 2114 loss 2.13823533058\n",
      "Iteration 2114 ... step 2115 loss 2.22038412094\n",
      "Iteration 2115 ... step 2116 loss 2.13261413574\n",
      "Iteration 2116 ... step 2117 loss 2.40101599693\n",
      "Iteration 2117 ... step 2118 loss 2.42436146736\n",
      "Iteration 2118 ... step 2119 loss 2.17359018326\n",
      "Iteration 2119 ... step 2120 loss 2.17465376854\n",
      "Iteration 2120 ... step 2121 loss 2.25154542923\n",
      "Iteration 2121 ... step 2122 loss 2.21365213394\n",
      "Iteration 2122 ... step 2123 loss 2.34218621254\n",
      "Iteration 2123 ... step 2124 loss 2.16564941406\n",
      "Iteration 2124 ... step 2125 loss 2.40555953979\n",
      "Iteration 2125 ... step 2126 loss 2.04614186287\n",
      "Iteration 2126 ... step 2127 loss 2.19968795776\n",
      "Iteration 2127 ... step 2128 loss 2.30177116394\n",
      "Iteration 2128 ... step 2129 loss 2.14855146408\n",
      "Iteration 2129 ... step 2130 loss 2.1419467926\n",
      "Iteration 2130 ... step 2131 loss 2.28266787529\n",
      "Iteration 2131 ... step 2132 loss 2.29915714264\n",
      "Iteration 2132 ... step 2133 loss 2.15633869171\n",
      "Iteration 2133 ... step 2134 loss 2.20776438713\n",
      "Iteration 2134 ... step 2135 loss 2.0969452858\n",
      "Iteration 2135 ... step 2136 loss 2.37480306625\n",
      "Iteration 2136 ... step 2137 loss 2.28199529648\n",
      "Iteration 2137 ... step 2138 loss 2.40188360214\n",
      "Iteration 2138 ... step 2139 loss 2.15055179596\n",
      "Iteration 2139 ... step 2140 loss 2.16572928429\n",
      "Iteration 2140 ... step 2141 loss 2.10113382339\n",
      "Iteration 2141 ... step 2142 loss 2.17476391792\n",
      "Iteration 2142 ... step 2143 loss 2.164498806\n",
      "Iteration 2143 ... step 2144 loss 2.35961484909\n",
      "Iteration 2144 ... step 2145 loss 2.31252479553\n",
      "Iteration 2145 ... step 2146 loss 2.1968998909\n",
      "Iteration 2146 ... step 2147 loss 2.06702470779\n",
      "Iteration 2147 ... step 2148 loss 2.36857271194\n",
      "Iteration 2148 ... step 2149 loss 2.1530251503\n",
      "Iteration 2149 ... step 2150 loss 2.25053310394\n",
      "Iteration 2150 ... step 2151 loss 2.42032384872\n",
      "Iteration 2151 ... step 2152 loss 2.29545593262\n",
      "Iteration 2152 ... step 2153 loss 2.14543032646\n",
      "Iteration 2153 ... step 2154 loss 2.36179828644\n",
      "Iteration 2154 ... step 2155 loss 2.22037029266\n",
      "Iteration 2155 ... step 2156 loss 2.15445065498\n",
      "Iteration 2156 ... step 2157 loss 2.49312901497\n",
      "Iteration 2157 ... step 2158 loss 2.23370075226\n",
      "Iteration 2158 ... step 2159 loss 2.11752176285\n",
      "Iteration 2159 ... step 2160 loss 2.17088460922\n",
      "Iteration 2160 ... step 2161 loss 2.44007778168\n",
      "Iteration 2161 ... step 2162 loss 2.14910149574\n",
      "Iteration 2162 ... step 2163 loss 2.24296212196\n",
      "Iteration 2163 ... step 2164 loss 2.18793725967\n",
      "Iteration 2164 ... step 2165 loss 2.1416516304\n",
      "Iteration 2165 ... step 2166 loss 2.24389696121\n",
      "Iteration 2166 ... step 2167 loss 2.32558679581\n",
      "Iteration 2167 ... step 2168 loss 2.24134731293\n",
      "Iteration 2168 ... step 2169 loss 2.33117103577\n",
      "Iteration 2169 ... step 2170 loss 2.21936321259\n",
      "Iteration 2170 ... step 2171 loss 2.17549943924\n",
      "Iteration 2171 ... step 2172 loss 2.10928440094\n",
      "Iteration 2172 ... step 2173 loss 2.2434720993\n",
      "Iteration 2173 ... step 2174 loss 2.26500940323\n",
      "Iteration 2174 ... step 2175 loss 2.32518005371\n",
      "Iteration 2175 ... step 2176 loss 2.32049608231\n",
      "Iteration 2176 ... step 2177 loss 2.26120138168\n",
      "Iteration 2177 ... step 2178 loss 2.35027503967\n",
      "Iteration 2178 ... step 2179 loss 2.30907011032\n",
      "Iteration 2179 ... step 2180 loss 2.21053409576\n",
      "Iteration 2180 ... step 2181 loss 2.15679454803\n",
      "Iteration 2181 ... step 2182 loss 2.00356650352\n",
      "Iteration 2182 ... step 2183 loss 2.17326688766\n",
      "Iteration 2183 ... step 2184 loss 2.14578151703\n",
      "Iteration 2184 ... step 2185 loss 2.26836538315\n",
      "Iteration 2185 ... step 2186 loss 2.24500131607\n",
      "Iteration 2186 ... step 2187 loss 2.06027603149\n",
      "Iteration 2187 ... step 2188 loss 2.27789449692\n",
      "Iteration 2188 ... step 2189 loss 2.14230251312\n",
      "Iteration 2189 ... step 2190 loss 2.40258026123\n",
      "Iteration 2190 ... step 2191 loss 2.26914262772\n",
      "Iteration 2191 ... step 2192 loss 2.10069131851\n",
      "Iteration 2192 ... step 2193 loss 2.26118659973\n",
      "Iteration 2193 ... step 2194 loss 2.32147407532\n",
      "Iteration 2194 ... step 2195 loss 2.08923196793\n",
      "Iteration 2195 ... step 2196 loss 2.17998623848\n",
      "Iteration 2196 ... step 2197 loss 2.33059692383\n",
      "Iteration 2197 ... step 2198 loss 2.08185863495\n",
      "Iteration 2198 ... step 2199 loss 2.13223695755\n",
      "Iteration 2199 ... step 2200 loss 1.9735584259\n",
      "Iteration 2200 ... step 2201 loss 2.41571617126\n",
      "Iteration 2201 ... step 2202 loss 2.20282077789\n",
      "Iteration 2202 ... step 2203 loss 2.28226041794\n",
      "Iteration 2203 ... step 2204 loss 2.25394773483\n",
      "Iteration 2204 ... step 2205 loss 2.11209392548\n",
      "Iteration 2205 ... step 2206 loss 2.27059316635\n",
      "Iteration 2206 ... step 2207 loss 2.08687448502\n",
      "Iteration 2207 ... step 2208 loss 2.34782361984\n",
      "Iteration 2208 ... step 2209 loss 2.14049291611\n",
      "Iteration 2209 ... step 2210 loss 2.16448736191\n",
      "Iteration 2210 ... step 2211 loss 2.24379491806\n",
      "Iteration 2211 ... step 2212 loss 2.2369992733\n",
      "Iteration 2212 ... step 2213 loss 2.21918964386\n",
      "Iteration 2213 ... step 2214 loss 2.28251409531\n",
      "Iteration 2214 ... step 2215 loss 2.23116350174\n",
      "Iteration 2215 ... step 2216 loss 2.36782407761\n",
      "Iteration 2216 ... step 2217 loss 2.14512515068\n",
      "Iteration 2217 ... step 2218 loss 2.31107902527\n",
      "Iteration 2218 ... step 2219 loss 2.22537946701\n",
      "Iteration 2219 ... step 2220 loss 2.1220934391\n",
      "Iteration 2220 ... step 2221 loss 2.29277944565\n",
      "Iteration 2221 ... step 2222 loss 2.18874168396\n",
      "Iteration 2222 ... step 2223 loss 2.3777256012\n",
      "Iteration 2223 ... step 2224 loss 2.16529273987\n",
      "Iteration 2224 ... step 2225 loss 2.25035762787\n",
      "Iteration 2225 ... step 2226 loss 2.09893250465\n",
      "Iteration 2226 ... step 2227 loss 2.42068696022\n",
      "Iteration 2227 ... step 2228 loss 2.31314516068\n",
      "Iteration 2228 ... step 2229 loss 2.09114980698\n",
      "Iteration 2229 ... step 2230 loss 2.18951749802\n",
      "Iteration 2230 ... step 2231 loss 2.08347034454\n",
      "Iteration 2231 ... step 2232 loss 2.1730966568\n",
      "Iteration 2232 ... step 2233 loss 2.50049638748\n",
      "Iteration 2233 ... step 2234 loss 2.0667386055\n",
      "Iteration 2234 ... step 2235 loss 2.38247919083\n",
      "Iteration 2235 ... step 2236 loss 2.13030052185\n",
      "Iteration 2236 ... step 2237 loss 2.14861226082\n",
      "Iteration 2237 ... step 2238 loss 2.07176232338\n",
      "Iteration 2238 ... step 2239 loss 2.07025337219\n",
      "Iteration 2239 ... step 2240 loss 2.3659453392\n",
      "Iteration 2240 ... step 2241 loss 2.11879062653\n",
      "Iteration 2241 ... step 2242 loss 2.34293985367\n",
      "Iteration 2242 ... step 2243 loss 2.34060049057\n",
      "Iteration 2243 ... step 2244 loss 2.11654376984\n",
      "Iteration 2244 ... step 2245 loss 2.14862322807\n",
      "Iteration 2245 ... step 2246 loss 2.12504768372\n",
      "Iteration 2246 ... step 2247 loss 2.06412744522\n",
      "Iteration 2247 ... step 2248 loss 2.43667149544\n",
      "Iteration 2248 ... step 2249 loss 2.08440303802\n",
      "Iteration 2249 ... step 2250 loss 2.10770058632\n",
      "Iteration 2250 ... step 2251 loss 2.27970218658\n",
      "Iteration 2251 ... step 2252 loss 2.14151287079\n",
      "Iteration 2252 ... step 2253 loss 2.38096046448\n",
      "Iteration 2253 ... step 2254 loss 2.30976343155\n",
      "Iteration 2254 ... step 2255 loss 2.33614301682\n",
      "Iteration 2255 ... step 2256 loss 2.23166418076\n",
      "Iteration 2256 ... step 2257 loss 2.12104511261\n",
      "Iteration 2257 ... step 2258 loss 2.19294309616\n",
      "Iteration 2258 ... step 2259 loss 2.29732847214\n",
      "Iteration 2259 ... step 2260 loss 2.09797286987\n",
      "Iteration 2260 ... step 2261 loss 2.27092218399\n",
      "Iteration 2261 ... step 2262 loss 2.11496376991\n",
      "Iteration 2262 ... step 2263 loss 2.20585560799\n",
      "Iteration 2263 ... step 2264 loss 2.2858300209\n",
      "Iteration 2264 ... step 2265 loss 2.24623346329\n",
      "Iteration 2265 ... step 2266 loss 2.14117336273\n",
      "Iteration 2266 ... step 2267 loss 1.94332325459\n",
      "Iteration 2267 ... step 2268 loss 2.52452278137\n",
      "Iteration 2268 ... step 2269 loss 2.16873407364\n",
      "Iteration 2269 ... step 2270 loss 2.14098477364\n",
      "Iteration 2270 ... step 2271 loss 1.97009623051\n",
      "Iteration 2271 ... step 2272 loss 2.31678557396\n",
      "Iteration 2272 ... step 2273 loss 2.16637659073\n",
      "Iteration 2273 ... step 2274 loss 2.27199411392\n",
      "Iteration 2274 ... step 2275 loss 2.32221508026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2275 ... step 2276 loss 2.32568502426\n",
      "Iteration 2276 ... step 2277 loss 2.39209079742\n",
      "Iteration 2277 ... step 2278 loss 2.21963214874\n",
      "Iteration 2278 ... step 2279 loss 2.24553060532\n",
      "Iteration 2279 ... step 2280 loss 2.13019156456\n",
      "Iteration 2280 ... step 2281 loss 2.1782374382\n",
      "Iteration 2281 ... step 2282 loss 2.29719495773\n",
      "Iteration 2282 ... step 2283 loss 2.11545848846\n",
      "Iteration 2283 ... step 2284 loss 2.32840013504\n",
      "Iteration 2284 ... step 2285 loss 2.29554224014\n",
      "Iteration 2285 ... step 2286 loss 2.06115245819\n",
      "Iteration 2286 ... step 2287 loss 2.10727739334\n",
      "Iteration 2287 ... step 2288 loss 2.2566576004\n",
      "Iteration 2288 ... step 2289 loss 2.23774528503\n",
      "Iteration 2289 ... step 2290 loss 2.22397565842\n",
      "Iteration 2290 ... step 2291 loss 2.3553917408\n",
      "Iteration 2291 ... step 2292 loss 1.98961853981\n",
      "Iteration 2292 ... step 2293 loss 2.27414941788\n",
      "Iteration 2293 ... step 2294 loss 2.26485967636\n",
      "Iteration 2294 ... step 2295 loss 2.27153921127\n",
      "Iteration 2295 ... step 2296 loss 2.47371006012\n",
      "Iteration 2296 ... step 2297 loss 2.0934779644\n",
      "Iteration 2297 ... step 2298 loss 2.31388902664\n",
      "Iteration 2298 ... step 2299 loss 2.18619680405\n",
      "Iteration 2299 ... step 2300 loss 2.16423082352\n",
      "Iteration 2300 ... step 2301 loss 2.37075853348\n",
      "Iteration 2301 ... step 2302 loss 2.30111217499\n",
      "Iteration 2302 ... step 2303 loss 1.94480109215\n",
      "Iteration 2303 ... step 2304 loss 2.05970001221\n",
      "Iteration 2304 ... step 2305 loss 2.16476583481\n",
      "Iteration 2305 ... step 2306 loss 2.21420431137\n",
      "Iteration 2306 ... step 2307 loss 2.18572711945\n",
      "Iteration 2307 ... step 2308 loss 2.19565176964\n",
      "Iteration 2308 ... step 2309 loss 2.27720928192\n",
      "Iteration 2309 ... step 2310 loss 2.0956492424\n",
      "Iteration 2310 ... step 2311 loss 2.43282222748\n",
      "Iteration 2311 ... step 2312 loss 2.10755872726\n",
      "Iteration 2312 ... step 2313 loss 2.21068000793\n",
      "Iteration 2313 ... step 2314 loss 2.32524394989\n",
      "Iteration 2314 ... step 2315 loss 2.25887203217\n",
      "Iteration 2315 ... step 2316 loss 2.15909028053\n",
      "Iteration 2316 ... step 2317 loss 2.35225391388\n",
      "Iteration 2317 ... step 2318 loss 2.15153694153\n",
      "Iteration 2318 ... step 2319 loss 2.10817837715\n",
      "Iteration 2319 ... step 2320 loss 2.22561502457\n",
      "Iteration 2320 ... step 2321 loss 2.27483081818\n",
      "Iteration 2321 ... step 2322 loss 2.53259325027\n",
      "Iteration 2322 ... step 2323 loss 2.16222572327\n",
      "Iteration 2323 ... step 2324 loss 2.1683011055\n",
      "Iteration 2324 ... step 2325 loss 2.16788434982\n",
      "Iteration 2325 ... step 2326 loss 2.20754623413\n",
      "Iteration 2326 ... step 2327 loss 2.18419027328\n",
      "Iteration 2327 ... step 2328 loss 2.17333126068\n",
      "Iteration 2328 ... step 2329 loss 2.12581443787\n",
      "Iteration 2329 ... step 2330 loss 2.10140275955\n",
      "Iteration 2330 ... step 2331 loss 2.30825281143\n",
      "Iteration 2331 ... step 2332 loss 2.32990980148\n",
      "Iteration 2332 ... step 2333 loss 2.05855560303\n",
      "Iteration 2333 ... step 2334 loss 2.09718990326\n",
      "Iteration 2334 ... step 2335 loss 2.08240413666\n",
      "Iteration 2335 ... step 2336 loss 2.19414758682\n",
      "Iteration 2336 ... step 2337 loss 2.12873363495\n",
      "Iteration 2337 ... step 2338 loss 2.07211089134\n",
      "Iteration 2338 ... step 2339 loss 2.35443544388\n",
      "Iteration 2339 ... step 2340 loss 2.09739637375\n",
      "Iteration 2340 ... step 2341 loss 2.07960724831\n",
      "Iteration 2341 ... step 2342 loss 2.22506856918\n",
      "Iteration 2342 ... step 2343 loss 2.2924554348\n",
      "Iteration 2343 ... step 2344 loss 2.1628921032\n",
      "Iteration 2344 ... step 2345 loss 2.37668561935\n",
      "Iteration 2345 ... step 2346 loss 2.18028211594\n",
      "Iteration 2346 ... step 2347 loss 2.21282577515\n",
      "Iteration 2347 ... step 2348 loss 2.21960449219\n",
      "Iteration 2348 ... step 2349 loss 2.03559398651\n",
      "Iteration 2349 ... step 2350 loss 2.39987134933\n",
      "Iteration 2350 ... step 2351 loss 2.06365084648\n",
      "Iteration 2351 ... step 2352 loss 2.31997299194\n",
      "Iteration 2352 ... step 2353 loss 2.16171646118\n",
      "Iteration 2353 ... step 2354 loss 2.17641448975\n",
      "Iteration 2354 ... step 2355 loss 2.20019340515\n",
      "Iteration 2355 ... step 2356 loss 2.28348612785\n",
      "Iteration 2356 ... step 2357 loss 2.33358430862\n",
      "Iteration 2357 ... step 2358 loss 2.32392215729\n",
      "Iteration 2358 ... step 2359 loss 2.16277360916\n",
      "Iteration 2359 ... step 2360 loss 2.21771764755\n",
      "Iteration 2360 ... step 2361 loss 2.20350313187\n",
      "Iteration 2361 ... step 2362 loss 2.15216612816\n",
      "Iteration 2362 ... step 2363 loss 2.25400686264\n",
      "Iteration 2363 ... step 2364 loss 2.28056907654\n",
      "Iteration 2364 ... step 2365 loss 2.16265845299\n",
      "Iteration 2365 ... step 2366 loss 2.28814029694\n",
      "Iteration 2366 ... step 2367 loss 2.12323284149\n",
      "Iteration 2367 ... step 2368 loss 2.16674804688\n",
      "Iteration 2368 ... step 2369 loss 2.07997941971\n",
      "Iteration 2369 ... step 2370 loss 2.40441846848\n",
      "Iteration 2370 ... step 2371 loss 2.22658491135\n",
      "Iteration 2371 ... step 2372 loss 2.17601490021\n",
      "Iteration 2372 ... step 2373 loss 2.24940156937\n",
      "Iteration 2373 ... step 2374 loss 2.12472200394\n",
      "Iteration 2374 ... step 2375 loss 2.18701314926\n",
      "Iteration 2375 ... step 2376 loss 2.25858545303\n",
      "Iteration 2376 ... step 2377 loss 2.13659262657\n",
      "Iteration 2377 ... step 2378 loss 2.4235496521\n",
      "Iteration 2378 ... step 2379 loss 2.21649169922\n",
      "Iteration 2379 ... step 2380 loss 2.26641559601\n",
      "Iteration 2380 ... step 2381 loss 2.19489955902\n",
      "Iteration 2381 ... step 2382 loss 2.24006319046\n",
      "Iteration 2382 ... step 2383 loss 2.28145551682\n",
      "Iteration 2383 ... step 2384 loss 2.24014949799\n",
      "Iteration 2384 ... step 2385 loss 2.25902414322\n",
      "Iteration 2385 ... step 2386 loss 2.24004173279\n",
      "Iteration 2386 ... step 2387 loss 2.33979535103\n",
      "Iteration 2387 ... step 2388 loss 2.1431388855\n",
      "Iteration 2388 ... step 2389 loss 2.12803149223\n",
      "Iteration 2389 ... step 2390 loss 2.07080221176\n",
      "Iteration 2390 ... step 2391 loss 2.19159388542\n",
      "Iteration 2391 ... step 2392 loss 2.06753516197\n",
      "Iteration 2392 ... step 2393 loss 2.25054550171\n",
      "Iteration 2393 ... step 2394 loss 2.05258131027\n",
      "Iteration 2394 ... step 2395 loss 2.30516767502\n",
      "Iteration 2395 ... step 2396 loss 2.4506444931\n",
      "Iteration 2396 ... step 2397 loss 2.27377033234\n",
      "Iteration 2397 ... step 2398 loss 2.10632658005\n",
      "Iteration 2398 ... step 2399 loss 2.32810783386\n",
      "Iteration 2399 ... step 2400 loss 2.48052740097\n",
      "Iteration 2400 ... step 2401 loss 2.21997261047\n",
      "Iteration 2401 ... step 2402 loss 2.19414997101\n",
      "Iteration 2402 ... step 2403 loss 2.49365091324\n",
      "Iteration 2403 ... step 2404 loss 2.15189981461\n",
      "Iteration 2404 ... step 2405 loss 2.05884671211\n",
      "Iteration 2405 ... step 2406 loss 1.97115850449\n",
      "Iteration 2406 ... step 2407 loss 2.32253360748\n",
      "Iteration 2407 ... step 2408 loss 2.32473731041\n",
      "Iteration 2408 ... step 2409 loss 2.17795419693\n",
      "Iteration 2409 ... step 2410 loss 2.21960401535\n",
      "Iteration 2410 ... step 2411 loss 2.24136400223\n",
      "Iteration 2411 ... step 2412 loss 2.36948490143\n",
      "Iteration 2412 ... step 2413 loss 2.21723270416\n",
      "Iteration 2413 ... step 2414 loss 2.16673588753\n",
      "Iteration 2414 ... step 2415 loss 2.19470977783\n",
      "Iteration 2415 ... step 2416 loss 2.12578678131\n",
      "Iteration 2416 ... step 2417 loss 2.1564104557\n",
      "Iteration 2417 ... step 2418 loss 2.21520161629\n",
      "Iteration 2418 ... step 2419 loss 2.2160885334\n",
      "Iteration 2419 ... step 2420 loss 2.48183393478\n",
      "Iteration 2420 ... step 2421 loss 2.14467167854\n",
      "Iteration 2421 ... step 2422 loss 2.05889368057\n",
      "Iteration 2422 ... step 2423 loss 2.40496635437\n",
      "Iteration 2423 ... step 2424 loss 2.15642094612\n",
      "Iteration 2424 ... step 2425 loss 2.16968655586\n",
      "Iteration 2425 ... step 2426 loss 2.15893125534\n",
      "Iteration 2426 ... step 2427 loss 2.36391592026\n",
      "Iteration 2427 ... step 2428 loss 2.08861708641\n",
      "Iteration 2428 ... step 2429 loss 2.2660381794\n",
      "Iteration 2429 ... step 2430 loss 2.09566140175\n",
      "Iteration 2430 ... step 2431 loss 2.15773868561\n",
      "Iteration 2431 ... step 2432 loss 2.27350568771\n",
      "Iteration 2432 ... step 2433 loss 2.23963117599\n",
      "Iteration 2433 ... step 2434 loss 2.07896208763\n",
      "Iteration 2434 ... step 2435 loss 2.24226236343\n",
      "Iteration 2435 ... step 2436 loss 2.20895814896\n",
      "Iteration 2436 ... step 2437 loss 2.13531756401\n",
      "Iteration 2437 ... step 2438 loss 2.22886228561\n",
      "Iteration 2438 ... step 2439 loss 2.32693958282\n",
      "Iteration 2439 ... step 2440 loss 2.23100233078\n",
      "Iteration 2440 ... step 2441 loss 2.09678316116\n",
      "Iteration 2441 ... step 2442 loss 2.35207223892\n",
      "Iteration 2442 ... step 2443 loss 2.06110954285\n",
      "Iteration 2443 ... step 2444 loss 2.20068430901\n",
      "Iteration 2444 ... step 2445 loss 2.2705540657\n",
      "Iteration 2445 ... step 2446 loss 2.44227457047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2446 ... step 2447 loss 2.21659326553\n",
      "Iteration 2447 ... step 2448 loss 2.29777860641\n",
      "Iteration 2448 ... step 2449 loss 2.30225276947\n",
      "Iteration 2449 ... step 2450 loss 2.13679695129\n",
      "Iteration 2450 ... step 2451 loss 2.27244925499\n",
      "Iteration 2451 ... step 2452 loss 2.18963718414\n",
      "Iteration 2452 ... step 2453 loss 2.27553129196\n",
      "Iteration 2453 ... step 2454 loss 2.14706945419\n",
      "Iteration 2454 ... step 2455 loss 2.1418390274\n",
      "Iteration 2455 ... step 2456 loss 1.99756920338\n",
      "Iteration 2456 ... step 2457 loss 2.34361314774\n",
      "Iteration 2457 ... step 2458 loss 2.27595257759\n",
      "Iteration 2458 ... step 2459 loss 2.14989995956\n",
      "Iteration 2459 ... step 2460 loss 2.1238720417\n",
      "Iteration 2460 ... step 2461 loss 2.09652400017\n",
      "Iteration 2461 ... step 2462 loss 2.24433851242\n",
      "Iteration 2462 ... step 2463 loss 2.33300089836\n",
      "Iteration 2463 ... step 2464 loss 2.14434790611\n",
      "Iteration 2464 ... step 2465 loss 2.11916065216\n",
      "Iteration 2465 ... step 2466 loss 2.18956756592\n",
      "Iteration 2466 ... step 2467 loss 2.1419262886\n",
      "Iteration 2467 ... step 2468 loss 2.2463092804\n",
      "Iteration 2468 ... step 2469 loss 2.21530294418\n",
      "Iteration 2469 ... step 2470 loss 2.21811914444\n",
      "Iteration 2470 ... step 2471 loss 2.31629252434\n",
      "Iteration 2471 ... step 2472 loss 2.02863788605\n",
      "Iteration 2472 ... step 2473 loss 2.16546368599\n",
      "Iteration 2473 ... step 2474 loss 2.22131109238\n",
      "Iteration 2474 ... step 2475 loss 2.08239555359\n",
      "Iteration 2475 ... step 2476 loss 2.00440001488\n",
      "Iteration 2476 ... step 2477 loss 2.23391246796\n",
      "Iteration 2477 ... step 2478 loss 2.28604412079\n",
      "Iteration 2478 ... step 2479 loss 2.2113609314\n",
      "Iteration 2479 ... step 2480 loss 2.23412561417\n",
      "Iteration 2480 ... step 2481 loss 2.23645782471\n",
      "Iteration 2481 ... step 2482 loss 2.18655872345\n",
      "Iteration 2482 ... step 2483 loss 2.17350387573\n",
      "Iteration 2483 ... step 2484 loss 2.20149803162\n",
      "Iteration 2484 ... step 2485 loss 2.34235191345\n",
      "Iteration 2485 ... step 2486 loss 2.17915129662\n",
      "Iteration 2486 ... step 2487 loss 2.15520167351\n",
      "Iteration 2487 ... step 2488 loss 2.2195751667\n",
      "Iteration 2488 ... step 2489 loss 2.20956230164\n",
      "Iteration 2489 ... step 2490 loss 2.02335548401\n",
      "Iteration 2490 ... step 2491 loss 2.17543172836\n",
      "Iteration 2491 ... step 2492 loss 2.27498054504\n",
      "Iteration 2492 ... step 2493 loss 2.29963827133\n",
      "Iteration 2493 ... step 2494 loss 2.16080951691\n",
      "Iteration 2494 ... step 2495 loss 2.20978212357\n",
      "Iteration 2495 ... step 2496 loss 2.22330093384\n",
      "Iteration 2496 ... step 2497 loss 2.04827404022\n",
      "Iteration 2497 ... step 2498 loss 2.21969175339\n",
      "Iteration 2498 ... step 2499 loss 2.10669207573\n",
      "Iteration 2499 ... step 2500 loss 2.11474370956\n",
      "Iteration 2500 ... step 2501 loss 2.10005784035\n",
      "Minibatch loss at step 2500: 2.100058\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 2501 ... step 2502 loss 2.2424030304\n",
      "Iteration 2502 ... step 2503 loss 2.23657131195\n",
      "Iteration 2503 ... step 2504 loss 2.09191155434\n",
      "Iteration 2504 ... step 2505 loss 2.24318552017\n",
      "Iteration 2505 ... step 2506 loss 2.43306112289\n",
      "Iteration 2506 ... step 2507 loss 2.16113710403\n",
      "Iteration 2507 ... step 2508 loss 2.18808078766\n",
      "Iteration 2508 ... step 2509 loss 2.17780256271\n",
      "Iteration 2509 ... step 2510 loss 2.11384153366\n",
      "Iteration 2510 ... step 2511 loss 2.12437105179\n",
      "Iteration 2511 ... step 2512 loss 2.22496509552\n",
      "Iteration 2512 ... step 2513 loss 2.07499170303\n",
      "Iteration 2513 ... step 2514 loss 2.20295071602\n",
      "Iteration 2514 ... step 2515 loss 2.1856970787\n",
      "Iteration 2515 ... step 2516 loss 2.21522331238\n",
      "Iteration 2516 ... step 2517 loss 2.15348744392\n",
      "Iteration 2517 ... step 2518 loss 2.18597221375\n",
      "Iteration 2518 ... step 2519 loss 2.32401585579\n",
      "Iteration 2519 ... step 2520 loss 2.22040081024\n",
      "Iteration 2520 ... step 2521 loss 2.17315626144\n",
      "Iteration 2521 ... step 2522 loss 2.23463201523\n",
      "Iteration 2522 ... step 2523 loss 2.0747756958\n",
      "Iteration 2523 ... step 2524 loss 2.30726146698\n",
      "Iteration 2524 ... step 2525 loss 2.22965240479\n",
      "Iteration 2525 ... step 2526 loss 2.10589027405\n",
      "Iteration 2526 ... step 2527 loss 2.24671649933\n",
      "Iteration 2527 ... step 2528 loss 2.22901344299\n",
      "Iteration 2528 ... step 2529 loss 2.27122855186\n",
      "Iteration 2529 ... step 2530 loss 2.12051033974\n",
      "Iteration 2530 ... step 2531 loss 2.28591394424\n",
      "Iteration 2531 ... step 2532 loss 2.12860059738\n",
      "Iteration 2532 ... step 2533 loss 2.21411299706\n",
      "Iteration 2533 ... step 2534 loss 2.18304586411\n",
      "Iteration 2534 ... step 2535 loss 2.11976909637\n",
      "Iteration 2535 ... step 2536 loss 2.33010101318\n",
      "Iteration 2536 ... step 2537 loss 2.29867696762\n",
      "Iteration 2537 ... step 2538 loss 2.42606759071\n",
      "Iteration 2538 ... step 2539 loss 2.2844452858\n",
      "Iteration 2539 ... step 2540 loss 2.21904826164\n",
      "Iteration 2540 ... step 2541 loss 1.99075555801\n",
      "Iteration 2541 ... step 2542 loss 2.27439141273\n",
      "Iteration 2542 ... step 2543 loss 2.11415481567\n",
      "Iteration 2543 ... step 2544 loss 2.27399969101\n",
      "Iteration 2544 ... step 2545 loss 2.15153169632\n",
      "Iteration 2545 ... step 2546 loss 2.3783082962\n",
      "Iteration 2546 ... step 2547 loss 2.15391349792\n",
      "Iteration 2547 ... step 2548 loss 2.07439088821\n",
      "Iteration 2548 ... step 2549 loss 2.29374027252\n",
      "Iteration 2549 ... step 2550 loss 2.18015670776\n",
      "Iteration 2550 ... step 2551 loss 2.0843975544\n",
      "Iteration 2551 ... step 2552 loss 2.25766086578\n",
      "Iteration 2552 ... step 2553 loss 2.30127811432\n",
      "Iteration 2553 ... step 2554 loss 2.15318059921\n",
      "Iteration 2554 ... step 2555 loss 2.12754154205\n",
      "Iteration 2555 ... step 2556 loss 2.16826176643\n",
      "Iteration 2556 ... step 2557 loss 2.20009827614\n",
      "Iteration 2557 ... step 2558 loss 2.15325069427\n",
      "Iteration 2558 ... step 2559 loss 2.16922092438\n",
      "Iteration 2559 ... step 2560 loss 2.16040754318\n",
      "Iteration 2560 ... step 2561 loss 2.27106785774\n",
      "Iteration 2561 ... step 2562 loss 2.34461832047\n",
      "Iteration 2562 ... step 2563 loss 2.17710876465\n",
      "Iteration 2563 ... step 2564 loss 2.1361656189\n",
      "Iteration 2564 ... step 2565 loss 2.36238479614\n",
      "Iteration 2565 ... step 2566 loss 2.31058645248\n",
      "Iteration 2566 ... step 2567 loss 2.05033731461\n",
      "Iteration 2567 ... step 2568 loss 2.20084762573\n",
      "Iteration 2568 ... step 2569 loss 1.97560071945\n",
      "Iteration 2569 ... step 2570 loss 2.18996143341\n",
      "Iteration 2570 ... step 2571 loss 2.16348099709\n",
      "Iteration 2571 ... step 2572 loss 2.1816444397\n",
      "Iteration 2572 ... step 2573 loss 2.3713312149\n",
      "Iteration 2573 ... step 2574 loss 2.12905597687\n",
      "Iteration 2574 ... step 2575 loss 2.22808122635\n",
      "Iteration 2575 ... step 2576 loss 2.36020565033\n",
      "Iteration 2576 ... step 2577 loss 2.25140810013\n",
      "Iteration 2577 ... step 2578 loss 2.3954744339\n",
      "Iteration 2578 ... step 2579 loss 2.19047212601\n",
      "Iteration 2579 ... step 2580 loss 2.13909053802\n",
      "Iteration 2580 ... step 2581 loss 2.11539340019\n",
      "Iteration 2581 ... step 2582 loss 2.11281633377\n",
      "Iteration 2582 ... step 2583 loss 2.09965324402\n",
      "Iteration 2583 ... step 2584 loss 2.30590391159\n",
      "Iteration 2584 ... step 2585 loss 1.95376110077\n",
      "Iteration 2585 ... step 2586 loss 2.22118806839\n",
      "Iteration 2586 ... step 2587 loss 2.1370806694\n",
      "Iteration 2587 ... step 2588 loss 2.20526599884\n",
      "Iteration 2588 ... step 2589 loss 2.12468218803\n",
      "Iteration 2589 ... step 2590 loss 2.29509782791\n",
      "Iteration 2590 ... step 2591 loss 2.11905813217\n",
      "Iteration 2591 ... step 2592 loss 2.06911993027\n",
      "Iteration 2592 ... step 2593 loss 2.07013654709\n",
      "Iteration 2593 ... step 2594 loss 2.00356888771\n",
      "Iteration 2594 ... step 2595 loss 2.24673938751\n",
      "Iteration 2595 ... step 2596 loss 2.03065991402\n",
      "Iteration 2596 ... step 2597 loss 2.19682502747\n",
      "Iteration 2597 ... step 2598 loss 2.13762235641\n",
      "Iteration 2598 ... step 2599 loss 2.2552728653\n",
      "Iteration 2599 ... step 2600 loss 2.22670269012\n",
      "Iteration 2600 ... step 2601 loss 2.27866172791\n",
      "Iteration 2601 ... step 2602 loss 2.27607297897\n",
      "Iteration 2602 ... step 2603 loss 2.26013898849\n",
      "Iteration 2603 ... step 2604 loss 2.16211509705\n",
      "Iteration 2604 ... step 2605 loss 2.27537155151\n",
      "Iteration 2605 ... step 2606 loss 2.3257727623\n",
      "Iteration 2606 ... step 2607 loss 2.2286863327\n",
      "Iteration 2607 ... step 2608 loss 2.21126890182\n",
      "Iteration 2608 ... step 2609 loss 2.20495581627\n",
      "Iteration 2609 ... step 2610 loss 2.2476670742\n",
      "Iteration 2610 ... step 2611 loss 2.13137793541\n",
      "Iteration 2611 ... step 2612 loss 2.11228489876\n",
      "Iteration 2612 ... step 2613 loss 2.45498490334\n",
      "Iteration 2613 ... step 2614 loss 2.3899307251\n",
      "Iteration 2614 ... step 2615 loss 2.28225374222\n",
      "Iteration 2615 ... step 2616 loss 2.05683326721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2616 ... step 2617 loss 2.16730713844\n",
      "Iteration 2617 ... step 2618 loss 2.30921363831\n",
      "Iteration 2618 ... step 2619 loss 2.17373180389\n",
      "Iteration 2619 ... step 2620 loss 2.32485055923\n",
      "Iteration 2620 ... step 2621 loss 2.15102529526\n",
      "Iteration 2621 ... step 2622 loss 2.18901157379\n",
      "Iteration 2622 ... step 2623 loss 2.19598770142\n",
      "Iteration 2623 ... step 2624 loss 2.12089681625\n",
      "Iteration 2624 ... step 2625 loss 2.19529008865\n",
      "Iteration 2625 ... step 2626 loss 2.04560112953\n",
      "Iteration 2626 ... step 2627 loss 2.17104244232\n",
      "Iteration 2627 ... step 2628 loss 2.46571922302\n",
      "Iteration 2628 ... step 2629 loss 2.18473911285\n",
      "Iteration 2629 ... step 2630 loss 2.37946534157\n",
      "Iteration 2630 ... step 2631 loss 2.27740883827\n",
      "Iteration 2631 ... step 2632 loss 2.26488304138\n",
      "Iteration 2632 ... step 2633 loss 2.25216913223\n",
      "Iteration 2633 ... step 2634 loss 2.23096108437\n",
      "Iteration 2634 ... step 2635 loss 2.15204596519\n",
      "Iteration 2635 ... step 2636 loss 2.225897789\n",
      "Iteration 2636 ... step 2637 loss 2.23968172073\n",
      "Iteration 2637 ... step 2638 loss 2.31854772568\n",
      "Iteration 2638 ... step 2639 loss 2.1172709465\n",
      "Iteration 2639 ... step 2640 loss 2.37225198746\n",
      "Iteration 2640 ... step 2641 loss 2.31779360771\n",
      "Iteration 2641 ... step 2642 loss 2.19176054001\n",
      "Iteration 2642 ... step 2643 loss 2.15499520302\n",
      "Iteration 2643 ... step 2644 loss 2.14199519157\n",
      "Iteration 2644 ... step 2645 loss 2.26763796806\n",
      "Iteration 2645 ... step 2646 loss 2.22000408173\n",
      "Iteration 2646 ... step 2647 loss 2.27557659149\n",
      "Iteration 2647 ... step 2648 loss 2.12803936005\n",
      "Iteration 2648 ... step 2649 loss 2.19961929321\n",
      "Iteration 2649 ... step 2650 loss 2.2896065712\n",
      "Iteration 2650 ... step 2651 loss 2.24822092056\n",
      "Iteration 2651 ... step 2652 loss 2.3443403244\n",
      "Iteration 2652 ... step 2653 loss 2.14774942398\n",
      "Iteration 2653 ... step 2654 loss 2.20016717911\n",
      "Iteration 2654 ... step 2655 loss 1.92100095749\n",
      "Iteration 2655 ... step 2656 loss 2.05761027336\n",
      "Iteration 2656 ... step 2657 loss 2.12983036041\n",
      "Iteration 2657 ... step 2658 loss 2.02225208282\n",
      "Iteration 2658 ... step 2659 loss 2.06332874298\n",
      "Iteration 2659 ... step 2660 loss 2.23014593124\n",
      "Iteration 2660 ... step 2661 loss 2.29275274277\n",
      "Iteration 2661 ... step 2662 loss 2.27973079681\n",
      "Iteration 2662 ... step 2663 loss 2.42754006386\n",
      "Iteration 2663 ... step 2664 loss 2.45120096207\n",
      "Iteration 2664 ... step 2665 loss 2.2759308815\n",
      "Iteration 2665 ... step 2666 loss 2.17423176765\n",
      "Iteration 2666 ... step 2667 loss 2.2102265358\n",
      "Iteration 2667 ... step 2668 loss 2.14301490784\n",
      "Iteration 2668 ... step 2669 loss 2.14372348785\n",
      "Iteration 2669 ... step 2670 loss 2.13071155548\n",
      "Iteration 2670 ... step 2671 loss 2.26152658463\n",
      "Iteration 2671 ... step 2672 loss 2.27198266983\n",
      "Iteration 2672 ... step 2673 loss 2.45385169983\n",
      "Iteration 2673 ... step 2674 loss 2.19147586823\n",
      "Iteration 2674 ... step 2675 loss 2.17126226425\n",
      "Iteration 2675 ... step 2676 loss 2.25355386734\n",
      "Iteration 2676 ... step 2677 loss 2.15767097473\n",
      "Iteration 2677 ... step 2678 loss 2.1841275692\n",
      "Iteration 2678 ... step 2679 loss 2.09659004211\n",
      "Iteration 2679 ... step 2680 loss 2.23904752731\n",
      "Iteration 2680 ... step 2681 loss 2.18493890762\n",
      "Iteration 2681 ... step 2682 loss 2.17677950859\n",
      "Iteration 2682 ... step 2683 loss 2.24382734299\n",
      "Iteration 2683 ... step 2684 loss 2.34497642517\n",
      "Iteration 2684 ... step 2685 loss 2.19712114334\n",
      "Iteration 2685 ... step 2686 loss 2.40977191925\n",
      "Iteration 2686 ... step 2687 loss 2.06475424767\n",
      "Iteration 2687 ... step 2688 loss 2.14573264122\n",
      "Iteration 2688 ... step 2689 loss 2.16730737686\n",
      "Iteration 2689 ... step 2690 loss 2.14589810371\n",
      "Iteration 2690 ... step 2691 loss 2.17857718468\n",
      "Iteration 2691 ... step 2692 loss 2.23330640793\n",
      "Iteration 2692 ... step 2693 loss 2.28326964378\n",
      "Iteration 2693 ... step 2694 loss 2.11816430092\n",
      "Iteration 2694 ... step 2695 loss 2.25726795197\n",
      "Iteration 2695 ... step 2696 loss 2.19778060913\n",
      "Iteration 2696 ... step 2697 loss 2.27232670784\n",
      "Iteration 2697 ... step 2698 loss 2.17761945724\n",
      "Iteration 2698 ... step 2699 loss 2.35039114952\n",
      "Iteration 2699 ... step 2700 loss 2.28985691071\n",
      "Iteration 2700 ... step 2701 loss 2.12345290184\n",
      "Iteration 2701 ... step 2702 loss 2.30378890038\n",
      "Iteration 2702 ... step 2703 loss 2.13315582275\n",
      "Iteration 2703 ... step 2704 loss 2.11726498604\n",
      "Iteration 2704 ... step 2705 loss 2.02267408371\n",
      "Iteration 2705 ... step 2706 loss 2.23586654663\n",
      "Iteration 2706 ... step 2707 loss 2.29869508743\n",
      "Iteration 2707 ... step 2708 loss 2.23509931564\n",
      "Iteration 2708 ... step 2709 loss 2.18638539314\n",
      "Iteration 2709 ... step 2710 loss 2.33964848518\n",
      "Iteration 2710 ... step 2711 loss 2.14618492126\n",
      "Iteration 2711 ... step 2712 loss 2.12680530548\n",
      "Iteration 2712 ... step 2713 loss 2.23119997978\n",
      "Iteration 2713 ... step 2714 loss 2.36627054214\n",
      "Iteration 2714 ... step 2715 loss 2.10844969749\n",
      "Iteration 2715 ... step 2716 loss 2.13130831718\n",
      "Iteration 2716 ... step 2717 loss 2.24897956848\n",
      "Iteration 2717 ... step 2718 loss 2.19921302795\n",
      "Iteration 2718 ... step 2719 loss 2.28310871124\n",
      "Iteration 2719 ... step 2720 loss 2.23818540573\n",
      "Iteration 2720 ... step 2721 loss 2.17310976982\n",
      "Iteration 2721 ... step 2722 loss 2.26664018631\n",
      "Iteration 2722 ... step 2723 loss 2.30726385117\n",
      "Iteration 2723 ... step 2724 loss 2.10133814812\n",
      "Iteration 2724 ... step 2725 loss 2.1511926651\n",
      "Iteration 2725 ... step 2726 loss 2.22436690331\n",
      "Iteration 2726 ... step 2727 loss 2.0540292263\n",
      "Iteration 2727 ... step 2728 loss 2.21743345261\n",
      "Iteration 2728 ... step 2729 loss 2.23921775818\n",
      "Iteration 2729 ... step 2730 loss 2.17557191849\n",
      "Iteration 2730 ... step 2731 loss 2.33822894096\n",
      "Iteration 2731 ... step 2732 loss 2.31921577454\n",
      "Iteration 2732 ... step 2733 loss 2.2888507843\n",
      "Iteration 2733 ... step 2734 loss 2.12116670609\n",
      "Iteration 2734 ... step 2735 loss 2.15160512924\n",
      "Iteration 2735 ... step 2736 loss 2.19211483002\n",
      "Iteration 2736 ... step 2737 loss 2.18402957916\n",
      "Iteration 2737 ... step 2738 loss 2.2430100441\n",
      "Iteration 2738 ... step 2739 loss 2.2707824707\n",
      "Iteration 2739 ... step 2740 loss 2.19656467438\n",
      "Iteration 2740 ... step 2741 loss 2.21691131592\n",
      "Iteration 2741 ... step 2742 loss 2.15814805031\n",
      "Iteration 2742 ... step 2743 loss 2.11725759506\n",
      "Iteration 2743 ... step 2744 loss 2.40023851395\n",
      "Iteration 2744 ... step 2745 loss 2.29552817345\n",
      "Iteration 2745 ... step 2746 loss 2.19780445099\n",
      "Iteration 2746 ... step 2747 loss 2.31121039391\n",
      "Iteration 2747 ... step 2748 loss 2.23875713348\n",
      "Iteration 2748 ... step 2749 loss 2.1593375206\n",
      "Iteration 2749 ... step 2750 loss 2.10160303116\n",
      "Iteration 2750 ... step 2751 loss 2.22054624557\n",
      "Iteration 2751 ... step 2752 loss 2.22402000427\n",
      "Iteration 2752 ... step 2753 loss 2.49909615517\n",
      "Iteration 2753 ... step 2754 loss 2.11924505234\n",
      "Iteration 2754 ... step 2755 loss 2.24216651917\n",
      "Iteration 2755 ... step 2756 loss 2.25307893753\n",
      "Iteration 2756 ... step 2757 loss 2.12144112587\n",
      "Iteration 2757 ... step 2758 loss 2.00404000282\n",
      "Iteration 2758 ... step 2759 loss 2.22754621506\n",
      "Iteration 2759 ... step 2760 loss 2.20500516891\n",
      "Iteration 2760 ... step 2761 loss 2.25377368927\n",
      "Iteration 2761 ... step 2762 loss 2.25564694405\n",
      "Iteration 2762 ... step 2763 loss 2.12633013725\n",
      "Iteration 2763 ... step 2764 loss 2.18577384949\n",
      "Iteration 2764 ... step 2765 loss 2.26077675819\n",
      "Iteration 2765 ... step 2766 loss 2.35602807999\n",
      "Iteration 2766 ... step 2767 loss 2.19190979004\n",
      "Iteration 2767 ... step 2768 loss 2.06320905685\n",
      "Iteration 2768 ... step 2769 loss 2.19371938705\n",
      "Iteration 2769 ... step 2770 loss 2.21499371529\n",
      "Iteration 2770 ... step 2771 loss 2.2340092659\n",
      "Iteration 2771 ... step 2772 loss 2.23185038567\n",
      "Iteration 2772 ... step 2773 loss 2.06419324875\n",
      "Iteration 2773 ... step 2774 loss 2.22368955612\n",
      "Iteration 2774 ... step 2775 loss 2.08203935623\n",
      "Iteration 2775 ... step 2776 loss 2.19700288773\n",
      "Iteration 2776 ... step 2777 loss 2.03407621384\n",
      "Iteration 2777 ... step 2778 loss 2.37390089035\n",
      "Iteration 2778 ... step 2779 loss 2.16877532005\n",
      "Iteration 2779 ... step 2780 loss 2.229367733\n",
      "Iteration 2780 ... step 2781 loss 2.32106018066\n",
      "Iteration 2781 ... step 2782 loss 2.00977897644\n",
      "Iteration 2782 ... step 2783 loss 2.19227027893\n",
      "Iteration 2783 ... step 2784 loss 2.29799222946\n",
      "Iteration 2784 ... step 2785 loss 2.22813558578\n",
      "Iteration 2785 ... step 2786 loss 2.20452976227\n",
      "Iteration 2786 ... step 2787 loss 2.18711924553\n",
      "Iteration 2787 ... step 2788 loss 2.20271253586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2788 ... step 2789 loss 2.0843744278\n",
      "Iteration 2789 ... step 2790 loss 2.24424409866\n",
      "Iteration 2790 ... step 2791 loss 2.25669622421\n",
      "Iteration 2791 ... step 2792 loss 2.03462314606\n",
      "Iteration 2792 ... step 2793 loss 2.21802854538\n",
      "Iteration 2793 ... step 2794 loss 2.21488857269\n",
      "Iteration 2794 ... step 2795 loss 2.16780996323\n",
      "Iteration 2795 ... step 2796 loss 2.2728741169\n",
      "Iteration 2796 ... step 2797 loss 2.22520542145\n",
      "Iteration 2797 ... step 2798 loss 2.31051254272\n",
      "Iteration 2798 ... step 2799 loss 2.14903640747\n",
      "Iteration 2799 ... step 2800 loss 2.14818906784\n",
      "Iteration 2800 ... step 2801 loss 2.19706583023\n",
      "Iteration 2801 ... step 2802 loss 2.24724340439\n",
      "Iteration 2802 ... step 2803 loss 2.21733760834\n",
      "Iteration 2803 ... step 2804 loss 2.38796520233\n",
      "Iteration 2804 ... step 2805 loss 2.31435346603\n",
      "Iteration 2805 ... step 2806 loss 2.20887756348\n",
      "Iteration 2806 ... step 2807 loss 2.29400014877\n",
      "Iteration 2807 ... step 2808 loss 2.21882081032\n",
      "Iteration 2808 ... step 2809 loss 2.32464027405\n",
      "Iteration 2809 ... step 2810 loss 2.1006963253\n",
      "Iteration 2810 ... step 2811 loss 2.26737308502\n",
      "Iteration 2811 ... step 2812 loss 2.23666477203\n",
      "Iteration 2812 ... step 2813 loss 2.21529603004\n",
      "Iteration 2813 ... step 2814 loss 2.35815620422\n",
      "Iteration 2814 ... step 2815 loss 2.10245680809\n",
      "Iteration 2815 ... step 2816 loss 2.15418982506\n",
      "Iteration 2816 ... step 2817 loss 2.2957034111\n",
      "Iteration 2817 ... step 2818 loss 2.00432181358\n",
      "Iteration 2818 ... step 2819 loss 2.42019319534\n",
      "Iteration 2819 ... step 2820 loss 2.06910419464\n",
      "Iteration 2820 ... step 2821 loss 2.12595033646\n",
      "Iteration 2821 ... step 2822 loss 2.2788567543\n",
      "Iteration 2822 ... step 2823 loss 2.41020345688\n",
      "Iteration 2823 ... step 2824 loss 2.09347915649\n",
      "Iteration 2824 ... step 2825 loss 2.2440392971\n",
      "Iteration 2825 ... step 2826 loss 2.15995597839\n",
      "Iteration 2826 ... step 2827 loss 2.29751777649\n",
      "Iteration 2827 ... step 2828 loss 2.24487733841\n",
      "Iteration 2828 ... step 2829 loss 2.13397789001\n",
      "Iteration 2829 ... step 2830 loss 2.09649825096\n",
      "Iteration 2830 ... step 2831 loss 2.23775291443\n",
      "Iteration 2831 ... step 2832 loss 2.23487329483\n",
      "Iteration 2832 ... step 2833 loss 2.31294441223\n",
      "Iteration 2833 ... step 2834 loss 2.23335552216\n",
      "Iteration 2834 ... step 2835 loss 2.06160211563\n",
      "Iteration 2835 ... step 2836 loss 2.13079547882\n",
      "Iteration 2836 ... step 2837 loss 2.40229105949\n",
      "Iteration 2837 ... step 2838 loss 2.108066082\n",
      "Iteration 2838 ... step 2839 loss 2.15455317497\n",
      "Iteration 2839 ... step 2840 loss 2.17341184616\n",
      "Iteration 2840 ... step 2841 loss 2.18429803848\n",
      "Iteration 2841 ... step 2842 loss 2.1117773056\n",
      "Iteration 2842 ... step 2843 loss 2.37000656128\n",
      "Iteration 2843 ... step 2844 loss 2.07037782669\n",
      "Iteration 2844 ... step 2845 loss 2.17979931831\n",
      "Iteration 2845 ... step 2846 loss 2.31197786331\n",
      "Iteration 2846 ... step 2847 loss 2.37161779404\n",
      "Iteration 2847 ... step 2848 loss 2.14390683174\n",
      "Iteration 2848 ... step 2849 loss 2.11796307564\n",
      "Iteration 2849 ... step 2850 loss 2.08407378197\n",
      "Iteration 2850 ... step 2851 loss 2.10392689705\n",
      "Iteration 2851 ... step 2852 loss 2.2947704792\n",
      "Iteration 2852 ... step 2853 loss 2.25161552429\n",
      "Iteration 2853 ... step 2854 loss 2.20404911041\n",
      "Iteration 2854 ... step 2855 loss 2.2120051384\n",
      "Iteration 2855 ... step 2856 loss 2.56504845619\n",
      "Iteration 2856 ... step 2857 loss 2.22135090828\n",
      "Iteration 2857 ... step 2858 loss 2.22798347473\n",
      "Iteration 2858 ... step 2859 loss 2.17787981033\n",
      "Iteration 2859 ... step 2860 loss 2.30714678764\n",
      "Iteration 2860 ... step 2861 loss 2.19280529022\n",
      "Iteration 2861 ... step 2862 loss 2.49052047729\n",
      "Iteration 2862 ... step 2863 loss 2.18449640274\n",
      "Iteration 2863 ... step 2864 loss 2.17609262466\n",
      "Iteration 2864 ... step 2865 loss 2.10862970352\n",
      "Iteration 2865 ... step 2866 loss 2.25144338608\n",
      "Iteration 2866 ... step 2867 loss 2.22950553894\n",
      "Iteration 2867 ... step 2868 loss 2.12725639343\n",
      "Iteration 2868 ... step 2869 loss 2.06824088097\n",
      "Iteration 2869 ... step 2870 loss 2.19021654129\n",
      "Iteration 2870 ... step 2871 loss 2.33746910095\n",
      "Iteration 2871 ... step 2872 loss 2.17229747772\n",
      "Iteration 2872 ... step 2873 loss 2.37234640121\n",
      "Iteration 2873 ... step 2874 loss 2.1683177948\n",
      "Iteration 2874 ... step 2875 loss 2.05430316925\n",
      "Iteration 2875 ... step 2876 loss 2.28365468979\n",
      "Iteration 2876 ... step 2877 loss 2.15724086761\n",
      "Iteration 2877 ... step 2878 loss 2.18614768982\n",
      "Iteration 2878 ... step 2879 loss 2.20870399475\n",
      "Iteration 2879 ... step 2880 loss 2.14328432083\n",
      "Iteration 2880 ... step 2881 loss 2.31503772736\n",
      "Iteration 2881 ... step 2882 loss 2.1849398613\n",
      "Iteration 2882 ... step 2883 loss 2.13665342331\n",
      "Iteration 2883 ... step 2884 loss 2.34495925903\n",
      "Iteration 2884 ... step 2885 loss 2.2826218605\n",
      "Iteration 2885 ... step 2886 loss 2.16624975204\n",
      "Iteration 2886 ... step 2887 loss 2.20760726929\n",
      "Iteration 2887 ... step 2888 loss 2.2059378624\n",
      "Iteration 2888 ... step 2889 loss 2.15466785431\n",
      "Iteration 2889 ... step 2890 loss 2.07152199745\n",
      "Iteration 2890 ... step 2891 loss 2.11358904839\n",
      "Iteration 2891 ... step 2892 loss 2.27530670166\n",
      "Iteration 2892 ... step 2893 loss 2.22671175003\n",
      "Iteration 2893 ... step 2894 loss 2.26714372635\n",
      "Iteration 2894 ... step 2895 loss 2.18535900116\n",
      "Iteration 2895 ... step 2896 loss 2.12521886826\n",
      "Iteration 2896 ... step 2897 loss 2.25288057327\n",
      "Iteration 2897 ... step 2898 loss 2.15930652618\n",
      "Iteration 2898 ... step 2899 loss 2.18835306168\n",
      "Iteration 2899 ... step 2900 loss 2.2629981041\n",
      "Iteration 2900 ... step 2901 loss 2.25821208954\n",
      "Iteration 2901 ... step 2902 loss 2.21406888962\n",
      "Iteration 2902 ... step 2903 loss 2.20567131042\n",
      "Iteration 2903 ... step 2904 loss 2.28581857681\n",
      "Iteration 2904 ... step 2905 loss 2.16279268265\n",
      "Iteration 2905 ... step 2906 loss 2.3051738739\n",
      "Iteration 2906 ... step 2907 loss 2.16554689407\n",
      "Iteration 2907 ... step 2908 loss 2.04134988785\n",
      "Iteration 2908 ... step 2909 loss 2.20426511765\n",
      "Iteration 2909 ... step 2910 loss 2.29874944687\n",
      "Iteration 2910 ... step 2911 loss 1.88495349884\n",
      "Iteration 2911 ... step 2912 loss 2.07848834991\n",
      "Iteration 2912 ... step 2913 loss 2.15069913864\n",
      "Iteration 2913 ... step 2914 loss 2.1303024292\n",
      "Iteration 2914 ... step 2915 loss 2.13957595825\n",
      "Iteration 2915 ... step 2916 loss 2.29972100258\n",
      "Iteration 2916 ... step 2917 loss 2.44393777847\n",
      "Iteration 2917 ... step 2918 loss 2.1971154213\n",
      "Iteration 2918 ... step 2919 loss 2.31649494171\n",
      "Iteration 2919 ... step 2920 loss 2.26876068115\n",
      "Iteration 2920 ... step 2921 loss 2.30478715897\n",
      "Iteration 2921 ... step 2922 loss 2.23039102554\n",
      "Iteration 2922 ... step 2923 loss 2.29641342163\n",
      "Iteration 2923 ... step 2924 loss 2.12415790558\n",
      "Iteration 2924 ... step 2925 loss 2.22606897354\n",
      "Iteration 2925 ... step 2926 loss 2.16568017006\n",
      "Iteration 2926 ... step 2927 loss 2.13963580132\n",
      "Iteration 2927 ... step 2928 loss 2.13262367249\n",
      "Iteration 2928 ... step 2929 loss 2.10943984985\n",
      "Iteration 2929 ... step 2930 loss 2.09489440918\n",
      "Iteration 2930 ... step 2931 loss 2.15771150589\n",
      "Iteration 2931 ... step 2932 loss 2.18303585052\n",
      "Iteration 2932 ... step 2933 loss 2.21433162689\n",
      "Iteration 2933 ... step 2934 loss 2.21331596375\n",
      "Iteration 2934 ... step 2935 loss 2.32216620445\n",
      "Iteration 2935 ... step 2936 loss 2.24159765244\n",
      "Iteration 2936 ... step 2937 loss 2.12756085396\n",
      "Iteration 2937 ... step 2938 loss 2.18589472771\n",
      "Iteration 2938 ... step 2939 loss 2.21215748787\n",
      "Iteration 2939 ... step 2940 loss 2.20116949081\n",
      "Iteration 2940 ... step 2941 loss 2.40813446045\n",
      "Iteration 2941 ... step 2942 loss 2.03514003754\n",
      "Iteration 2942 ... step 2943 loss 2.18071293831\n",
      "Iteration 2943 ... step 2944 loss 2.21148633957\n",
      "Iteration 2944 ... step 2945 loss 2.16117119789\n",
      "Iteration 2945 ... step 2946 loss 2.11628937721\n",
      "Iteration 2946 ... step 2947 loss 2.10609960556\n",
      "Iteration 2947 ... step 2948 loss 2.17730045319\n",
      "Iteration 2948 ... step 2949 loss 2.34565782547\n",
      "Iteration 2949 ... step 2950 loss 2.47005915642\n",
      "Iteration 2950 ... step 2951 loss 2.381483078\n",
      "Iteration 2951 ... step 2952 loss 2.30309391022\n",
      "Iteration 2952 ... step 2953 loss 2.15176725388\n",
      "Iteration 2953 ... step 2954 loss 2.38732481003\n",
      "Iteration 2954 ... step 2955 loss 2.20914554596\n",
      "Iteration 2955 ... step 2956 loss 2.03254747391\n",
      "Iteration 2956 ... step 2957 loss 2.15291213989\n",
      "Iteration 2957 ... step 2958 loss 2.12306237221\n",
      "Iteration 2958 ... step 2959 loss 2.19554758072\n",
      "Iteration 2959 ... step 2960 loss 2.15345788002\n",
      "Iteration 2960 ... step 2961 loss 2.16444301605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2961 ... step 2962 loss 2.36795568466\n",
      "Iteration 2962 ... step 2963 loss 2.26505327225\n",
      "Iteration 2963 ... step 2964 loss 2.09931850433\n",
      "Iteration 2964 ... step 2965 loss 2.14358854294\n",
      "Iteration 2965 ... step 2966 loss 2.22896957397\n",
      "Iteration 2966 ... step 2967 loss 2.19827222824\n",
      "Iteration 2967 ... step 2968 loss 2.30629396439\n",
      "Iteration 2968 ... step 2969 loss 2.15724182129\n",
      "Iteration 2969 ... step 2970 loss 2.23570632935\n",
      "Iteration 2970 ... step 2971 loss 2.07783985138\n",
      "Iteration 2971 ... step 2972 loss 2.27611732483\n",
      "Iteration 2972 ... step 2973 loss 2.20256209373\n",
      "Iteration 2973 ... step 2974 loss 2.2149503231\n",
      "Iteration 2974 ... step 2975 loss 2.19763040543\n",
      "Iteration 2975 ... step 2976 loss 2.39218235016\n",
      "Iteration 2976 ... step 2977 loss 2.11875391006\n",
      "Iteration 2977 ... step 2978 loss 2.04354763031\n",
      "Iteration 2978 ... step 2979 loss 2.21970462799\n",
      "Iteration 2979 ... step 2980 loss 2.13312673569\n",
      "Iteration 2980 ... step 2981 loss 2.1635055542\n",
      "Iteration 2981 ... step 2982 loss 2.31633663177\n",
      "Iteration 2982 ... step 2983 loss 2.14873313904\n",
      "Iteration 2983 ... step 2984 loss 2.11462521553\n",
      "Iteration 2984 ... step 2985 loss 2.02205061913\n",
      "Iteration 2985 ... step 2986 loss 2.33010053635\n",
      "Iteration 2986 ... step 2987 loss 2.06087088585\n",
      "Iteration 2987 ... step 2988 loss 2.24828720093\n",
      "Iteration 2988 ... step 2989 loss 2.04393529892\n",
      "Iteration 2989 ... step 2990 loss 2.08973312378\n",
      "Iteration 2990 ... step 2991 loss 2.08346176147\n",
      "Iteration 2991 ... step 2992 loss 2.23531723022\n",
      "Iteration 2992 ... step 2993 loss 2.27754092216\n",
      "Iteration 2993 ... step 2994 loss 2.197909832\n",
      "Iteration 2994 ... step 2995 loss 2.30082607269\n",
      "Iteration 2995 ... step 2996 loss 2.05408215523\n",
      "Iteration 2996 ... step 2997 loss 2.02348709106\n",
      "Iteration 2997 ... step 2998 loss 2.18041419983\n",
      "Iteration 2998 ... step 2999 loss 2.25103473663\n",
      "Iteration 2999 ... step 3000 loss 2.34283065796\n",
      "Iteration 3000 ... step 3001 loss 2.11944794655\n",
      "Minibatch loss at step 3000: 2.119448\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 3001 ... step 3002 loss 2.25742149353\n",
      "Iteration 3002 ... step 3003 loss 2.23755502701\n",
      "Iteration 3003 ... step 3004 loss 2.54114866257\n",
      "Iteration 3004 ... step 3005 loss 2.19179344177\n",
      "Iteration 3005 ... step 3006 loss 2.24223279953\n",
      "Iteration 3006 ... step 3007 loss 2.12337350845\n",
      "Iteration 3007 ... step 3008 loss 2.2816362381\n",
      "Iteration 3008 ... step 3009 loss 2.18472385406\n",
      "Iteration 3009 ... step 3010 loss 2.17908096313\n",
      "Iteration 3010 ... step 3011 loss 2.18796372414\n",
      "Iteration 3011 ... step 3012 loss 2.28752827644\n",
      "Iteration 3012 ... step 3013 loss 2.31870269775\n",
      "Iteration 3013 ... step 3014 loss 2.29004240036\n",
      "Iteration 3014 ... step 3015 loss 2.38509941101\n",
      "Iteration 3015 ... step 3016 loss 2.09649896622\n",
      "Iteration 3016 ... step 3017 loss 2.26463603973\n",
      "Iteration 3017 ... step 3018 loss 2.16725587845\n",
      "Iteration 3018 ... step 3019 loss 2.27895784378\n",
      "Iteration 3019 ... step 3020 loss 2.1176276207\n",
      "Iteration 3020 ... step 3021 loss 2.33725309372\n",
      "Iteration 3021 ... step 3022 loss 2.06725144386\n",
      "Iteration 3022 ... step 3023 loss 2.36646747589\n",
      "Iteration 3023 ... step 3024 loss 2.05982089043\n",
      "Iteration 3024 ... step 3025 loss 2.23986577988\n",
      "Iteration 3025 ... step 3026 loss 2.22979974747\n",
      "Iteration 3026 ... step 3027 loss 2.25290226936\n",
      "Iteration 3027 ... step 3028 loss 2.27067089081\n",
      "Iteration 3028 ... step 3029 loss 2.01018333435\n",
      "Iteration 3029 ... step 3030 loss 2.04951000214\n",
      "Iteration 3030 ... step 3031 loss 2.15233898163\n",
      "Iteration 3031 ... step 3032 loss 2.2604804039\n",
      "Iteration 3032 ... step 3033 loss 2.2717666626\n",
      "Iteration 3033 ... step 3034 loss 2.37611627579\n",
      "Iteration 3034 ... step 3035 loss 2.0584359169\n",
      "Iteration 3035 ... step 3036 loss 2.1740128994\n",
      "Iteration 3036 ... step 3037 loss 1.90731775761\n",
      "Iteration 3037 ... step 3038 loss 2.23415660858\n",
      "Iteration 3038 ... step 3039 loss 2.47651672363\n",
      "Iteration 3039 ... step 3040 loss 2.25311422348\n",
      "Iteration 3040 ... step 3041 loss 2.29166889191\n",
      "Iteration 3041 ... step 3042 loss 2.25392317772\n",
      "Iteration 3042 ... step 3043 loss 2.15621232986\n",
      "Iteration 3043 ... step 3044 loss 2.07086443901\n",
      "Iteration 3044 ... step 3045 loss 2.20232725143\n",
      "Iteration 3045 ... step 3046 loss 2.1270160675\n",
      "Iteration 3046 ... step 3047 loss 1.98363602161\n",
      "Iteration 3047 ... step 3048 loss 2.36030578613\n",
      "Iteration 3048 ... step 3049 loss 2.43440556526\n",
      "Iteration 3049 ... step 3050 loss 2.26778078079\n",
      "Iteration 3050 ... step 3051 loss 2.30538702011\n",
      "Iteration 3051 ... step 3052 loss 2.08360505104\n",
      "Iteration 3052 ... step 3053 loss 2.25376152992\n",
      "Iteration 3053 ... step 3054 loss 2.32912755013\n",
      "Iteration 3054 ... step 3055 loss 2.45725727081\n",
      "Iteration 3055 ... step 3056 loss 2.26843547821\n",
      "Iteration 3056 ... step 3057 loss 2.24410152435\n",
      "Iteration 3057 ... step 3058 loss 2.07734847069\n",
      "Iteration 3058 ... step 3059 loss 2.02166795731\n",
      "Iteration 3059 ... step 3060 loss 2.29277849197\n",
      "Iteration 3060 ... step 3061 loss 2.19016647339\n",
      "Iteration 3061 ... step 3062 loss 2.05395126343\n",
      "Iteration 3062 ... step 3063 loss 1.95199978352\n",
      "Iteration 3063 ... step 3064 loss 2.14208650589\n",
      "Iteration 3064 ... step 3065 loss 2.33298397064\n",
      "Iteration 3065 ... step 3066 loss 2.13671803474\n",
      "Iteration 3066 ... step 3067 loss 2.25277662277\n",
      "Iteration 3067 ... step 3068 loss 2.13579797745\n",
      "Iteration 3068 ... step 3069 loss 2.40486192703\n",
      "Iteration 3069 ... step 3070 loss 2.19634866714\n",
      "Iteration 3070 ... step 3071 loss 2.20629358292\n",
      "Iteration 3071 ... step 3072 loss 2.22164750099\n",
      "Iteration 3072 ... step 3073 loss 1.96165657043\n",
      "Iteration 3073 ... step 3074 loss 2.11284828186\n",
      "Iteration 3074 ... step 3075 loss 2.09199976921\n",
      "Iteration 3075 ... step 3076 loss 2.09465098381\n",
      "Iteration 3076 ... step 3077 loss 2.16998887062\n",
      "Iteration 3077 ... step 3078 loss 2.3293402195\n",
      "Iteration 3078 ... step 3079 loss 2.20883750916\n",
      "Iteration 3079 ... step 3080 loss 2.27438640594\n",
      "Iteration 3080 ... step 3081 loss 2.28016376495\n",
      "Iteration 3081 ... step 3082 loss 2.20049571991\n",
      "Iteration 3082 ... step 3083 loss 2.07486248016\n",
      "Iteration 3083 ... step 3084 loss 2.38142228127\n",
      "Iteration 3084 ... step 3085 loss 2.1119146347\n",
      "Iteration 3085 ... step 3086 loss 2.50150465965\n",
      "Iteration 3086 ... step 3087 loss 2.16502928734\n",
      "Iteration 3087 ... step 3088 loss 2.31846380234\n",
      "Iteration 3088 ... step 3089 loss 2.22631716728\n",
      "Iteration 3089 ... step 3090 loss 2.09797382355\n",
      "Iteration 3090 ... step 3091 loss 2.38507890701\n",
      "Iteration 3091 ... step 3092 loss 2.27922296524\n",
      "Iteration 3092 ... step 3093 loss 2.181661129\n",
      "Iteration 3093 ... step 3094 loss 2.11577796936\n",
      "Iteration 3094 ... step 3095 loss 2.30144453049\n",
      "Iteration 3095 ... step 3096 loss 2.46354913712\n",
      "Iteration 3096 ... step 3097 loss 2.08197021484\n",
      "Iteration 3097 ... step 3098 loss 2.22344207764\n",
      "Iteration 3098 ... step 3099 loss 2.36158657074\n",
      "Iteration 3099 ... step 3100 loss 2.14609074593\n",
      "Iteration 3100 ... step 3101 loss 2.4026465416\n",
      "Iteration 3101 ... step 3102 loss 2.13029050827\n",
      "Iteration 3102 ... step 3103 loss 2.16662454605\n",
      "Iteration 3103 ... step 3104 loss 2.20839309692\n",
      "Iteration 3104 ... step 3105 loss 2.18662500381\n",
      "Iteration 3105 ... step 3106 loss 2.17530632019\n",
      "Iteration 3106 ... step 3107 loss 2.19921016693\n",
      "Iteration 3107 ... step 3108 loss 2.14056158066\n",
      "Iteration 3108 ... step 3109 loss 2.03678703308\n",
      "Iteration 3109 ... step 3110 loss 2.31594800949\n",
      "Iteration 3110 ... step 3111 loss 2.11017870903\n",
      "Iteration 3111 ... step 3112 loss 2.14394879341\n",
      "Iteration 3112 ... step 3113 loss 2.16725468636\n",
      "Iteration 3113 ... step 3114 loss 2.26906824112\n",
      "Iteration 3114 ... step 3115 loss 2.1906504631\n",
      "Iteration 3115 ... step 3116 loss 2.17033815384\n",
      "Iteration 3116 ... step 3117 loss 2.16058921814\n",
      "Iteration 3117 ... step 3118 loss 2.36232757568\n",
      "Iteration 3118 ... step 3119 loss 2.27135825157\n",
      "Iteration 3119 ... step 3120 loss 2.13370847702\n",
      "Iteration 3120 ... step 3121 loss 2.28134417534\n",
      "Iteration 3121 ... step 3122 loss 2.22363567352\n",
      "Iteration 3122 ... step 3123 loss 2.31494092941\n",
      "Iteration 3123 ... step 3124 loss 2.13177442551\n",
      "Iteration 3124 ... step 3125 loss 2.1274459362\n",
      "Iteration 3125 ... step 3126 loss 2.19239902496\n",
      "Iteration 3126 ... step 3127 loss 2.12895703316\n",
      "Iteration 3127 ... step 3128 loss 2.17462611198\n",
      "Iteration 3128 ... step 3129 loss 2.09326505661\n",
      "Iteration 3129 ... step 3130 loss 2.20648908615\n",
      "Iteration 3130 ... step 3131 loss 2.36872005463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3131 ... step 3132 loss 2.03716802597\n",
      "Iteration 3132 ... step 3133 loss 2.13200139999\n",
      "Iteration 3133 ... step 3134 loss 2.17334723473\n",
      "Iteration 3134 ... step 3135 loss 2.28112649918\n",
      "Iteration 3135 ... step 3136 loss 2.33235311508\n",
      "Iteration 3136 ... step 3137 loss 2.41420221329\n",
      "Iteration 3137 ... step 3138 loss 2.30096435547\n",
      "Iteration 3138 ... step 3139 loss 2.2007484436\n",
      "Iteration 3139 ... step 3140 loss 2.39120411873\n",
      "Iteration 3140 ... step 3141 loss 2.13041639328\n",
      "Iteration 3141 ... step 3142 loss 2.11395955086\n",
      "Iteration 3142 ... step 3143 loss 2.30442762375\n",
      "Iteration 3143 ... step 3144 loss 2.08242344856\n",
      "Iteration 3144 ... step 3145 loss 2.20257043839\n",
      "Iteration 3145 ... step 3146 loss 2.24393033981\n",
      "Iteration 3146 ... step 3147 loss 2.27346611023\n",
      "Iteration 3147 ... step 3148 loss 2.24015140533\n",
      "Iteration 3148 ... step 3149 loss 2.29560422897\n",
      "Iteration 3149 ... step 3150 loss 2.28014993668\n",
      "Iteration 3150 ... step 3151 loss 2.40236759186\n",
      "Iteration 3151 ... step 3152 loss 2.15284371376\n",
      "Iteration 3152 ... step 3153 loss 2.28338098526\n",
      "Iteration 3153 ... step 3154 loss 1.98236250877\n",
      "Iteration 3154 ... step 3155 loss 2.13019704819\n",
      "Iteration 3155 ... step 3156 loss 2.28341960907\n",
      "Iteration 3156 ... step 3157 loss 2.38729548454\n",
      "Iteration 3157 ... step 3158 loss 2.33302593231\n",
      "Iteration 3158 ... step 3159 loss 2.07570695877\n",
      "Iteration 3159 ... step 3160 loss 2.18959379196\n",
      "Iteration 3160 ... step 3161 loss 2.28209352493\n",
      "Iteration 3161 ... step 3162 loss 2.08038830757\n",
      "Iteration 3162 ... step 3163 loss 2.25406646729\n",
      "Iteration 3163 ... step 3164 loss 2.28532600403\n",
      "Iteration 3164 ... step 3165 loss 2.12220716476\n",
      "Iteration 3165 ... step 3166 loss 2.1638507843\n",
      "Iteration 3166 ... step 3167 loss 2.06904172897\n",
      "Iteration 3167 ... step 3168 loss 2.12998247147\n",
      "Iteration 3168 ... step 3169 loss 2.33508014679\n",
      "Iteration 3169 ... step 3170 loss 2.14765691757\n",
      "Iteration 3170 ... step 3171 loss 2.15819120407\n",
      "Iteration 3171 ... step 3172 loss 2.18351078033\n",
      "Iteration 3172 ... step 3173 loss 2.27939939499\n",
      "Iteration 3173 ... step 3174 loss 2.2233042717\n",
      "Iteration 3174 ... step 3175 loss 2.04842948914\n",
      "Iteration 3175 ... step 3176 loss 2.39818191528\n",
      "Iteration 3176 ... step 3177 loss 2.0993783474\n",
      "Iteration 3177 ... step 3178 loss 2.19172859192\n",
      "Iteration 3178 ... step 3179 loss 2.13643789291\n",
      "Iteration 3179 ... step 3180 loss 2.26104164124\n",
      "Iteration 3180 ... step 3181 loss 2.37977027893\n",
      "Iteration 3181 ... step 3182 loss 2.21041512489\n",
      "Iteration 3182 ... step 3183 loss 2.15288209915\n",
      "Iteration 3183 ... step 3184 loss 2.10283899307\n",
      "Iteration 3184 ... step 3185 loss 2.25826120377\n",
      "Iteration 3185 ... step 3186 loss 2.12204217911\n",
      "Iteration 3186 ... step 3187 loss 2.05625033379\n",
      "Iteration 3187 ... step 3188 loss 2.17762041092\n",
      "Iteration 3188 ... step 3189 loss 2.14096641541\n",
      "Iteration 3189 ... step 3190 loss 2.29133462906\n",
      "Iteration 3190 ... step 3191 loss 2.22962117195\n",
      "Iteration 3191 ... step 3192 loss 2.15244412422\n",
      "Iteration 3192 ... step 3193 loss 2.12985658646\n",
      "Iteration 3193 ... step 3194 loss 2.153226614\n",
      "Iteration 3194 ... step 3195 loss 2.29242420197\n",
      "Iteration 3195 ... step 3196 loss 2.16476750374\n",
      "Iteration 3196 ... step 3197 loss 2.4086022377\n",
      "Iteration 3197 ... step 3198 loss 2.07707452774\n",
      "Iteration 3198 ... step 3199 loss 2.26723957062\n",
      "Iteration 3199 ... step 3200 loss 2.19598221779\n",
      "Iteration 3200 ... step 3201 loss 2.33126091957\n",
      "Iteration 3201 ... step 3202 loss 2.22434616089\n",
      "Iteration 3202 ... step 3203 loss 2.26498174667\n",
      "Iteration 3203 ... step 3204 loss 2.00080966949\n",
      "Iteration 3204 ... step 3205 loss 2.07195687294\n",
      "Iteration 3205 ... step 3206 loss 1.97865891457\n",
      "Iteration 3206 ... step 3207 loss 2.11314082146\n",
      "Iteration 3207 ... step 3208 loss 2.22780036926\n",
      "Iteration 3208 ... step 3209 loss 2.17704677582\n",
      "Iteration 3209 ... step 3210 loss 2.07952070236\n",
      "Iteration 3210 ... step 3211 loss 2.24578714371\n",
      "Iteration 3211 ... step 3212 loss 2.26722431183\n",
      "Iteration 3212 ... step 3213 loss 2.30960798264\n",
      "Iteration 3213 ... step 3214 loss 2.3115773201\n",
      "Iteration 3214 ... step 3215 loss 2.36516094208\n",
      "Iteration 3215 ... step 3216 loss 2.18791699409\n",
      "Iteration 3216 ... step 3217 loss 1.99130034447\n",
      "Iteration 3217 ... step 3218 loss 2.29716300964\n",
      "Iteration 3218 ... step 3219 loss 2.15722966194\n",
      "Iteration 3219 ... step 3220 loss 2.40029335022\n",
      "Iteration 3220 ... step 3221 loss 2.32169818878\n",
      "Iteration 3221 ... step 3222 loss 2.21246600151\n",
      "Iteration 3222 ... step 3223 loss 2.4784412384\n",
      "Iteration 3223 ... step 3224 loss 2.21414327621\n",
      "Iteration 3224 ... step 3225 loss 2.19946956635\n",
      "Iteration 3225 ... step 3226 loss 2.26263141632\n",
      "Iteration 3226 ... step 3227 loss 2.08303165436\n",
      "Iteration 3227 ... step 3228 loss 2.19690704346\n",
      "Iteration 3228 ... step 3229 loss 2.14110612869\n",
      "Iteration 3229 ... step 3230 loss 2.24676847458\n",
      "Iteration 3230 ... step 3231 loss 2.06148481369\n",
      "Iteration 3231 ... step 3232 loss 2.15355205536\n",
      "Iteration 3232 ... step 3233 loss 2.17434072495\n",
      "Iteration 3233 ... step 3234 loss 2.22352194786\n",
      "Iteration 3234 ... step 3235 loss 2.25346517563\n",
      "Iteration 3235 ... step 3236 loss 2.19274663925\n",
      "Iteration 3236 ... step 3237 loss 2.02450585365\n",
      "Iteration 3237 ... step 3238 loss 2.07901144028\n",
      "Iteration 3238 ... step 3239 loss 2.00583338737\n",
      "Iteration 3239 ... step 3240 loss 2.29099464417\n",
      "Iteration 3240 ... step 3241 loss 2.19357585907\n",
      "Iteration 3241 ... step 3242 loss 2.05209040642\n",
      "Iteration 3242 ... step 3243 loss 1.99811017513\n",
      "Iteration 3243 ... step 3244 loss 2.07935571671\n",
      "Iteration 3244 ... step 3245 loss 2.11023139954\n",
      "Iteration 3245 ... step 3246 loss 2.10980892181\n",
      "Iteration 3246 ... step 3247 loss 2.2844452858\n",
      "Iteration 3247 ... step 3248 loss 2.15803194046\n",
      "Iteration 3248 ... step 3249 loss 2.27125644684\n",
      "Iteration 3249 ... step 3250 loss 2.07895851135\n",
      "Iteration 3250 ... step 3251 loss 2.07446098328\n",
      "Iteration 3251 ... step 3252 loss 2.11099648476\n",
      "Iteration 3252 ... step 3253 loss 2.11778640747\n",
      "Iteration 3253 ... step 3254 loss 2.2663269043\n",
      "Iteration 3254 ... step 3255 loss 2.19318795204\n",
      "Iteration 3255 ... step 3256 loss 1.9981815815\n",
      "Iteration 3256 ... step 3257 loss 2.22224903107\n",
      "Iteration 3257 ... step 3258 loss 2.18910646439\n",
      "Iteration 3258 ... step 3259 loss 2.0901761055\n",
      "Iteration 3259 ... step 3260 loss 2.19959211349\n",
      "Iteration 3260 ... step 3261 loss 2.34460306168\n",
      "Iteration 3261 ... step 3262 loss 2.139138937\n",
      "Iteration 3262 ... step 3263 loss 1.99379479885\n",
      "Iteration 3263 ... step 3264 loss 2.24679660797\n",
      "Iteration 3264 ... step 3265 loss 2.02321529388\n",
      "Iteration 3265 ... step 3266 loss 2.0776181221\n",
      "Iteration 3266 ... step 3267 loss 2.23766469955\n",
      "Iteration 3267 ... step 3268 loss 2.19020938873\n",
      "Iteration 3268 ... step 3269 loss 2.16902279854\n",
      "Iteration 3269 ... step 3270 loss 2.36873483658\n",
      "Iteration 3270 ... step 3271 loss 1.99982976913\n",
      "Iteration 3271 ... step 3272 loss 2.22709178925\n",
      "Iteration 3272 ... step 3273 loss 2.08914613724\n",
      "Iteration 3273 ... step 3274 loss 2.21054935455\n",
      "Iteration 3274 ... step 3275 loss 2.32596969604\n",
      "Iteration 3275 ... step 3276 loss 2.24366235733\n",
      "Iteration 3276 ... step 3277 loss 2.1766910553\n",
      "Iteration 3277 ... step 3278 loss 2.30946302414\n",
      "Iteration 3278 ... step 3279 loss 2.29878330231\n",
      "Iteration 3279 ... step 3280 loss 2.23039197922\n",
      "Iteration 3280 ... step 3281 loss 2.1530122757\n",
      "Iteration 3281 ... step 3282 loss 2.24570178986\n",
      "Iteration 3282 ... step 3283 loss 2.13060855865\n",
      "Iteration 3283 ... step 3284 loss 2.13754034042\n",
      "Iteration 3284 ... step 3285 loss 2.25647234917\n",
      "Iteration 3285 ... step 3286 loss 2.22699689865\n",
      "Iteration 3286 ... step 3287 loss 2.17581510544\n",
      "Iteration 3287 ... step 3288 loss 2.22786951065\n",
      "Iteration 3288 ... step 3289 loss 2.16130256653\n",
      "Iteration 3289 ... step 3290 loss 2.16088724136\n",
      "Iteration 3290 ... step 3291 loss 2.18375301361\n",
      "Iteration 3291 ... step 3292 loss 1.99881196022\n",
      "Iteration 3292 ... step 3293 loss 2.19167661667\n",
      "Iteration 3293 ... step 3294 loss 2.18835735321\n",
      "Iteration 3294 ... step 3295 loss 2.30609369278\n",
      "Iteration 3295 ... step 3296 loss 2.13650465012\n",
      "Iteration 3296 ... step 3297 loss 2.16062641144\n",
      "Iteration 3297 ... step 3298 loss 2.25915718079\n",
      "Iteration 3298 ... step 3299 loss 2.22350025177\n",
      "Iteration 3299 ... step 3300 loss 2.15129232407\n",
      "Iteration 3300 ... step 3301 loss 2.29131364822\n",
      "Iteration 3301 ... step 3302 loss 2.14076018333\n",
      "Iteration 3302 ... step 3303 loss 2.10809135437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3303 ... step 3304 loss 2.24611759186\n",
      "Iteration 3304 ... step 3305 loss 2.39161205292\n",
      "Iteration 3305 ... step 3306 loss 2.02744197845\n",
      "Iteration 3306 ... step 3307 loss 2.20204162598\n",
      "Iteration 3307 ... step 3308 loss 2.14105653763\n",
      "Iteration 3308 ... step 3309 loss 2.16784262657\n",
      "Iteration 3309 ... step 3310 loss 2.37452626228\n",
      "Iteration 3310 ... step 3311 loss 2.23527240753\n",
      "Iteration 3311 ... step 3312 loss 2.250623703\n",
      "Iteration 3312 ... step 3313 loss 2.16459155083\n",
      "Iteration 3313 ... step 3314 loss 2.27687883377\n",
      "Iteration 3314 ... step 3315 loss 2.11578416824\n",
      "Iteration 3315 ... step 3316 loss 2.03489208221\n",
      "Iteration 3316 ... step 3317 loss 2.25171232224\n",
      "Iteration 3317 ... step 3318 loss 2.25235438347\n",
      "Iteration 3318 ... step 3319 loss 2.19529819489\n",
      "Iteration 3319 ... step 3320 loss 2.10462999344\n",
      "Iteration 3320 ... step 3321 loss 2.1226978302\n",
      "Iteration 3321 ... step 3322 loss 2.00334048271\n",
      "Iteration 3322 ... step 3323 loss 2.0452837944\n",
      "Iteration 3323 ... step 3324 loss 2.37166881561\n",
      "Iteration 3324 ... step 3325 loss 2.07701683044\n",
      "Iteration 3325 ... step 3326 loss 2.06715679169\n",
      "Iteration 3326 ... step 3327 loss 2.33599090576\n",
      "Iteration 3327 ... step 3328 loss 2.02889060974\n",
      "Iteration 3328 ... step 3329 loss 1.98788416386\n",
      "Iteration 3329 ... step 3330 loss 2.13572955132\n",
      "Iteration 3330 ... step 3331 loss 2.21175837517\n",
      "Iteration 3331 ... step 3332 loss 2.2003068924\n",
      "Iteration 3332 ... step 3333 loss 2.20991516113\n",
      "Iteration 3333 ... step 3334 loss 2.20449733734\n",
      "Iteration 3334 ... step 3335 loss 2.17792367935\n",
      "Iteration 3335 ... step 3336 loss 2.3823132515\n",
      "Iteration 3336 ... step 3337 loss 2.2186498642\n",
      "Iteration 3337 ... step 3338 loss 2.2285284996\n",
      "Iteration 3338 ... step 3339 loss 2.1781129837\n",
      "Iteration 3339 ... step 3340 loss 2.24388313293\n",
      "Iteration 3340 ... step 3341 loss 2.14273667336\n",
      "Iteration 3341 ... step 3342 loss 2.26701259613\n",
      "Iteration 3342 ... step 3343 loss 2.28406453133\n",
      "Iteration 3343 ... step 3344 loss 2.30672550201\n",
      "Iteration 3344 ... step 3345 loss 2.13470983505\n",
      "Iteration 3345 ... step 3346 loss 2.40680217743\n",
      "Iteration 3346 ... step 3347 loss 2.10138344765\n",
      "Iteration 3347 ... step 3348 loss 2.02523136139\n",
      "Iteration 3348 ... step 3349 loss 2.27322483063\n",
      "Iteration 3349 ... step 3350 loss 2.31545114517\n",
      "Iteration 3350 ... step 3351 loss 2.24067091942\n",
      "Iteration 3351 ... step 3352 loss 2.30126309395\n",
      "Iteration 3352 ... step 3353 loss 2.26478767395\n",
      "Iteration 3353 ... step 3354 loss 2.05687499046\n",
      "Iteration 3354 ... step 3355 loss 2.25734448433\n",
      "Iteration 3355 ... step 3356 loss 2.1541826725\n",
      "Iteration 3356 ... step 3357 loss 2.25406503677\n",
      "Iteration 3357 ... step 3358 loss 2.34353399277\n",
      "Iteration 3358 ... step 3359 loss 2.18313360214\n",
      "Iteration 3359 ... step 3360 loss 2.08880257607\n",
      "Iteration 3360 ... step 3361 loss 2.14118242264\n",
      "Iteration 3361 ... step 3362 loss 2.11500501633\n",
      "Iteration 3362 ... step 3363 loss 2.15615081787\n",
      "Iteration 3363 ... step 3364 loss 2.12748599052\n",
      "Iteration 3364 ... step 3365 loss 2.21344852448\n",
      "Iteration 3365 ... step 3366 loss 2.19814920425\n",
      "Iteration 3366 ... step 3367 loss 2.20364952087\n",
      "Iteration 3367 ... step 3368 loss 2.39878988266\n",
      "Iteration 3368 ... step 3369 loss 2.27386140823\n",
      "Iteration 3369 ... step 3370 loss 2.12165117264\n",
      "Iteration 3370 ... step 3371 loss 2.35688400269\n",
      "Iteration 3371 ... step 3372 loss 2.07858181\n",
      "Iteration 3372 ... step 3373 loss 2.1049683094\n",
      "Iteration 3373 ... step 3374 loss 2.18287992477\n",
      "Iteration 3374 ... step 3375 loss 2.13023138046\n",
      "Iteration 3375 ... step 3376 loss 2.02814483643\n",
      "Iteration 3376 ... step 3377 loss 2.10885095596\n",
      "Iteration 3377 ... step 3378 loss 2.21577787399\n",
      "Iteration 3378 ... step 3379 loss 2.13857603073\n",
      "Iteration 3379 ... step 3380 loss 2.06156158447\n",
      "Iteration 3380 ... step 3381 loss 2.21654486656\n",
      "Iteration 3381 ... step 3382 loss 2.20133924484\n",
      "Iteration 3382 ... step 3383 loss 2.4690952301\n",
      "Iteration 3383 ... step 3384 loss 2.16124486923\n",
      "Iteration 3384 ... step 3385 loss 2.19128036499\n",
      "Iteration 3385 ... step 3386 loss 2.1338698864\n",
      "Iteration 3386 ... step 3387 loss 2.29530596733\n",
      "Iteration 3387 ... step 3388 loss 2.25483465195\n",
      "Iteration 3388 ... step 3389 loss 1.9461824894\n",
      "Iteration 3389 ... step 3390 loss 2.23016524315\n",
      "Iteration 3390 ... step 3391 loss 2.19885015488\n",
      "Iteration 3391 ... step 3392 loss 2.14660382271\n",
      "Iteration 3392 ... step 3393 loss 2.2898311615\n",
      "Iteration 3393 ... step 3394 loss 2.05288267136\n",
      "Iteration 3394 ... step 3395 loss 2.07048892975\n",
      "Iteration 3395 ... step 3396 loss 2.06944322586\n",
      "Iteration 3396 ... step 3397 loss 2.18578338623\n",
      "Iteration 3397 ... step 3398 loss 2.07824897766\n",
      "Iteration 3398 ... step 3399 loss 2.27805399895\n",
      "Iteration 3399 ... step 3400 loss 2.40897512436\n",
      "Iteration 3400 ... step 3401 loss 2.20890331268\n",
      "Iteration 3401 ... step 3402 loss 2.20714783669\n",
      "Iteration 3402 ... step 3403 loss 2.12934470177\n",
      "Iteration 3403 ... step 3404 loss 2.54524278641\n",
      "Iteration 3404 ... step 3405 loss 1.93899512291\n",
      "Iteration 3405 ... step 3406 loss 2.38895940781\n",
      "Iteration 3406 ... step 3407 loss 2.33996391296\n",
      "Iteration 3407 ... step 3408 loss 2.1978802681\n",
      "Iteration 3408 ... step 3409 loss 2.2120347023\n",
      "Iteration 3409 ... step 3410 loss 2.38835287094\n",
      "Iteration 3410 ... step 3411 loss 2.25407171249\n",
      "Iteration 3411 ... step 3412 loss 2.08494901657\n",
      "Iteration 3412 ... step 3413 loss 2.18339729309\n",
      "Iteration 3413 ... step 3414 loss 2.05450582504\n",
      "Iteration 3414 ... step 3415 loss 2.24694013596\n",
      "Iteration 3415 ... step 3416 loss 2.12631559372\n",
      "Iteration 3416 ... step 3417 loss 2.28076910973\n",
      "Iteration 3417 ... step 3418 loss 2.41988182068\n",
      "Iteration 3418 ... step 3419 loss 2.29243206978\n",
      "Iteration 3419 ... step 3420 loss 2.08527898788\n",
      "Iteration 3420 ... step 3421 loss 2.28518152237\n",
      "Iteration 3421 ... step 3422 loss 2.21705007553\n",
      "Iteration 3422 ... step 3423 loss 2.14315152168\n",
      "Iteration 3423 ... step 3424 loss 2.33813524246\n",
      "Iteration 3424 ... step 3425 loss 2.28012514114\n",
      "Iteration 3425 ... step 3426 loss 2.48460102081\n",
      "Iteration 3426 ... step 3427 loss 2.14794898033\n",
      "Iteration 3427 ... step 3428 loss 2.400806427\n",
      "Iteration 3428 ... step 3429 loss 2.38954210281\n",
      "Iteration 3429 ... step 3430 loss 2.02407717705\n",
      "Iteration 3430 ... step 3431 loss 2.12859940529\n",
      "Iteration 3431 ... step 3432 loss 2.44910621643\n",
      "Iteration 3432 ... step 3433 loss 2.2474656105\n",
      "Iteration 3433 ... step 3434 loss 2.09885168076\n",
      "Iteration 3434 ... step 3435 loss 2.254529953\n",
      "Iteration 3435 ... step 3436 loss 2.36653852463\n",
      "Iteration 3436 ... step 3437 loss 2.12068557739\n",
      "Iteration 3437 ... step 3438 loss 2.25425386429\n",
      "Iteration 3438 ... step 3439 loss 2.35849118233\n",
      "Iteration 3439 ... step 3440 loss 2.27626132965\n",
      "Iteration 3440 ... step 3441 loss 2.36938977242\n",
      "Iteration 3441 ... step 3442 loss 2.26751184464\n",
      "Iteration 3442 ... step 3443 loss 2.22094535828\n",
      "Iteration 3443 ... step 3444 loss 2.1259572506\n",
      "Iteration 3444 ... step 3445 loss 2.2753162384\n",
      "Iteration 3445 ... step 3446 loss 2.14634895325\n",
      "Iteration 3446 ... step 3447 loss 2.28059577942\n",
      "Iteration 3447 ... step 3448 loss 2.2537484169\n",
      "Iteration 3448 ... step 3449 loss 2.26417350769\n",
      "Iteration 3449 ... step 3450 loss 2.15142607689\n",
      "Iteration 3450 ... step 3451 loss 2.15660238266\n",
      "Iteration 3451 ... step 3452 loss 2.11742353439\n",
      "Iteration 3452 ... step 3453 loss 2.30455732346\n",
      "Iteration 3453 ... step 3454 loss 2.24838066101\n",
      "Iteration 3454 ... step 3455 loss 2.34984493256\n",
      "Iteration 3455 ... step 3456 loss 2.32698106766\n",
      "Iteration 3456 ... step 3457 loss 2.18035411835\n",
      "Iteration 3457 ... step 3458 loss 2.15964508057\n",
      "Iteration 3458 ... step 3459 loss 2.40525126457\n",
      "Iteration 3459 ... step 3460 loss 2.19037342072\n",
      "Iteration 3460 ... step 3461 loss 1.98364281654\n",
      "Iteration 3461 ... step 3462 loss 2.03398442268\n",
      "Iteration 3462 ... step 3463 loss 2.24197244644\n",
      "Iteration 3463 ... step 3464 loss 2.2679848671\n",
      "Iteration 3464 ... step 3465 loss 2.31077432632\n",
      "Iteration 3465 ... step 3466 loss 2.27853751183\n",
      "Iteration 3466 ... step 3467 loss 2.2339849472\n",
      "Iteration 3467 ... step 3468 loss 2.16327643394\n",
      "Iteration 3468 ... step 3469 loss 2.31539273262\n",
      "Iteration 3469 ... step 3470 loss 2.18126535416\n",
      "Iteration 3470 ... step 3471 loss 2.33573818207\n",
      "Iteration 3471 ... step 3472 loss 2.20821857452\n",
      "Iteration 3472 ... step 3473 loss 2.23935413361\n",
      "Iteration 3473 ... step 3474 loss 2.39960098267\n",
      "Iteration 3474 ... step 3475 loss 2.22616386414\n",
      "Iteration 3475 ... step 3476 loss 2.51835727692\n",
      "Iteration 3476 ... step 3477 loss 2.29968166351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3477 ... step 3478 loss 2.21596384048\n",
      "Iteration 3478 ... step 3479 loss 2.20852470398\n",
      "Iteration 3479 ... step 3480 loss 2.37309503555\n",
      "Iteration 3480 ... step 3481 loss 2.34013772011\n",
      "Iteration 3481 ... step 3482 loss 2.28705739975\n",
      "Iteration 3482 ... step 3483 loss 2.3839597702\n",
      "Iteration 3483 ... step 3484 loss 2.42164707184\n",
      "Iteration 3484 ... step 3485 loss 2.35727262497\n",
      "Iteration 3485 ... step 3486 loss 2.28365778923\n",
      "Iteration 3486 ... step 3487 loss 2.28152513504\n",
      "Iteration 3487 ... step 3488 loss 2.34094715118\n",
      "Iteration 3488 ... step 3489 loss 2.35876083374\n",
      "Iteration 3489 ... step 3490 loss 2.33537626266\n",
      "Iteration 3490 ... step 3491 loss 2.32159805298\n",
      "Iteration 3491 ... step 3492 loss 2.18983745575\n",
      "Iteration 3492 ... step 3493 loss 2.2795548439\n",
      "Iteration 3493 ... step 3494 loss 2.28612804413\n",
      "Iteration 3494 ... step 3495 loss 2.27651691437\n",
      "Iteration 3495 ... step 3496 loss 2.27888798714\n",
      "Iteration 3496 ... step 3497 loss 2.17710542679\n",
      "Iteration 3497 ... step 3498 loss 2.13408875465\n",
      "Iteration 3498 ... step 3499 loss 2.25952649117\n",
      "Iteration 3499 ... step 3500 loss 2.52291870117\n",
      "Iteration 3500 ... step 3501 loss 2.18776917458\n",
      "Minibatch loss at step 3500: 2.187769\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 3501 ... step 3502 loss 2.22806882858\n",
      "Iteration 3502 ... step 3503 loss 2.3724603653\n",
      "Iteration 3503 ... step 3504 loss 2.19829845428\n",
      "Iteration 3504 ... step 3505 loss 2.14821267128\n",
      "Iteration 3505 ... step 3506 loss 2.26965618134\n",
      "Iteration 3506 ... step 3507 loss 2.29461669922\n",
      "Iteration 3507 ... step 3508 loss 2.19485425949\n",
      "Iteration 3508 ... step 3509 loss 2.2297065258\n",
      "Iteration 3509 ... step 3510 loss 2.22028064728\n",
      "Iteration 3510 ... step 3511 loss 2.5907201767\n",
      "Iteration 3511 ... step 3512 loss 2.28759026527\n",
      "Iteration 3512 ... step 3513 loss 2.43919229507\n",
      "Iteration 3513 ... step 3514 loss 2.23570394516\n",
      "Iteration 3514 ... step 3515 loss 2.09240913391\n",
      "Iteration 3515 ... step 3516 loss 2.26550889015\n",
      "Iteration 3516 ... step 3517 loss 2.1519613266\n",
      "Iteration 3517 ... step 3518 loss 2.45018196106\n",
      "Iteration 3518 ... step 3519 loss 2.24148106575\n",
      "Iteration 3519 ... step 3520 loss 2.2275967598\n",
      "Iteration 3520 ... step 3521 loss 2.3787727356\n",
      "Iteration 3521 ... step 3522 loss 2.36876130104\n",
      "Iteration 3522 ... step 3523 loss 2.39265441895\n",
      "Iteration 3523 ... step 3524 loss 2.28908896446\n",
      "Iteration 3524 ... step 3525 loss 2.35810995102\n",
      "Iteration 3525 ... step 3526 loss 2.31689333916\n",
      "Iteration 3526 ... step 3527 loss 2.06868934631\n",
      "Iteration 3527 ... step 3528 loss 2.38679838181\n",
      "Iteration 3528 ... step 3529 loss 2.22179031372\n",
      "Iteration 3529 ... step 3530 loss 2.32757592201\n",
      "Iteration 3530 ... step 3531 loss 2.24349451065\n",
      "Iteration 3531 ... step 3532 loss 2.31488847733\n",
      "Iteration 3532 ... step 3533 loss 2.3789563179\n",
      "Iteration 3533 ... step 3534 loss 2.24828863144\n",
      "Iteration 3534 ... step 3535 loss 2.24194836617\n",
      "Iteration 3535 ... step 3536 loss 2.23201608658\n",
      "Iteration 3536 ... step 3537 loss 2.10724687576\n",
      "Iteration 3537 ... step 3538 loss 2.32434463501\n",
      "Iteration 3538 ... step 3539 loss 2.22957181931\n",
      "Iteration 3539 ... step 3540 loss 2.25156259537\n",
      "Iteration 3540 ... step 3541 loss 2.38509464264\n",
      "Iteration 3541 ... step 3542 loss 2.19624567032\n",
      "Iteration 3542 ... step 3543 loss 2.20812225342\n",
      "Iteration 3543 ... step 3544 loss 2.35618448257\n",
      "Iteration 3544 ... step 3545 loss 2.05162644386\n",
      "Iteration 3545 ... step 3546 loss 2.27442312241\n",
      "Iteration 3546 ... step 3547 loss 2.21032977104\n",
      "Iteration 3547 ... step 3548 loss 2.40012073517\n",
      "Iteration 3548 ... step 3549 loss 2.24389076233\n",
      "Iteration 3549 ... step 3550 loss 2.16398072243\n",
      "Iteration 3550 ... step 3551 loss 2.23774433136\n",
      "Iteration 3551 ... step 3552 loss 2.36486291885\n",
      "Iteration 3552 ... step 3553 loss 2.26970624924\n",
      "Iteration 3553 ... step 3554 loss 2.32164621353\n",
      "Iteration 3554 ... step 3555 loss 2.26525974274\n",
      "Iteration 3555 ... step 3556 loss 2.0266263485\n",
      "Iteration 3556 ... step 3557 loss 2.30626344681\n",
      "Iteration 3557 ... step 3558 loss 2.40000343323\n",
      "Iteration 3558 ... step 3559 loss 2.32127833366\n",
      "Iteration 3559 ... step 3560 loss 2.30129432678\n",
      "Iteration 3560 ... step 3561 loss 2.35292005539\n",
      "Iteration 3561 ... step 3562 loss 2.43061256409\n",
      "Iteration 3562 ... step 3563 loss 2.20905709267\n",
      "Iteration 3563 ... step 3564 loss 2.30966234207\n",
      "Iteration 3564 ... step 3565 loss 2.16728258133\n",
      "Iteration 3565 ... step 3566 loss 2.34066343307\n",
      "Iteration 3566 ... step 3567 loss 2.35831546783\n",
      "Iteration 3567 ... step 3568 loss 2.31650447845\n",
      "Iteration 3568 ... step 3569 loss 2.16369152069\n",
      "Iteration 3569 ... step 3570 loss 2.16500091553\n",
      "Iteration 3570 ... step 3571 loss 2.51182937622\n",
      "Iteration 3571 ... step 3572 loss 2.33807301521\n",
      "Iteration 3572 ... step 3573 loss 2.19395399094\n",
      "Iteration 3573 ... step 3574 loss 2.16654729843\n",
      "Iteration 3574 ... step 3575 loss 2.1380033493\n",
      "Iteration 3575 ... step 3576 loss 2.21509170532\n",
      "Iteration 3576 ... step 3577 loss 2.1174955368\n",
      "Iteration 3577 ... step 3578 loss 2.20391368866\n",
      "Iteration 3578 ... step 3579 loss 2.33822345734\n",
      "Iteration 3579 ... step 3580 loss 2.16229581833\n",
      "Iteration 3580 ... step 3581 loss 2.19230604172\n",
      "Iteration 3581 ... step 3582 loss 2.28066635132\n",
      "Iteration 3582 ... step 3583 loss 2.27467107773\n",
      "Iteration 3583 ... step 3584 loss 2.26341676712\n",
      "Iteration 3584 ... step 3585 loss 2.2718975544\n",
      "Iteration 3585 ... step 3586 loss 2.14489364624\n",
      "Iteration 3586 ... step 3587 loss 2.17432832718\n",
      "Iteration 3587 ... step 3588 loss 2.10922288895\n",
      "Iteration 3588 ... step 3589 loss 2.25063753128\n",
      "Iteration 3589 ... step 3590 loss 2.29273223877\n",
      "Iteration 3590 ... step 3591 loss 2.05603599548\n",
      "Iteration 3591 ... step 3592 loss 2.23081922531\n",
      "Iteration 3592 ... step 3593 loss 2.12106227875\n",
      "Iteration 3593 ... step 3594 loss 2.30458498001\n",
      "Iteration 3594 ... step 3595 loss 2.28809595108\n",
      "Iteration 3595 ... step 3596 loss 2.34368515015\n",
      "Iteration 3596 ... step 3597 loss 2.2389330864\n",
      "Iteration 3597 ... step 3598 loss 2.31461572647\n",
      "Iteration 3598 ... step 3599 loss 2.19620513916\n",
      "Iteration 3599 ... step 3600 loss 2.21028923988\n",
      "Iteration 3600 ... step 3601 loss 2.28172111511\n",
      "Iteration 3601 ... step 3602 loss 2.03943657875\n",
      "Iteration 3602 ... step 3603 loss 2.35052776337\n",
      "Iteration 3603 ... step 3604 loss 2.13683056831\n",
      "Iteration 3604 ... step 3605 loss 2.08712863922\n",
      "Iteration 3605 ... step 3606 loss 2.25653648376\n",
      "Iteration 3606 ... step 3607 loss 2.25487804413\n",
      "Iteration 3607 ... step 3608 loss 2.09533262253\n",
      "Iteration 3608 ... step 3609 loss 2.31069946289\n",
      "Iteration 3609 ... step 3610 loss 2.30476427078\n",
      "Iteration 3610 ... step 3611 loss 2.29602718353\n",
      "Iteration 3611 ... step 3612 loss 2.32661390305\n",
      "Iteration 3612 ... step 3613 loss 2.38823127747\n",
      "Iteration 3613 ... step 3614 loss 2.16468763351\n",
      "Iteration 3614 ... step 3615 loss 2.2657251358\n",
      "Iteration 3615 ... step 3616 loss 2.33781290054\n",
      "Iteration 3616 ... step 3617 loss 2.22336125374\n",
      "Iteration 3617 ... step 3618 loss 2.06739974022\n",
      "Iteration 3618 ... step 3619 loss 2.3184363842\n",
      "Iteration 3619 ... step 3620 loss 2.14443588257\n",
      "Iteration 3620 ... step 3621 loss 2.14634037018\n",
      "Iteration 3621 ... step 3622 loss 2.2686021328\n",
      "Iteration 3622 ... step 3623 loss 2.33407497406\n",
      "Iteration 3623 ... step 3624 loss 2.22400569916\n",
      "Iteration 3624 ... step 3625 loss 2.30826950073\n",
      "Iteration 3625 ... step 3626 loss 2.4801402092\n",
      "Iteration 3626 ... step 3627 loss 2.31378984451\n",
      "Iteration 3627 ... step 3628 loss 2.17346382141\n",
      "Iteration 3628 ... step 3629 loss 2.30710577965\n",
      "Iteration 3629 ... step 3630 loss 2.3919262886\n",
      "Iteration 3630 ... step 3631 loss 2.4019203186\n",
      "Iteration 3631 ... step 3632 loss 2.08179330826\n",
      "Iteration 3632 ... step 3633 loss 2.43357515335\n",
      "Iteration 3633 ... step 3634 loss 2.18133997917\n",
      "Iteration 3634 ... step 3635 loss 2.15660643578\n",
      "Iteration 3635 ... step 3636 loss 2.10385465622\n",
      "Iteration 3636 ... step 3637 loss 2.33800458908\n",
      "Iteration 3637 ... step 3638 loss 2.30042910576\n",
      "Iteration 3638 ... step 3639 loss 2.25246763229\n",
      "Iteration 3639 ... step 3640 loss 2.29481554031\n",
      "Iteration 3640 ... step 3641 loss 2.43294525146\n",
      "Iteration 3641 ... step 3642 loss 2.39281272888\n",
      "Iteration 3642 ... step 3643 loss 2.37809228897\n",
      "Iteration 3643 ... step 3644 loss 2.35484838486\n",
      "Iteration 3644 ... step 3645 loss 2.16420173645\n",
      "Iteration 3645 ... step 3646 loss 2.42861843109\n",
      "Iteration 3646 ... step 3647 loss 2.24328565598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3647 ... step 3648 loss 2.24862098694\n",
      "Iteration 3648 ... step 3649 loss 2.34262704849\n",
      "Iteration 3649 ... step 3650 loss 2.24974536896\n",
      "Iteration 3650 ... step 3651 loss 2.14370584488\n",
      "Iteration 3651 ... step 3652 loss 2.20548534393\n",
      "Iteration 3652 ... step 3653 loss 2.23995018005\n",
      "Iteration 3653 ... step 3654 loss 2.29864406586\n",
      "Iteration 3654 ... step 3655 loss 2.16721534729\n",
      "Iteration 3655 ... step 3656 loss 2.50606918335\n",
      "Iteration 3656 ... step 3657 loss 2.36922621727\n",
      "Iteration 3657 ... step 3658 loss 2.294028759\n",
      "Iteration 3658 ... step 3659 loss 2.29752349854\n",
      "Iteration 3659 ... step 3660 loss 2.16760468483\n",
      "Iteration 3660 ... step 3661 loss 2.17304706573\n",
      "Iteration 3661 ... step 3662 loss 2.40203237534\n",
      "Iteration 3662 ... step 3663 loss 2.30830430984\n",
      "Iteration 3663 ... step 3664 loss 2.383228302\n",
      "Iteration 3664 ... step 3665 loss 2.13196158409\n",
      "Iteration 3665 ... step 3666 loss 2.42613863945\n",
      "Iteration 3666 ... step 3667 loss 2.1744093895\n",
      "Iteration 3667 ... step 3668 loss 2.20050764084\n",
      "Iteration 3668 ... step 3669 loss 2.24113559723\n",
      "Iteration 3669 ... step 3670 loss 2.41322803497\n",
      "Iteration 3670 ... step 3671 loss 2.1992828846\n",
      "Iteration 3671 ... step 3672 loss 2.13370418549\n",
      "Iteration 3672 ... step 3673 loss 2.27945041656\n",
      "Iteration 3673 ... step 3674 loss 2.12004995346\n",
      "Iteration 3674 ... step 3675 loss 2.13054537773\n",
      "Iteration 3675 ... step 3676 loss 2.25048494339\n",
      "Iteration 3676 ... step 3677 loss 2.31418585777\n",
      "Iteration 3677 ... step 3678 loss 2.34424304962\n",
      "Iteration 3678 ... step 3679 loss 2.16505336761\n",
      "Iteration 3679 ... step 3680 loss 2.25576210022\n",
      "Iteration 3680 ... step 3681 loss 2.27343416214\n",
      "Iteration 3681 ... step 3682 loss 2.26180052757\n",
      "Iteration 3682 ... step 3683 loss 2.32492160797\n",
      "Iteration 3683 ... step 3684 loss 2.23484134674\n",
      "Iteration 3684 ... step 3685 loss 2.29828500748\n",
      "Iteration 3685 ... step 3686 loss 2.27098655701\n",
      "Iteration 3686 ... step 3687 loss 2.2562084198\n",
      "Iteration 3687 ... step 3688 loss 2.12873649597\n",
      "Iteration 3688 ... step 3689 loss 2.20702219009\n",
      "Iteration 3689 ... step 3690 loss 2.16430544853\n",
      "Iteration 3690 ... step 3691 loss 2.29124450684\n",
      "Iteration 3691 ... step 3692 loss 2.24594783783\n",
      "Iteration 3692 ... step 3693 loss 2.37153196335\n",
      "Iteration 3693 ... step 3694 loss 2.25749778748\n",
      "Iteration 3694 ... step 3695 loss 2.17012572289\n",
      "Iteration 3695 ... step 3696 loss 2.30766963959\n",
      "Iteration 3696 ... step 3697 loss 2.23477005959\n",
      "Iteration 3697 ... step 3698 loss 2.31328630447\n",
      "Iteration 3698 ... step 3699 loss 2.23365688324\n",
      "Iteration 3699 ... step 3700 loss 2.40625166893\n",
      "Iteration 3700 ... step 3701 loss 2.18849945068\n",
      "Iteration 3701 ... step 3702 loss 2.24496912956\n",
      "Iteration 3702 ... step 3703 loss 2.37540602684\n",
      "Iteration 3703 ... step 3704 loss 2.3618106842\n",
      "Iteration 3704 ... step 3705 loss 2.17007184029\n",
      "Iteration 3705 ... step 3706 loss 2.35790157318\n",
      "Iteration 3706 ... step 3707 loss 2.09558057785\n",
      "Iteration 3707 ... step 3708 loss 2.4888586998\n",
      "Iteration 3708 ... step 3709 loss 2.32590007782\n",
      "Iteration 3709 ... step 3710 loss 2.16648411751\n",
      "Iteration 3710 ... step 3711 loss 2.14399886131\n",
      "Iteration 3711 ... step 3712 loss 2.35885286331\n",
      "Iteration 3712 ... step 3713 loss 2.21235156059\n",
      "Iteration 3713 ... step 3714 loss 2.34706640244\n",
      "Iteration 3714 ... step 3715 loss 2.30810976028\n",
      "Iteration 3715 ... step 3716 loss 2.17275500298\n",
      "Iteration 3716 ... step 3717 loss 2.58838415146\n",
      "Iteration 3717 ... step 3718 loss 2.46442317963\n",
      "Iteration 3718 ... step 3719 loss 2.25850057602\n",
      "Iteration 3719 ... step 3720 loss 2.34262990952\n",
      "Iteration 3720 ... step 3721 loss 2.27639627457\n",
      "Iteration 3721 ... step 3722 loss 2.35311412811\n",
      "Iteration 3722 ... step 3723 loss 2.28299045563\n",
      "Iteration 3723 ... step 3724 loss 2.16846036911\n",
      "Iteration 3724 ... step 3725 loss 2.48191928864\n",
      "Iteration 3725 ... step 3726 loss 2.23999643326\n",
      "Iteration 3726 ... step 3727 loss 2.27992391586\n",
      "Iteration 3727 ... step 3728 loss 2.23936605453\n",
      "Iteration 3728 ... step 3729 loss 2.2254781723\n",
      "Iteration 3729 ... step 3730 loss 2.36471843719\n",
      "Iteration 3730 ... step 3731 loss 2.57795858383\n",
      "Iteration 3731 ... step 3732 loss 2.3748049736\n",
      "Iteration 3732 ... step 3733 loss 2.09376955032\n",
      "Iteration 3733 ... step 3734 loss 2.30630207062\n",
      "Iteration 3734 ... step 3735 loss 2.41701364517\n",
      "Iteration 3735 ... step 3736 loss 2.25135421753\n",
      "Iteration 3736 ... step 3737 loss 2.34078168869\n",
      "Iteration 3737 ... step 3738 loss 2.12781429291\n",
      "Iteration 3738 ... step 3739 loss 2.20133113861\n",
      "Iteration 3739 ... step 3740 loss 2.22125434875\n",
      "Iteration 3740 ... step 3741 loss 2.34077215195\n",
      "Iteration 3741 ... step 3742 loss 2.27409172058\n",
      "Iteration 3742 ... step 3743 loss 2.01301908493\n",
      "Iteration 3743 ... step 3744 loss 2.22604370117\n",
      "Iteration 3744 ... step 3745 loss 2.48511123657\n",
      "Iteration 3745 ... step 3746 loss 2.12285041809\n",
      "Iteration 3746 ... step 3747 loss 2.48746538162\n",
      "Iteration 3747 ... step 3748 loss 2.18115377426\n",
      "Iteration 3748 ... step 3749 loss 2.19061613083\n",
      "Iteration 3749 ... step 3750 loss 2.22996139526\n",
      "Iteration 3750 ... step 3751 loss 2.40041828156\n",
      "Iteration 3751 ... step 3752 loss 2.48445892334\n",
      "Iteration 3752 ... step 3753 loss 2.16964507103\n",
      "Iteration 3753 ... step 3754 loss 2.5430855751\n",
      "Iteration 3754 ... step 3755 loss 2.13394403458\n",
      "Iteration 3755 ... step 3756 loss 2.26716351509\n",
      "Iteration 3756 ... step 3757 loss 2.30136966705\n",
      "Iteration 3757 ... step 3758 loss 2.2405462265\n",
      "Iteration 3758 ... step 3759 loss 2.26004505157\n",
      "Iteration 3759 ... step 3760 loss 2.28310060501\n",
      "Iteration 3760 ... step 3761 loss 2.03453302383\n",
      "Iteration 3761 ... step 3762 loss 2.22869682312\n",
      "Iteration 3762 ... step 3763 loss 2.28384304047\n",
      "Iteration 3763 ... step 3764 loss 2.14682269096\n",
      "Iteration 3764 ... step 3765 loss 2.13629198074\n",
      "Iteration 3765 ... step 3766 loss 2.30140113831\n",
      "Iteration 3766 ... step 3767 loss 2.05190968513\n",
      "Iteration 3767 ... step 3768 loss 2.12394118309\n",
      "Iteration 3768 ... step 3769 loss 2.27644300461\n",
      "Iteration 3769 ... step 3770 loss 2.27595877647\n",
      "Iteration 3770 ... step 3771 loss 2.2453455925\n",
      "Iteration 3771 ... step 3772 loss 2.19761228561\n",
      "Iteration 3772 ... step 3773 loss 2.4072561264\n",
      "Iteration 3773 ... step 3774 loss 2.22766637802\n",
      "Iteration 3774 ... step 3775 loss 2.31397294998\n",
      "Iteration 3775 ... step 3776 loss 2.35360336304\n",
      "Iteration 3776 ... step 3777 loss 2.36182832718\n",
      "Iteration 3777 ... step 3778 loss 2.0955016613\n",
      "Iteration 3778 ... step 3779 loss 2.27560162544\n",
      "Iteration 3779 ... step 3780 loss 2.26999592781\n",
      "Iteration 3780 ... step 3781 loss 2.34946632385\n",
      "Iteration 3781 ... step 3782 loss 2.39648365974\n",
      "Iteration 3782 ... step 3783 loss 2.44406843185\n",
      "Iteration 3783 ... step 3784 loss 2.28992342949\n",
      "Iteration 3784 ... step 3785 loss 2.24603509903\n",
      "Iteration 3785 ... step 3786 loss 2.31823635101\n",
      "Iteration 3786 ... step 3787 loss 2.27584600449\n",
      "Iteration 3787 ... step 3788 loss 2.35152864456\n",
      "Iteration 3788 ... step 3789 loss 2.27544355392\n",
      "Iteration 3789 ... step 3790 loss 2.36700201035\n",
      "Iteration 3790 ... step 3791 loss 2.36986970901\n",
      "Iteration 3791 ... step 3792 loss 2.29918909073\n",
      "Iteration 3792 ... step 3793 loss 2.32059383392\n",
      "Iteration 3793 ... step 3794 loss 2.25387597084\n",
      "Iteration 3794 ... step 3795 loss 2.01565265656\n",
      "Iteration 3795 ... step 3796 loss 2.30401420593\n",
      "Iteration 3796 ... step 3797 loss 2.17942094803\n",
      "Iteration 3797 ... step 3798 loss 2.11559319496\n",
      "Iteration 3798 ... step 3799 loss 2.25619459152\n",
      "Iteration 3799 ... step 3800 loss 2.08870697021\n",
      "Iteration 3800 ... step 3801 loss 2.40684747696\n",
      "Iteration 3801 ... step 3802 loss 2.29006004333\n",
      "Iteration 3802 ... step 3803 loss 2.41298246384\n",
      "Iteration 3803 ... step 3804 loss 2.42464399338\n",
      "Iteration 3804 ... step 3805 loss 2.26409006119\n",
      "Iteration 3805 ... step 3806 loss 2.34453368187\n",
      "Iteration 3806 ... step 3807 loss 2.21993494034\n",
      "Iteration 3807 ... step 3808 loss 2.34620118141\n",
      "Iteration 3808 ... step 3809 loss 2.29974532127\n",
      "Iteration 3809 ... step 3810 loss 2.19266080856\n",
      "Iteration 3810 ... step 3811 loss 2.35401058197\n",
      "Iteration 3811 ... step 3812 loss 2.13658976555\n",
      "Iteration 3812 ... step 3813 loss 2.20657444\n",
      "Iteration 3813 ... step 3814 loss 2.20630741119\n",
      "Iteration 3814 ... step 3815 loss 2.23403310776\n",
      "Iteration 3815 ... step 3816 loss 2.38140487671\n",
      "Iteration 3816 ... step 3817 loss 2.10945367813\n",
      "Iteration 3817 ... step 3818 loss 2.15434145927\n",
      "Iteration 3818 ... step 3819 loss 2.40111637115\n",
      "Iteration 3819 ... step 3820 loss 2.32322359085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3820 ... step 3821 loss 2.50867319107\n",
      "Iteration 3821 ... step 3822 loss 2.24246501923\n",
      "Iteration 3822 ... step 3823 loss 2.12464237213\n",
      "Iteration 3823 ... step 3824 loss 2.10463500023\n",
      "Iteration 3824 ... step 3825 loss 2.29991149902\n",
      "Iteration 3825 ... step 3826 loss 2.31628417969\n",
      "Iteration 3826 ... step 3827 loss 2.19140195847\n",
      "Iteration 3827 ... step 3828 loss 2.11113142967\n",
      "Iteration 3828 ... step 3829 loss 2.06286430359\n",
      "Iteration 3829 ... step 3830 loss 2.35942840576\n",
      "Iteration 3830 ... step 3831 loss 2.31856966019\n",
      "Iteration 3831 ... step 3832 loss 2.16686749458\n",
      "Iteration 3832 ... step 3833 loss 2.27651596069\n",
      "Iteration 3833 ... step 3834 loss 2.38879299164\n",
      "Iteration 3834 ... step 3835 loss 2.2546377182\n",
      "Iteration 3835 ... step 3836 loss 2.14262270927\n",
      "Iteration 3836 ... step 3837 loss 2.38227438927\n",
      "Iteration 3837 ... step 3838 loss 2.09808826447\n",
      "Iteration 3838 ... step 3839 loss 2.39423155785\n",
      "Iteration 3839 ... step 3840 loss 2.28658008575\n",
      "Iteration 3840 ... step 3841 loss 2.07736468315\n",
      "Iteration 3841 ... step 3842 loss 2.2576789856\n",
      "Iteration 3842 ... step 3843 loss 2.45587348938\n",
      "Iteration 3843 ... step 3844 loss 2.22490692139\n",
      "Iteration 3844 ... step 3845 loss 2.35850000381\n",
      "Iteration 3845 ... step 3846 loss 2.17187595367\n",
      "Iteration 3846 ... step 3847 loss 2.18031382561\n",
      "Iteration 3847 ... step 3848 loss 2.21879577637\n",
      "Iteration 3848 ... step 3849 loss 2.18395900726\n",
      "Iteration 3849 ... step 3850 loss 2.33874034882\n",
      "Iteration 3850 ... step 3851 loss 1.96855783463\n",
      "Iteration 3851 ... step 3852 loss 2.37564563751\n",
      "Iteration 3852 ... step 3853 loss 2.45073318481\n",
      "Iteration 3853 ... step 3854 loss 2.12206268311\n",
      "Iteration 3854 ... step 3855 loss 2.21330118179\n",
      "Iteration 3855 ... step 3856 loss 2.28632998466\n",
      "Iteration 3856 ... step 3857 loss 2.45471525192\n",
      "Iteration 3857 ... step 3858 loss 2.46090316772\n",
      "Iteration 3858 ... step 3859 loss 2.22295951843\n",
      "Iteration 3859 ... step 3860 loss 2.17325091362\n",
      "Iteration 3860 ... step 3861 loss 2.32502889633\n",
      "Iteration 3861 ... step 3862 loss 2.15521287918\n",
      "Iteration 3862 ... step 3863 loss 2.1886305809\n",
      "Iteration 3863 ... step 3864 loss 2.29102945328\n",
      "Iteration 3864 ... step 3865 loss 2.33540821075\n",
      "Iteration 3865 ... step 3866 loss 2.21562290192\n",
      "Iteration 3866 ... step 3867 loss 2.2762401104\n",
      "Iteration 3867 ... step 3868 loss 2.24622631073\n",
      "Iteration 3868 ... step 3869 loss 2.26739120483\n",
      "Iteration 3869 ... step 3870 loss 2.29845094681\n",
      "Iteration 3870 ... step 3871 loss 2.21985054016\n",
      "Iteration 3871 ... step 3872 loss 2.05005574226\n",
      "Iteration 3872 ... step 3873 loss 2.18631362915\n",
      "Iteration 3873 ... step 3874 loss 2.44173336029\n",
      "Iteration 3874 ... step 3875 loss 2.29802155495\n",
      "Iteration 3875 ... step 3876 loss 2.12395429611\n",
      "Iteration 3876 ... step 3877 loss 2.19457960129\n",
      "Iteration 3877 ... step 3878 loss 2.08636951447\n",
      "Iteration 3878 ... step 3879 loss 2.15903902054\n",
      "Iteration 3879 ... step 3880 loss 2.16518616676\n",
      "Iteration 3880 ... step 3881 loss 2.2291765213\n",
      "Iteration 3881 ... step 3882 loss 2.34648013115\n",
      "Iteration 3882 ... step 3883 loss 2.23220014572\n",
      "Iteration 3883 ... step 3884 loss 2.16779470444\n",
      "Iteration 3884 ... step 3885 loss 2.12803173065\n",
      "Iteration 3885 ... step 3886 loss 2.28351402283\n",
      "Iteration 3886 ... step 3887 loss 2.22549939156\n",
      "Iteration 3887 ... step 3888 loss 2.20867013931\n",
      "Iteration 3888 ... step 3889 loss 2.34280729294\n",
      "Iteration 3889 ... step 3890 loss 2.30025935173\n",
      "Iteration 3890 ... step 3891 loss 2.13471651077\n",
      "Iteration 3891 ... step 3892 loss 2.09999847412\n",
      "Iteration 3892 ... step 3893 loss 2.40535306931\n",
      "Iteration 3893 ... step 3894 loss 2.39991235733\n",
      "Iteration 3894 ... step 3895 loss 2.34026408195\n",
      "Iteration 3895 ... step 3896 loss 2.45817232132\n",
      "Iteration 3896 ... step 3897 loss 2.22715568542\n",
      "Iteration 3897 ... step 3898 loss 2.02577495575\n",
      "Iteration 3898 ... step 3899 loss 2.27443647385\n",
      "Iteration 3899 ... step 3900 loss 2.34558296204\n",
      "Iteration 3900 ... step 3901 loss 2.2600851059\n",
      "Iteration 3901 ... step 3902 loss 2.0956492424\n",
      "Iteration 3902 ... step 3903 loss 2.28667879105\n",
      "Iteration 3903 ... step 3904 loss 2.17199444771\n",
      "Iteration 3904 ... step 3905 loss 2.31472301483\n",
      "Iteration 3905 ... step 3906 loss 2.12058854103\n",
      "Iteration 3906 ... step 3907 loss 2.17384195328\n",
      "Iteration 3907 ... step 3908 loss 2.26070356369\n",
      "Iteration 3908 ... step 3909 loss 2.22602367401\n",
      "Iteration 3909 ... step 3910 loss 2.28172969818\n",
      "Iteration 3910 ... step 3911 loss 2.21681547165\n",
      "Iteration 3911 ... step 3912 loss 2.11512565613\n",
      "Iteration 3912 ... step 3913 loss 2.18309640884\n",
      "Iteration 3913 ... step 3914 loss 2.15144634247\n",
      "Iteration 3914 ... step 3915 loss 2.38853979111\n",
      "Iteration 3915 ... step 3916 loss 2.30402183533\n",
      "Iteration 3916 ... step 3917 loss 2.38991165161\n",
      "Iteration 3917 ... step 3918 loss 2.27106523514\n",
      "Iteration 3918 ... step 3919 loss 1.92499530315\n",
      "Iteration 3919 ... step 3920 loss 2.23154163361\n",
      "Iteration 3920 ... step 3921 loss 2.14736557007\n",
      "Iteration 3921 ... step 3922 loss 2.17812538147\n",
      "Iteration 3922 ... step 3923 loss 2.32959890366\n",
      "Iteration 3923 ... step 3924 loss 2.3771739006\n",
      "Iteration 3924 ... step 3925 loss 2.53006219864\n",
      "Iteration 3925 ... step 3926 loss 2.42583703995\n",
      "Iteration 3926 ... step 3927 loss 2.23291683197\n",
      "Iteration 3927 ... step 3928 loss 2.28791928291\n",
      "Iteration 3928 ... step 3929 loss 2.32719278336\n",
      "Iteration 3929 ... step 3930 loss 2.1543545723\n",
      "Iteration 3930 ... step 3931 loss 2.40083456039\n",
      "Iteration 3931 ... step 3932 loss 2.17556476593\n",
      "Iteration 3932 ... step 3933 loss 2.3183016777\n",
      "Iteration 3933 ... step 3934 loss 2.17938876152\n",
      "Iteration 3934 ... step 3935 loss 2.10840845108\n",
      "Iteration 3935 ... step 3936 loss 2.19286060333\n",
      "Iteration 3936 ... step 3937 loss 2.14743947983\n",
      "Iteration 3937 ... step 3938 loss 2.42181348801\n",
      "Iteration 3938 ... step 3939 loss 2.32278251648\n",
      "Iteration 3939 ... step 3940 loss 2.24300718307\n",
      "Iteration 3940 ... step 3941 loss 2.16571521759\n",
      "Iteration 3941 ... step 3942 loss 1.97554540634\n",
      "Iteration 3942 ... step 3943 loss 2.27907657623\n",
      "Iteration 3943 ... step 3944 loss 2.21526002884\n",
      "Iteration 3944 ... step 3945 loss 2.31200361252\n",
      "Iteration 3945 ... step 3946 loss 2.25646162033\n",
      "Iteration 3946 ... step 3947 loss 2.14962649345\n",
      "Iteration 3947 ... step 3948 loss 2.12095379829\n",
      "Iteration 3948 ... step 3949 loss 2.2456870079\n",
      "Iteration 3949 ... step 3950 loss 2.26798081398\n",
      "Iteration 3950 ... step 3951 loss 2.09935617447\n",
      "Iteration 3951 ... step 3952 loss 2.37336468697\n",
      "Iteration 3952 ... step 3953 loss 2.34363937378\n",
      "Iteration 3953 ... step 3954 loss 2.46070051193\n",
      "Iteration 3954 ... step 3955 loss 2.12412977219\n",
      "Iteration 3955 ... step 3956 loss 2.40175104141\n",
      "Iteration 3956 ... step 3957 loss 2.12364625931\n",
      "Iteration 3957 ... step 3958 loss 2.14807033539\n",
      "Iteration 3958 ... step 3959 loss 2.29435873032\n",
      "Iteration 3959 ... step 3960 loss 2.25613498688\n",
      "Iteration 3960 ... step 3961 loss 2.27711009979\n",
      "Iteration 3961 ... step 3962 loss 2.08586645126\n",
      "Iteration 3962 ... step 3963 loss 2.37552881241\n",
      "Iteration 3963 ... step 3964 loss 2.24454784393\n",
      "Iteration 3964 ... step 3965 loss 2.10586285591\n",
      "Iteration 3965 ... step 3966 loss 2.29278063774\n",
      "Iteration 3966 ... step 3967 loss 2.40631580353\n",
      "Iteration 3967 ... step 3968 loss 2.32747030258\n",
      "Iteration 3968 ... step 3969 loss 2.19615030289\n",
      "Iteration 3969 ... step 3970 loss 2.2989783287\n",
      "Iteration 3970 ... step 3971 loss 2.09394741058\n",
      "Iteration 3971 ... step 3972 loss 2.20725488663\n",
      "Iteration 3972 ... step 3973 loss 2.29438304901\n",
      "Iteration 3973 ... step 3974 loss 2.12125182152\n",
      "Iteration 3974 ... step 3975 loss 2.2295320034\n",
      "Iteration 3975 ... step 3976 loss 2.49180984497\n",
      "Iteration 3976 ... step 3977 loss 2.23424625397\n",
      "Iteration 3977 ... step 3978 loss 2.14987134933\n",
      "Iteration 3978 ... step 3979 loss 2.23531007767\n",
      "Iteration 3979 ... step 3980 loss 2.31159973145\n",
      "Iteration 3980 ... step 3981 loss 2.34975671768\n",
      "Iteration 3981 ... step 3982 loss 2.19874572754\n",
      "Iteration 3982 ... step 3983 loss 2.24195718765\n",
      "Iteration 3983 ... step 3984 loss 2.31472682953\n",
      "Iteration 3984 ... step 3985 loss 2.26850891113\n",
      "Iteration 3985 ... step 3986 loss 2.30474472046\n",
      "Iteration 3986 ... step 3987 loss 2.21789932251\n",
      "Iteration 3987 ... step 3988 loss 2.26030015945\n",
      "Iteration 3988 ... step 3989 loss 2.47002983093\n",
      "Iteration 3989 ... step 3990 loss 2.24663734436\n",
      "Iteration 3990 ... step 3991 loss 2.31844615936\n",
      "Iteration 3991 ... step 3992 loss 2.13491630554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3992 ... step 3993 loss 2.38815402985\n",
      "Iteration 3993 ... step 3994 loss 2.0404651165\n",
      "Iteration 3994 ... step 3995 loss 2.3652677536\n",
      "Iteration 3995 ... step 3996 loss 2.46602869034\n",
      "Iteration 3996 ... step 3997 loss 2.27634310722\n",
      "Iteration 3997 ... step 3998 loss 2.25155448914\n",
      "Iteration 3998 ... step 3999 loss 2.20129299164\n",
      "Iteration 3999 ... step 4000 loss 2.1582608223\n",
      "Iteration 4000 ... step 4001 loss 2.12885713577\n",
      "Minibatch loss at step 4000: 2.128857\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 4001 ... step 4002 loss 2.22766590118\n",
      "Iteration 4002 ... step 4003 loss 2.07173252106\n",
      "Iteration 4003 ... step 4004 loss 2.31889009476\n",
      "Iteration 4004 ... step 4005 loss 2.41744661331\n",
      "Iteration 4005 ... step 4006 loss 2.02735710144\n",
      "Iteration 4006 ... step 4007 loss 2.20852971077\n",
      "Iteration 4007 ... step 4008 loss 2.39478349686\n",
      "Iteration 4008 ... step 4009 loss 2.2167570591\n",
      "Iteration 4009 ... step 4010 loss 2.25523138046\n",
      "Iteration 4010 ... step 4011 loss 2.51251983643\n",
      "Iteration 4011 ... step 4012 loss 1.98615193367\n",
      "Iteration 4012 ... step 4013 loss 2.12143683434\n",
      "Iteration 4013 ... step 4014 loss 2.32297468185\n",
      "Iteration 4014 ... step 4015 loss 2.3919262886\n",
      "Iteration 4015 ... step 4016 loss 2.07487130165\n",
      "Iteration 4016 ... step 4017 loss 2.40647745132\n",
      "Iteration 4017 ... step 4018 loss 2.4036154747\n",
      "Iteration 4018 ... step 4019 loss 2.37068843842\n",
      "Iteration 4019 ... step 4020 loss 2.35084867477\n",
      "Iteration 4020 ... step 4021 loss 2.22054123878\n",
      "Iteration 4021 ... step 4022 loss 2.14721894264\n",
      "Iteration 4022 ... step 4023 loss 2.09418344498\n",
      "Iteration 4023 ... step 4024 loss 2.20714187622\n",
      "Iteration 4024 ... step 4025 loss 2.44056797028\n",
      "Iteration 4025 ... step 4026 loss 2.28490352631\n",
      "Iteration 4026 ... step 4027 loss 2.26578068733\n",
      "Iteration 4027 ... step 4028 loss 2.5387673378\n",
      "Iteration 4028 ... step 4029 loss 2.24105358124\n",
      "Iteration 4029 ... step 4030 loss 2.26432275772\n",
      "Iteration 4030 ... step 4031 loss 2.27267599106\n",
      "Iteration 4031 ... step 4032 loss 2.04452061653\n",
      "Iteration 4032 ... step 4033 loss 2.52789497375\n",
      "Iteration 4033 ... step 4034 loss 2.2386636734\n",
      "Iteration 4034 ... step 4035 loss 2.17979311943\n",
      "Iteration 4035 ... step 4036 loss 2.18567419052\n",
      "Iteration 4036 ... step 4037 loss 2.34207344055\n",
      "Iteration 4037 ... step 4038 loss 2.1720867157\n",
      "Iteration 4038 ... step 4039 loss 2.30955457687\n",
      "Iteration 4039 ... step 4040 loss 2.26195764542\n",
      "Iteration 4040 ... step 4041 loss 2.18042325974\n",
      "Iteration 4041 ... step 4042 loss 2.41216754913\n",
      "Iteration 4042 ... step 4043 loss 2.01601958275\n",
      "Iteration 4043 ... step 4044 loss 2.29613685608\n",
      "Iteration 4044 ... step 4045 loss 2.23299503326\n",
      "Iteration 4045 ... step 4046 loss 2.18608999252\n",
      "Iteration 4046 ... step 4047 loss 2.27310395241\n",
      "Iteration 4047 ... step 4048 loss 2.41281318665\n",
      "Iteration 4048 ... step 4049 loss 2.14107394218\n",
      "Iteration 4049 ... step 4050 loss 2.13633704185\n",
      "Iteration 4050 ... step 4051 loss 2.28900671005\n",
      "Iteration 4051 ... step 4052 loss 2.25392913818\n",
      "Iteration 4052 ... step 4053 loss 2.34950566292\n",
      "Iteration 4053 ... step 4054 loss 2.1604244709\n",
      "Iteration 4054 ... step 4055 loss 2.20169401169\n",
      "Iteration 4055 ... step 4056 loss 2.29232406616\n",
      "Iteration 4056 ... step 4057 loss 2.28019618988\n",
      "Iteration 4057 ... step 4058 loss 2.35183286667\n",
      "Iteration 4058 ... step 4059 loss 2.257396698\n",
      "Iteration 4059 ... step 4060 loss 2.07252883911\n",
      "Iteration 4060 ... step 4061 loss 2.26826047897\n",
      "Iteration 4061 ... step 4062 loss 2.4298915863\n",
      "Iteration 4062 ... step 4063 loss 2.17245459557\n",
      "Iteration 4063 ... step 4064 loss 2.46908569336\n",
      "Iteration 4064 ... step 4065 loss 2.34394836426\n",
      "Iteration 4065 ... step 4066 loss 2.53871822357\n",
      "Iteration 4066 ... step 4067 loss 2.31454896927\n",
      "Iteration 4067 ... step 4068 loss 2.33336496353\n",
      "Iteration 4068 ... step 4069 loss 2.18366241455\n",
      "Iteration 4069 ... step 4070 loss 2.27190518379\n",
      "Iteration 4070 ... step 4071 loss 2.31507277489\n",
      "Iteration 4071 ... step 4072 loss 2.24450588226\n",
      "Iteration 4072 ... step 4073 loss 2.35091114044\n",
      "Iteration 4073 ... step 4074 loss 2.17558550835\n",
      "Iteration 4074 ... step 4075 loss 2.25269317627\n",
      "Iteration 4075 ... step 4076 loss 2.26771688461\n",
      "Iteration 4076 ... step 4077 loss 2.35235691071\n",
      "Iteration 4077 ... step 4078 loss 2.35061311722\n",
      "Iteration 4078 ... step 4079 loss 2.5336663723\n",
      "Iteration 4079 ... step 4080 loss 2.28495359421\n",
      "Iteration 4080 ... step 4081 loss 2.47516798973\n",
      "Iteration 4081 ... step 4082 loss 2.20152235031\n",
      "Iteration 4082 ... step 4083 loss 2.41154527664\n",
      "Iteration 4083 ... step 4084 loss 2.30912446976\n",
      "Iteration 4084 ... step 4085 loss 2.18332481384\n",
      "Iteration 4085 ... step 4086 loss 2.15772342682\n",
      "Iteration 4086 ... step 4087 loss 2.31440067291\n",
      "Iteration 4087 ... step 4088 loss 2.08188152313\n",
      "Iteration 4088 ... step 4089 loss 2.15229034424\n",
      "Iteration 4089 ... step 4090 loss 2.01061224937\n",
      "Iteration 4090 ... step 4091 loss 2.21233272552\n",
      "Iteration 4091 ... step 4092 loss 2.29428386688\n",
      "Iteration 4092 ... step 4093 loss 2.23102712631\n",
      "Iteration 4093 ... step 4094 loss 2.40179014206\n",
      "Iteration 4094 ... step 4095 loss 2.2684442997\n",
      "Iteration 4095 ... step 4096 loss 2.38804888725\n",
      "Iteration 4096 ... step 4097 loss 2.10072088242\n",
      "Iteration 4097 ... step 4098 loss 2.27115774155\n",
      "Iteration 4098 ... step 4099 loss 2.3223824501\n",
      "Iteration 4099 ... step 4100 loss 2.61846971512\n",
      "Iteration 4100 ... step 4101 loss 2.29202032089\n",
      "Iteration 4101 ... step 4102 loss 2.23832011223\n",
      "Iteration 4102 ... step 4103 loss 2.27671861649\n",
      "Iteration 4103 ... step 4104 loss 2.24899482727\n",
      "Iteration 4104 ... step 4105 loss 2.14877867699\n",
      "Iteration 4105 ... step 4106 loss 2.29590439796\n",
      "Iteration 4106 ... step 4107 loss 2.12855029106\n",
      "Iteration 4107 ... step 4108 loss 2.12107563019\n",
      "Iteration 4108 ... step 4109 loss 2.19989871979\n",
      "Iteration 4109 ... step 4110 loss 2.17746806145\n",
      "Iteration 4110 ... step 4111 loss 2.2432923317\n",
      "Iteration 4111 ... step 4112 loss 2.1757850647\n",
      "Iteration 4112 ... step 4113 loss 2.22704458237\n",
      "Iteration 4113 ... step 4114 loss 2.15148067474\n",
      "Iteration 4114 ... step 4115 loss 2.2111082077\n",
      "Iteration 4115 ... step 4116 loss 2.2897503376\n",
      "Iteration 4116 ... step 4117 loss 2.26089668274\n",
      "Iteration 4117 ... step 4118 loss 2.2411365509\n",
      "Iteration 4118 ... step 4119 loss 2.26815533638\n",
      "Iteration 4119 ... step 4120 loss 2.30726337433\n",
      "Iteration 4120 ... step 4121 loss 2.12003612518\n",
      "Iteration 4121 ... step 4122 loss 2.2203772068\n",
      "Iteration 4122 ... step 4123 loss 2.22568798065\n",
      "Iteration 4123 ... step 4124 loss 2.1659617424\n",
      "Iteration 4124 ... step 4125 loss 2.29817271233\n",
      "Iteration 4125 ... step 4126 loss 2.36375236511\n",
      "Iteration 4126 ... step 4127 loss 2.22622346878\n",
      "Iteration 4127 ... step 4128 loss 2.1639995575\n",
      "Iteration 4128 ... step 4129 loss 2.22344827652\n",
      "Iteration 4129 ... step 4130 loss 2.29350376129\n",
      "Iteration 4130 ... step 4131 loss 2.17153930664\n",
      "Iteration 4131 ... step 4132 loss 2.23575735092\n",
      "Iteration 4132 ... step 4133 loss 2.11051464081\n",
      "Iteration 4133 ... step 4134 loss 2.16770148277\n",
      "Iteration 4134 ... step 4135 loss 2.24996852875\n",
      "Iteration 4135 ... step 4136 loss 2.37796020508\n",
      "Iteration 4136 ... step 4137 loss 2.31875514984\n",
      "Iteration 4137 ... step 4138 loss 2.19421815872\n",
      "Iteration 4138 ... step 4139 loss 2.34241485596\n",
      "Iteration 4139 ... step 4140 loss 2.19074177742\n",
      "Iteration 4140 ... step 4141 loss 2.41199874878\n",
      "Iteration 4141 ... step 4142 loss 2.19100475311\n",
      "Iteration 4142 ... step 4143 loss 2.17500948906\n",
      "Iteration 4143 ... step 4144 loss 2.16156291962\n",
      "Iteration 4144 ... step 4145 loss 2.31628108025\n",
      "Iteration 4145 ... step 4146 loss 2.25244092941\n",
      "Iteration 4146 ... step 4147 loss 2.40598869324\n",
      "Iteration 4147 ... step 4148 loss 2.24610567093\n",
      "Iteration 4148 ... step 4149 loss 2.12666273117\n",
      "Iteration 4149 ... step 4150 loss 2.28474831581\n",
      "Iteration 4150 ... step 4151 loss 2.36318135262\n",
      "Iteration 4151 ... step 4152 loss 2.12306642532\n",
      "Iteration 4152 ... step 4153 loss 2.31592273712\n",
      "Iteration 4153 ... step 4154 loss 2.39186215401\n",
      "Iteration 4154 ... step 4155 loss 2.36480522156\n",
      "Iteration 4155 ... step 4156 loss 2.40201377869\n",
      "Iteration 4156 ... step 4157 loss 2.20740270615\n",
      "Iteration 4157 ... step 4158 loss 2.35006546974\n",
      "Iteration 4158 ... step 4159 loss 2.34326267242\n",
      "Iteration 4159 ... step 4160 loss 2.12796235085\n",
      "Iteration 4160 ... step 4161 loss 2.40392947197\n",
      "Iteration 4161 ... step 4162 loss 2.28690004349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4162 ... step 4163 loss 2.40405464172\n",
      "Iteration 4163 ... step 4164 loss 1.9656701088\n",
      "Iteration 4164 ... step 4165 loss 2.24430656433\n",
      "Iteration 4165 ... step 4166 loss 2.40727233887\n",
      "Iteration 4166 ... step 4167 loss 2.43100690842\n",
      "Iteration 4167 ... step 4168 loss 2.40746736526\n",
      "Iteration 4168 ... step 4169 loss 2.27206516266\n",
      "Iteration 4169 ... step 4170 loss 2.30111885071\n",
      "Iteration 4170 ... step 4171 loss 2.2162399292\n",
      "Iteration 4171 ... step 4172 loss 2.25551319122\n",
      "Iteration 4172 ... step 4173 loss 2.14721250534\n",
      "Iteration 4173 ... step 4174 loss 2.41800069809\n",
      "Iteration 4174 ... step 4175 loss 2.44130468369\n",
      "Iteration 4175 ... step 4176 loss 2.4920835495\n",
      "Iteration 4176 ... step 4177 loss 2.14889454842\n",
      "Iteration 4177 ... step 4178 loss 2.41633224487\n",
      "Iteration 4178 ... step 4179 loss 2.22031092644\n",
      "Iteration 4179 ... step 4180 loss 2.16749286652\n",
      "Iteration 4180 ... step 4181 loss 2.23188591003\n",
      "Iteration 4181 ... step 4182 loss 2.47765350342\n",
      "Iteration 4182 ... step 4183 loss 2.30977869034\n",
      "Iteration 4183 ... step 4184 loss 2.32551717758\n",
      "Iteration 4184 ... step 4185 loss 2.40749025345\n",
      "Iteration 4185 ... step 4186 loss 2.20485901833\n",
      "Iteration 4186 ... step 4187 loss 2.17930030823\n",
      "Iteration 4187 ... step 4188 loss 2.3235013485\n",
      "Iteration 4188 ... step 4189 loss 2.22618603706\n",
      "Iteration 4189 ... step 4190 loss 2.23684024811\n",
      "Iteration 4190 ... step 4191 loss 2.15312767029\n",
      "Iteration 4191 ... step 4192 loss 2.05127930641\n",
      "Iteration 4192 ... step 4193 loss 2.22736930847\n",
      "Iteration 4193 ... step 4194 loss 2.23477363586\n",
      "Iteration 4194 ... step 4195 loss 2.1859498024\n",
      "Iteration 4195 ... step 4196 loss 2.14238762856\n",
      "Iteration 4196 ... step 4197 loss 2.39640903473\n",
      "Iteration 4197 ... step 4198 loss 2.14219713211\n",
      "Iteration 4198 ... step 4199 loss 2.17920970917\n",
      "Iteration 4199 ... step 4200 loss 2.21534252167\n",
      "Iteration 4200 ... step 4201 loss 2.16246175766\n",
      "Iteration 4201 ... step 4202 loss 2.13745546341\n",
      "Iteration 4202 ... step 4203 loss 2.25581741333\n",
      "Iteration 4203 ... step 4204 loss 2.42724990845\n",
      "Iteration 4204 ... step 4205 loss 2.24758291245\n",
      "Iteration 4205 ... step 4206 loss 2.26505947113\n",
      "Iteration 4206 ... step 4207 loss 2.40525531769\n",
      "Iteration 4207 ... step 4208 loss 2.20322036743\n",
      "Iteration 4208 ... step 4209 loss 2.06514191628\n",
      "Iteration 4209 ... step 4210 loss 2.35380077362\n",
      "Iteration 4210 ... step 4211 loss 2.35930347443\n",
      "Iteration 4211 ... step 4212 loss 2.30493402481\n",
      "Iteration 4212 ... step 4213 loss 2.26268005371\n",
      "Iteration 4213 ... step 4214 loss 2.26400899887\n",
      "Iteration 4214 ... step 4215 loss 2.11020612717\n",
      "Iteration 4215 ... step 4216 loss 2.28639554977\n",
      "Iteration 4216 ... step 4217 loss 2.19934058189\n",
      "Iteration 4217 ... step 4218 loss 2.4884762764\n",
      "Iteration 4218 ... step 4219 loss 2.40717363358\n",
      "Iteration 4219 ... step 4220 loss 2.14319396019\n",
      "Iteration 4220 ... step 4221 loss 2.24320220947\n",
      "Iteration 4221 ... step 4222 loss 2.26573133469\n",
      "Iteration 4222 ... step 4223 loss 2.18948364258\n",
      "Iteration 4223 ... step 4224 loss 2.27200675011\n",
      "Iteration 4224 ... step 4225 loss 2.16322350502\n",
      "Iteration 4225 ... step 4226 loss 2.4307808876\n",
      "Iteration 4226 ... step 4227 loss 2.43166589737\n",
      "Iteration 4227 ... step 4228 loss 2.05831193924\n",
      "Iteration 4228 ... step 4229 loss 2.17458701134\n",
      "Iteration 4229 ... step 4230 loss 2.0284717083\n",
      "Iteration 4230 ... step 4231 loss 2.12133193016\n",
      "Iteration 4231 ... step 4232 loss 2.33642673492\n",
      "Iteration 4232 ... step 4233 loss 2.23335957527\n",
      "Iteration 4233 ... step 4234 loss 2.37190818787\n",
      "Iteration 4234 ... step 4235 loss 2.34122872353\n",
      "Iteration 4235 ... step 4236 loss 2.23422622681\n",
      "Iteration 4236 ... step 4237 loss 2.34957742691\n",
      "Iteration 4237 ... step 4238 loss 2.25750017166\n",
      "Iteration 4238 ... step 4239 loss 2.2450928688\n",
      "Iteration 4239 ... step 4240 loss 2.16989994049\n",
      "Iteration 4240 ... step 4241 loss 2.31800079346\n",
      "Iteration 4241 ... step 4242 loss 2.24784231186\n",
      "Iteration 4242 ... step 4243 loss 2.33248329163\n",
      "Iteration 4243 ... step 4244 loss 2.30250549316\n",
      "Iteration 4244 ... step 4245 loss 2.13187241554\n",
      "Iteration 4245 ... step 4246 loss 2.21756505966\n",
      "Iteration 4246 ... step 4247 loss 2.15281248093\n",
      "Iteration 4247 ... step 4248 loss 2.27062678337\n",
      "Iteration 4248 ... step 4249 loss 2.23865556717\n",
      "Iteration 4249 ... step 4250 loss 2.18119311333\n",
      "Iteration 4250 ... step 4251 loss 2.05878400803\n",
      "Iteration 4251 ... step 4252 loss 2.35424280167\n",
      "Iteration 4252 ... step 4253 loss 2.20801544189\n",
      "Iteration 4253 ... step 4254 loss 2.21616744995\n",
      "Iteration 4254 ... step 4255 loss 1.97196292877\n",
      "Iteration 4255 ... step 4256 loss 2.30568933487\n",
      "Iteration 4256 ... step 4257 loss 2.29198503494\n",
      "Iteration 4257 ... step 4258 loss 2.33104372025\n",
      "Iteration 4258 ... step 4259 loss 2.32711982727\n",
      "Iteration 4259 ... step 4260 loss 2.19201850891\n",
      "Iteration 4260 ... step 4261 loss 2.25893497467\n",
      "Iteration 4261 ... step 4262 loss 2.3702788353\n",
      "Iteration 4262 ... step 4263 loss 2.30744552612\n",
      "Iteration 4263 ... step 4264 loss 2.08502316475\n",
      "Iteration 4264 ... step 4265 loss 2.14442825317\n",
      "Iteration 4265 ... step 4266 loss 2.26301956177\n",
      "Iteration 4266 ... step 4267 loss 2.15734529495\n",
      "Iteration 4267 ... step 4268 loss 2.13719725609\n",
      "Iteration 4268 ... step 4269 loss 2.12703180313\n",
      "Iteration 4269 ... step 4270 loss 2.36533308029\n",
      "Iteration 4270 ... step 4271 loss 2.28203058243\n",
      "Iteration 4271 ... step 4272 loss 2.26063036919\n",
      "Iteration 4272 ... step 4273 loss 2.35332608223\n",
      "Iteration 4273 ... step 4274 loss 2.22765016556\n",
      "Iteration 4274 ... step 4275 loss 2.11375427246\n",
      "Iteration 4275 ... step 4276 loss 2.40252542496\n",
      "Iteration 4276 ... step 4277 loss 2.12051820755\n",
      "Iteration 4277 ... step 4278 loss 2.1682806015\n",
      "Iteration 4278 ... step 4279 loss 2.34221696854\n",
      "Iteration 4279 ... step 4280 loss 2.10972046852\n",
      "Iteration 4280 ... step 4281 loss 2.20343732834\n",
      "Iteration 4281 ... step 4282 loss 2.09920692444\n",
      "Iteration 4282 ... step 4283 loss 2.37863540649\n",
      "Iteration 4283 ... step 4284 loss 2.34347677231\n",
      "Iteration 4284 ... step 4285 loss 2.11110067368\n",
      "Iteration 4285 ... step 4286 loss 2.21941518784\n",
      "Iteration 4286 ... step 4287 loss 2.22888660431\n",
      "Iteration 4287 ... step 4288 loss 2.6568236351\n",
      "Iteration 4288 ... step 4289 loss 2.06860017776\n",
      "Iteration 4289 ... step 4290 loss 2.17771434784\n",
      "Iteration 4290 ... step 4291 loss 2.33489227295\n",
      "Iteration 4291 ... step 4292 loss 2.33396887779\n",
      "Iteration 4292 ... step 4293 loss 2.43088984489\n",
      "Iteration 4293 ... step 4294 loss 2.24244213104\n",
      "Iteration 4294 ... step 4295 loss 2.37363219261\n",
      "Iteration 4295 ... step 4296 loss 2.0415802002\n",
      "Iteration 4296 ... step 4297 loss 2.44462442398\n",
      "Iteration 4297 ... step 4298 loss 2.20720124245\n",
      "Iteration 4298 ... step 4299 loss 2.09733963013\n",
      "Iteration 4299 ... step 4300 loss 2.1319463253\n",
      "Iteration 4300 ... step 4301 loss 2.03666687012\n",
      "Iteration 4301 ... step 4302 loss 2.32988119125\n",
      "Iteration 4302 ... step 4303 loss 2.25475311279\n",
      "Iteration 4303 ... step 4304 loss 2.16181540489\n",
      "Iteration 4304 ... step 4305 loss 2.21839332581\n",
      "Iteration 4305 ... step 4306 loss 2.19815349579\n",
      "Iteration 4306 ... step 4307 loss 2.23891162872\n",
      "Iteration 4307 ... step 4308 loss 2.45520114899\n",
      "Iteration 4308 ... step 4309 loss 2.26158714294\n",
      "Iteration 4309 ... step 4310 loss 2.39264631271\n",
      "Iteration 4310 ... step 4311 loss 2.32080245018\n",
      "Iteration 4311 ... step 4312 loss 2.2751994133\n",
      "Iteration 4312 ... step 4313 loss 2.33275842667\n",
      "Iteration 4313 ... step 4314 loss 2.33028721809\n",
      "Iteration 4314 ... step 4315 loss 2.16835546494\n",
      "Iteration 4315 ... step 4316 loss 2.24643373489\n",
      "Iteration 4316 ... step 4317 loss 2.16189646721\n",
      "Iteration 4317 ... step 4318 loss 2.12798595428\n",
      "Iteration 4318 ... step 4319 loss 2.29337835312\n",
      "Iteration 4319 ... step 4320 loss 2.17495918274\n",
      "Iteration 4320 ... step 4321 loss 2.18615055084\n",
      "Iteration 4321 ... step 4322 loss 2.20820069313\n",
      "Iteration 4322 ... step 4323 loss 2.22371816635\n",
      "Iteration 4323 ... step 4324 loss 2.39365673065\n",
      "Iteration 4324 ... step 4325 loss 2.28591775894\n",
      "Iteration 4325 ... step 4326 loss 2.35313510895\n",
      "Iteration 4326 ... step 4327 loss 2.38045835495\n",
      "Iteration 4327 ... step 4328 loss 2.36950349808\n",
      "Iteration 4328 ... step 4329 loss 2.03820729256\n",
      "Iteration 4329 ... step 4330 loss 2.26126480103\n",
      "Iteration 4330 ... step 4331 loss 2.22928476334\n",
      "Iteration 4331 ... step 4332 loss 2.61513137817\n",
      "Iteration 4332 ... step 4333 loss 2.07552242279\n",
      "Iteration 4333 ... step 4334 loss 2.23085784912\n",
      "Iteration 4334 ... step 4335 loss 2.2458717823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4335 ... step 4336 loss 2.14153862\n",
      "Iteration 4336 ... step 4337 loss 2.38944125175\n",
      "Iteration 4337 ... step 4338 loss 2.2693798542\n",
      "Iteration 4338 ... step 4339 loss 2.17234420776\n",
      "Iteration 4339 ... step 4340 loss 2.21367740631\n",
      "Iteration 4340 ... step 4341 loss 2.23308753967\n",
      "Iteration 4341 ... step 4342 loss 2.23752641678\n",
      "Iteration 4342 ... step 4343 loss 2.26184558868\n",
      "Iteration 4343 ... step 4344 loss 2.27874994278\n",
      "Iteration 4344 ... step 4345 loss 2.21308898926\n",
      "Iteration 4345 ... step 4346 loss 2.10750770569\n",
      "Iteration 4346 ... step 4347 loss 2.29979085922\n",
      "Iteration 4347 ... step 4348 loss 2.12385749817\n",
      "Iteration 4348 ... step 4349 loss 2.24344730377\n",
      "Iteration 4349 ... step 4350 loss 1.96873474121\n",
      "Iteration 4350 ... step 4351 loss 2.07800459862\n",
      "Iteration 4351 ... step 4352 loss 2.30914783478\n",
      "Iteration 4352 ... step 4353 loss 2.25379419327\n",
      "Iteration 4353 ... step 4354 loss 2.23103761673\n",
      "Iteration 4354 ... step 4355 loss 2.15904355049\n",
      "Iteration 4355 ... step 4356 loss 2.29639148712\n",
      "Iteration 4356 ... step 4357 loss 2.21212100983\n",
      "Iteration 4357 ... step 4358 loss 2.03707122803\n",
      "Iteration 4358 ... step 4359 loss 2.07813119888\n",
      "Iteration 4359 ... step 4360 loss 2.28839826584\n",
      "Iteration 4360 ... step 4361 loss 2.28649187088\n",
      "Iteration 4361 ... step 4362 loss 2.27723193169\n",
      "Iteration 4362 ... step 4363 loss 2.0946688652\n",
      "Iteration 4363 ... step 4364 loss 2.37679195404\n",
      "Iteration 4364 ... step 4365 loss 2.17795157433\n",
      "Iteration 4365 ... step 4366 loss 2.36955952644\n",
      "Iteration 4366 ... step 4367 loss 2.2395529747\n",
      "Iteration 4367 ... step 4368 loss 2.26646518707\n",
      "Iteration 4368 ... step 4369 loss 2.33298778534\n",
      "Iteration 4369 ... step 4370 loss 2.37981748581\n",
      "Iteration 4370 ... step 4371 loss 2.15653085709\n",
      "Iteration 4371 ... step 4372 loss 2.25822758675\n",
      "Iteration 4372 ... step 4373 loss 2.44734811783\n",
      "Iteration 4373 ... step 4374 loss 2.22054052353\n",
      "Iteration 4374 ... step 4375 loss 2.34241628647\n",
      "Iteration 4375 ... step 4376 loss 2.01795291901\n",
      "Iteration 4376 ... step 4377 loss 2.20673513412\n",
      "Iteration 4377 ... step 4378 loss 2.15184378624\n",
      "Iteration 4378 ... step 4379 loss 2.38933801651\n",
      "Iteration 4379 ... step 4380 loss 2.37593603134\n",
      "Iteration 4380 ... step 4381 loss 2.20431661606\n",
      "Iteration 4381 ... step 4382 loss 2.08379888535\n",
      "Iteration 4382 ... step 4383 loss 2.2202091217\n",
      "Iteration 4383 ... step 4384 loss 2.0874710083\n",
      "Iteration 4384 ... step 4385 loss 2.31856918335\n",
      "Iteration 4385 ... step 4386 loss 2.41533565521\n",
      "Iteration 4386 ... step 4387 loss 2.26056623459\n",
      "Iteration 4387 ... step 4388 loss 2.29220509529\n",
      "Iteration 4388 ... step 4389 loss 2.23877859116\n",
      "Iteration 4389 ... step 4390 loss 2.33074975014\n",
      "Iteration 4390 ... step 4391 loss 2.35336732864\n",
      "Iteration 4391 ... step 4392 loss 2.43570709229\n",
      "Iteration 4392 ... step 4393 loss 2.23379397392\n",
      "Iteration 4393 ... step 4394 loss 2.27263212204\n",
      "Iteration 4394 ... step 4395 loss 2.19169831276\n",
      "Iteration 4395 ... step 4396 loss 2.13689231873\n",
      "Iteration 4396 ... step 4397 loss 2.17109060287\n",
      "Iteration 4397 ... step 4398 loss 2.61694431305\n",
      "Iteration 4398 ... step 4399 loss 2.23038291931\n",
      "Iteration 4399 ... step 4400 loss 2.37831163406\n",
      "Iteration 4400 ... step 4401 loss 2.46131420135\n",
      "Iteration 4401 ... step 4402 loss 2.13383102417\n",
      "Iteration 4402 ... step 4403 loss 2.11340045929\n",
      "Iteration 4403 ... step 4404 loss 2.18156766891\n",
      "Iteration 4404 ... step 4405 loss 2.46642494202\n",
      "Iteration 4405 ... step 4406 loss 2.30919647217\n",
      "Iteration 4406 ... step 4407 loss 2.19333577156\n",
      "Iteration 4407 ... step 4408 loss 2.22281169891\n",
      "Iteration 4408 ... step 4409 loss 2.12750387192\n",
      "Iteration 4409 ... step 4410 loss 2.12460565567\n",
      "Iteration 4410 ... step 4411 loss 2.32132911682\n",
      "Iteration 4411 ... step 4412 loss 2.2375793457\n",
      "Iteration 4412 ... step 4413 loss 2.33959817886\n",
      "Iteration 4413 ... step 4414 loss 1.95802354813\n",
      "Iteration 4414 ... step 4415 loss 2.27089118958\n",
      "Iteration 4415 ... step 4416 loss 2.226354599\n",
      "Iteration 4416 ... step 4417 loss 2.28641366959\n",
      "Iteration 4417 ... step 4418 loss 2.31963062286\n",
      "Iteration 4418 ... step 4419 loss 2.21325755119\n",
      "Iteration 4419 ... step 4420 loss 2.36896371841\n",
      "Iteration 4420 ... step 4421 loss 2.43044066429\n",
      "Iteration 4421 ... step 4422 loss 2.05797481537\n",
      "Iteration 4422 ... step 4423 loss 2.1335670948\n",
      "Iteration 4423 ... step 4424 loss 2.40123271942\n",
      "Iteration 4424 ... step 4425 loss 2.47376394272\n",
      "Iteration 4425 ... step 4426 loss 2.12673258781\n",
      "Iteration 4426 ... step 4427 loss 2.22139883041\n",
      "Iteration 4427 ... step 4428 loss 2.27983093262\n",
      "Iteration 4428 ... step 4429 loss 2.13710999489\n",
      "Iteration 4429 ... step 4430 loss 2.49192333221\n",
      "Iteration 4430 ... step 4431 loss 2.21154546738\n",
      "Iteration 4431 ... step 4432 loss 2.32737517357\n",
      "Iteration 4432 ... step 4433 loss 2.20846748352\n",
      "Iteration 4433 ... step 4434 loss 2.20337986946\n",
      "Iteration 4434 ... step 4435 loss 2.39238786697\n",
      "Iteration 4435 ... step 4436 loss 2.29626274109\n",
      "Iteration 4436 ... step 4437 loss 2.31316328049\n",
      "Iteration 4437 ... step 4438 loss 2.19509649277\n",
      "Iteration 4438 ... step 4439 loss 2.31014895439\n",
      "Iteration 4439 ... step 4440 loss 1.89255106449\n",
      "Iteration 4440 ... step 4441 loss 2.1977083683\n",
      "Iteration 4441 ... step 4442 loss 2.11729097366\n",
      "Iteration 4442 ... step 4443 loss 2.38335561752\n",
      "Iteration 4443 ... step 4444 loss 2.26317477226\n",
      "Iteration 4444 ... step 4445 loss 2.1315715313\n",
      "Iteration 4445 ... step 4446 loss 2.210896492\n",
      "Iteration 4446 ... step 4447 loss 2.39372348785\n",
      "Iteration 4447 ... step 4448 loss 2.24096918106\n",
      "Iteration 4448 ... step 4449 loss 2.23982691765\n",
      "Iteration 4449 ... step 4450 loss 2.25069093704\n",
      "Iteration 4450 ... step 4451 loss 2.18762564659\n",
      "Iteration 4451 ... step 4452 loss 2.21609926224\n",
      "Iteration 4452 ... step 4453 loss 2.37864637375\n",
      "Iteration 4453 ... step 4454 loss 2.13075041771\n",
      "Iteration 4454 ... step 4455 loss 2.19458055496\n",
      "Iteration 4455 ... step 4456 loss 2.16750812531\n",
      "Iteration 4456 ... step 4457 loss 2.34443235397\n",
      "Iteration 4457 ... step 4458 loss 2.34991550446\n",
      "Iteration 4458 ... step 4459 loss 2.20100593567\n",
      "Iteration 4459 ... step 4460 loss 1.96362435818\n",
      "Iteration 4460 ... step 4461 loss 2.17989325523\n",
      "Iteration 4461 ... step 4462 loss 2.33974313736\n",
      "Iteration 4462 ... step 4463 loss 2.25201463699\n",
      "Iteration 4463 ... step 4464 loss 2.34128522873\n",
      "Iteration 4464 ... step 4465 loss 2.1577167511\n",
      "Iteration 4465 ... step 4466 loss 2.0855755806\n",
      "Iteration 4466 ... step 4467 loss 2.25818729401\n",
      "Iteration 4467 ... step 4468 loss 2.24365139008\n",
      "Iteration 4468 ... step 4469 loss 2.30637407303\n",
      "Iteration 4469 ... step 4470 loss 2.23466396332\n",
      "Iteration 4470 ... step 4471 loss 2.12525844574\n",
      "Iteration 4471 ... step 4472 loss 2.40303182602\n",
      "Iteration 4472 ... step 4473 loss 2.14848136902\n",
      "Iteration 4473 ... step 4474 loss 2.42592668533\n",
      "Iteration 4474 ... step 4475 loss 2.12447094917\n",
      "Iteration 4475 ... step 4476 loss 2.27528762817\n",
      "Iteration 4476 ... step 4477 loss 2.37014865875\n",
      "Iteration 4477 ... step 4478 loss 2.49408721924\n",
      "Iteration 4478 ... step 4479 loss 2.31061983109\n",
      "Iteration 4479 ... step 4480 loss 2.29745674133\n",
      "Iteration 4480 ... step 4481 loss 2.45697975159\n",
      "Iteration 4481 ... step 4482 loss 2.21953868866\n",
      "Iteration 4482 ... step 4483 loss 2.28300189972\n",
      "Iteration 4483 ... step 4484 loss 2.26697683334\n",
      "Iteration 4484 ... step 4485 loss 2.39402627945\n",
      "Iteration 4485 ... step 4486 loss 2.20882129669\n",
      "Iteration 4486 ... step 4487 loss 2.36440563202\n",
      "Iteration 4487 ... step 4488 loss 2.31772756577\n",
      "Iteration 4488 ... step 4489 loss 2.39536571503\n",
      "Iteration 4489 ... step 4490 loss 2.30310297012\n",
      "Iteration 4490 ... step 4491 loss 2.15828084946\n",
      "Iteration 4491 ... step 4492 loss 2.07678890228\n",
      "Iteration 4492 ... step 4493 loss 2.180113554\n",
      "Iteration 4493 ... step 4494 loss 2.23445415497\n",
      "Iteration 4494 ... step 4495 loss 2.39264059067\n",
      "Iteration 4495 ... step 4496 loss 2.26168632507\n",
      "Iteration 4496 ... step 4497 loss 2.31798887253\n",
      "Iteration 4497 ... step 4498 loss 2.15938639641\n",
      "Iteration 4498 ... step 4499 loss 2.17468523979\n",
      "Iteration 4499 ... step 4500 loss 2.26137733459\n",
      "Iteration 4500 ... step 4501 loss 2.27524852753\n",
      "Minibatch loss at step 4500: 2.275249\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 73.5%\n",
      "Iteration 4501 ... step 4502 loss 2.31954050064\n",
      "Iteration 4502 ... step 4503 loss 2.23795413971\n",
      "Iteration 4503 ... step 4504 loss 2.4208779335\n",
      "Iteration 4504 ... step 4505 loss 2.28685498238\n",
      "Iteration 4505 ... step 4506 loss 2.25529003143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4506 ... step 4507 loss 2.30861234665\n",
      "Iteration 4507 ... step 4508 loss 2.20932769775\n",
      "Iteration 4508 ... step 4509 loss 2.24362325668\n",
      "Iteration 4509 ... step 4510 loss 2.31160974503\n",
      "Iteration 4510 ... step 4511 loss 2.33646321297\n",
      "Iteration 4511 ... step 4512 loss 2.51183795929\n",
      "Iteration 4512 ... step 4513 loss 2.22676610947\n",
      "Iteration 4513 ... step 4514 loss 2.3626408577\n",
      "Iteration 4514 ... step 4515 loss 2.13108539581\n",
      "Iteration 4515 ... step 4516 loss 2.26925373077\n",
      "Iteration 4516 ... step 4517 loss 2.14818167686\n",
      "Iteration 4517 ... step 4518 loss 2.26777172089\n",
      "Iteration 4518 ... step 4519 loss 2.19441699982\n",
      "Iteration 4519 ... step 4520 loss 2.43984699249\n",
      "Iteration 4520 ... step 4521 loss 2.05193662643\n",
      "Iteration 4521 ... step 4522 loss 2.35463571548\n",
      "Iteration 4522 ... step 4523 loss 2.2155547142\n",
      "Iteration 4523 ... step 4524 loss 2.37968921661\n",
      "Iteration 4524 ... step 4525 loss 2.24558210373\n",
      "Iteration 4525 ... step 4526 loss 2.17898082733\n",
      "Iteration 4526 ... step 4527 loss 2.31341838837\n",
      "Iteration 4527 ... step 4528 loss 2.26498842239\n",
      "Iteration 4528 ... step 4529 loss 2.13072347641\n",
      "Iteration 4529 ... step 4530 loss 2.21836328506\n",
      "Iteration 4530 ... step 4531 loss 2.16464972496\n",
      "Iteration 4531 ... step 4532 loss 2.38537669182\n",
      "Iteration 4532 ... step 4533 loss 2.14641141891\n",
      "Iteration 4533 ... step 4534 loss 2.1997718811\n",
      "Iteration 4534 ... step 4535 loss 2.55463600159\n",
      "Iteration 4535 ... step 4536 loss 2.35159730911\n",
      "Iteration 4536 ... step 4537 loss 2.10680985451\n",
      "Iteration 4537 ... step 4538 loss 2.38012099266\n",
      "Iteration 4538 ... step 4539 loss 2.28008723259\n",
      "Iteration 4539 ... step 4540 loss 2.19565844536\n",
      "Iteration 4540 ... step 4541 loss 2.22633266449\n",
      "Iteration 4541 ... step 4542 loss 2.29768705368\n",
      "Iteration 4542 ... step 4543 loss 2.24343729019\n",
      "Iteration 4543 ... step 4544 loss 2.42334842682\n",
      "Iteration 4544 ... step 4545 loss 2.24803829193\n",
      "Iteration 4545 ... step 4546 loss 2.14126968384\n",
      "Iteration 4546 ... step 4547 loss 2.21886920929\n",
      "Iteration 4547 ... step 4548 loss 2.26036715508\n",
      "Iteration 4548 ... step 4549 loss 2.19778990746\n",
      "Iteration 4549 ... step 4550 loss 2.30598402023\n",
      "Iteration 4550 ... step 4551 loss 2.29784393311\n",
      "Iteration 4551 ... step 4552 loss 2.27978372574\n",
      "Iteration 4552 ... step 4553 loss 2.19753360748\n",
      "Iteration 4553 ... step 4554 loss 2.26287889481\n",
      "Iteration 4554 ... step 4555 loss 2.40713024139\n",
      "Iteration 4555 ... step 4556 loss 2.17722845078\n",
      "Iteration 4556 ... step 4557 loss 2.4187169075\n",
      "Iteration 4557 ... step 4558 loss 2.24106669426\n",
      "Iteration 4558 ... step 4559 loss 2.50278806686\n",
      "Iteration 4559 ... step 4560 loss 2.31070232391\n",
      "Iteration 4560 ... step 4561 loss 2.3176651001\n",
      "Iteration 4561 ... step 4562 loss 2.14247322083\n",
      "Iteration 4562 ... step 4563 loss 2.19215083122\n",
      "Iteration 4563 ... step 4564 loss 2.26553153992\n",
      "Iteration 4564 ... step 4565 loss 2.46501064301\n",
      "Iteration 4565 ... step 4566 loss 2.23463821411\n",
      "Iteration 4566 ... step 4567 loss 2.18800163269\n",
      "Iteration 4567 ... step 4568 loss 2.18851876259\n",
      "Iteration 4568 ... step 4569 loss 2.16099834442\n",
      "Iteration 4569 ... step 4570 loss 2.35918283463\n",
      "Iteration 4570 ... step 4571 loss 2.23966026306\n",
      "Iteration 4571 ... step 4572 loss 1.97758984566\n",
      "Iteration 4572 ... step 4573 loss 2.15190672874\n",
      "Iteration 4573 ... step 4574 loss 2.27801513672\n",
      "Iteration 4574 ... step 4575 loss 2.39920902252\n",
      "Iteration 4575 ... step 4576 loss 1.96557080746\n",
      "Iteration 4576 ... step 4577 loss 2.29988718033\n",
      "Iteration 4577 ... step 4578 loss 2.17620611191\n",
      "Iteration 4578 ... step 4579 loss 2.40566730499\n",
      "Iteration 4579 ... step 4580 loss 2.20021247864\n",
      "Iteration 4580 ... step 4581 loss 2.16155385971\n",
      "Iteration 4581 ... step 4582 loss 2.30551862717\n",
      "Iteration 4582 ... step 4583 loss 2.14102172852\n",
      "Iteration 4583 ... step 4584 loss 2.39148569107\n",
      "Iteration 4584 ... step 4585 loss 2.39655685425\n",
      "Iteration 4585 ... step 4586 loss 2.04816150665\n",
      "Iteration 4586 ... step 4587 loss 2.15194177628\n",
      "Iteration 4587 ... step 4588 loss 2.364153862\n",
      "Iteration 4588 ... step 4589 loss 2.14895057678\n",
      "Iteration 4589 ... step 4590 loss 2.22627592087\n",
      "Iteration 4590 ... step 4591 loss 2.16119432449\n",
      "Iteration 4591 ... step 4592 loss 2.13259077072\n",
      "Iteration 4592 ... step 4593 loss 2.2286605835\n",
      "Iteration 4593 ... step 4594 loss 2.35882496834\n",
      "Iteration 4594 ... step 4595 loss 2.24897670746\n",
      "Iteration 4595 ... step 4596 loss 2.236546278\n",
      "Iteration 4596 ... step 4597 loss 2.37593388557\n",
      "Iteration 4597 ... step 4598 loss 2.14703202248\n",
      "Iteration 4598 ... step 4599 loss 2.19131708145\n",
      "Iteration 4599 ... step 4600 loss 2.23508548737\n",
      "Iteration 4600 ... step 4601 loss 2.13805770874\n",
      "Iteration 4601 ... step 4602 loss 2.20884084702\n",
      "Iteration 4602 ... step 4603 loss 2.1330909729\n",
      "Iteration 4603 ... step 4604 loss 2.43369340897\n",
      "Iteration 4604 ... step 4605 loss 2.19536876678\n",
      "Iteration 4605 ... step 4606 loss 2.14689278603\n",
      "Iteration 4606 ... step 4607 loss 2.26785326004\n",
      "Iteration 4607 ... step 4608 loss 2.39123558998\n",
      "Iteration 4608 ... step 4609 loss 2.38790941238\n",
      "Iteration 4609 ... step 4610 loss 2.18182754517\n",
      "Iteration 4610 ... step 4611 loss 2.32211589813\n",
      "Iteration 4611 ... step 4612 loss 2.25554227829\n",
      "Iteration 4612 ... step 4613 loss 2.20069956779\n",
      "Iteration 4613 ... step 4614 loss 2.22148370743\n",
      "Iteration 4614 ... step 4615 loss 2.18037581444\n",
      "Iteration 4615 ... step 4616 loss 2.2345199585\n",
      "Iteration 4616 ... step 4617 loss 2.24666404724\n",
      "Iteration 4617 ... step 4618 loss 2.14792203903\n",
      "Iteration 4618 ... step 4619 loss 2.13144874573\n",
      "Iteration 4619 ... step 4620 loss 2.12826299667\n",
      "Iteration 4620 ... step 4621 loss 2.27871227264\n",
      "Iteration 4621 ... step 4622 loss 2.28475546837\n",
      "Iteration 4622 ... step 4623 loss 2.22235298157\n",
      "Iteration 4623 ... step 4624 loss 2.2591843605\n",
      "Iteration 4624 ... step 4625 loss 2.19184684753\n",
      "Iteration 4625 ... step 4626 loss 2.26118683815\n",
      "Iteration 4626 ... step 4627 loss 2.47083616257\n",
      "Iteration 4627 ... step 4628 loss 2.20448350906\n",
      "Iteration 4628 ... step 4629 loss 2.36666965485\n",
      "Iteration 4629 ... step 4630 loss 2.23587226868\n",
      "Iteration 4630 ... step 4631 loss 2.23324108124\n",
      "Iteration 4631 ... step 4632 loss 2.24363803864\n",
      "Iteration 4632 ... step 4633 loss 2.23409557343\n",
      "Iteration 4633 ... step 4634 loss 2.22299742699\n",
      "Iteration 4634 ... step 4635 loss 2.27057886124\n",
      "Iteration 4635 ... step 4636 loss 2.20805311203\n",
      "Iteration 4636 ... step 4637 loss 2.2817196846\n",
      "Iteration 4637 ... step 4638 loss 2.15861654282\n",
      "Iteration 4638 ... step 4639 loss 2.23116707802\n",
      "Iteration 4639 ... step 4640 loss 2.38024806976\n",
      "Iteration 4640 ... step 4641 loss 2.31232643127\n",
      "Iteration 4641 ... step 4642 loss 2.20717954636\n",
      "Iteration 4642 ... step 4643 loss 2.52246618271\n",
      "Iteration 4643 ... step 4644 loss 2.15963697433\n",
      "Iteration 4644 ... step 4645 loss 2.08960151672\n",
      "Iteration 4645 ... step 4646 loss 2.32480049133\n",
      "Iteration 4646 ... step 4647 loss 2.28646564484\n",
      "Iteration 4647 ... step 4648 loss 2.27514600754\n",
      "Iteration 4648 ... step 4649 loss 2.08803033829\n",
      "Iteration 4649 ... step 4650 loss 2.17349934578\n",
      "Iteration 4650 ... step 4651 loss 2.25321030617\n",
      "Iteration 4651 ... step 4652 loss 2.33310985565\n",
      "Iteration 4652 ... step 4653 loss 2.14498090744\n",
      "Iteration 4653 ... step 4654 loss 2.2612361908\n",
      "Iteration 4654 ... step 4655 loss 2.14194989204\n",
      "Iteration 4655 ... step 4656 loss 2.1069214344\n",
      "Iteration 4656 ... step 4657 loss 2.39280605316\n",
      "Iteration 4657 ... step 4658 loss 2.38070106506\n",
      "Iteration 4658 ... step 4659 loss 2.16267824173\n",
      "Iteration 4659 ... step 4660 loss 2.1551194191\n",
      "Iteration 4660 ... step 4661 loss 2.23797249794\n",
      "Iteration 4661 ... step 4662 loss 2.21148300171\n",
      "Iteration 4662 ... step 4663 loss 2.29133605957\n",
      "Iteration 4663 ... step 4664 loss 2.07738232613\n",
      "Iteration 4664 ... step 4665 loss 2.21608400345\n",
      "Iteration 4665 ... step 4666 loss 2.31173944473\n",
      "Iteration 4666 ... step 4667 loss 2.2257809639\n",
      "Iteration 4667 ... step 4668 loss 2.42943763733\n",
      "Iteration 4668 ... step 4669 loss 2.32270717621\n",
      "Iteration 4669 ... step 4670 loss 2.60683584213\n",
      "Iteration 4670 ... step 4671 loss 2.25074005127\n",
      "Iteration 4671 ... step 4672 loss 2.31547999382\n",
      "Iteration 4672 ... step 4673 loss 2.19419193268\n",
      "Iteration 4673 ... step 4674 loss 2.26761007309\n",
      "Iteration 4674 ... step 4675 loss 2.33415746689\n",
      "Iteration 4675 ... step 4676 loss 2.30183744431\n",
      "Iteration 4676 ... step 4677 loss 2.31638979912\n",
      "Iteration 4677 ... step 4678 loss 2.28018522263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4678 ... step 4679 loss 2.4354801178\n",
      "Iteration 4679 ... step 4680 loss 2.12939214706\n",
      "Iteration 4680 ... step 4681 loss 2.24769949913\n",
      "Iteration 4681 ... step 4682 loss 2.20419549942\n",
      "Iteration 4682 ... step 4683 loss 2.11875200272\n",
      "Iteration 4683 ... step 4684 loss 2.25779294968\n",
      "Iteration 4684 ... step 4685 loss 2.16524648666\n",
      "Iteration 4685 ... step 4686 loss 2.4493894577\n",
      "Iteration 4686 ... step 4687 loss 2.32692337036\n",
      "Iteration 4687 ... step 4688 loss 2.26749753952\n",
      "Iteration 4688 ... step 4689 loss 2.4220495224\n",
      "Iteration 4689 ... step 4690 loss 2.3079764843\n",
      "Iteration 4690 ... step 4691 loss 2.21507763863\n",
      "Iteration 4691 ... step 4692 loss 2.20037150383\n",
      "Iteration 4692 ... step 4693 loss 2.3014755249\n",
      "Iteration 4693 ... step 4694 loss 2.18905544281\n",
      "Iteration 4694 ... step 4695 loss 2.4322977066\n",
      "Iteration 4695 ... step 4696 loss 2.13825154305\n",
      "Iteration 4696 ... step 4697 loss 2.14553833008\n",
      "Iteration 4697 ... step 4698 loss 2.30180740356\n",
      "Iteration 4698 ... step 4699 loss 2.17816662788\n",
      "Iteration 4699 ... step 4700 loss 2.25036168098\n",
      "Iteration 4700 ... step 4701 loss 2.28234696388\n",
      "Iteration 4701 ... step 4702 loss 2.2796497345\n",
      "Iteration 4702 ... step 4703 loss 2.22539567947\n",
      "Iteration 4703 ... step 4704 loss 2.19584274292\n",
      "Iteration 4704 ... step 4705 loss 2.36456394196\n",
      "Iteration 4705 ... step 4706 loss 2.38156175613\n",
      "Iteration 4706 ... step 4707 loss 2.29321670532\n",
      "Iteration 4707 ... step 4708 loss 2.19176006317\n",
      "Iteration 4708 ... step 4709 loss 2.25623297691\n",
      "Iteration 4709 ... step 4710 loss 2.14303159714\n",
      "Iteration 4710 ... step 4711 loss 2.09269952774\n",
      "Iteration 4711 ... step 4712 loss 2.07050204277\n",
      "Iteration 4712 ... step 4713 loss 2.22240829468\n",
      "Iteration 4713 ... step 4714 loss 2.43777799606\n",
      "Iteration 4714 ... step 4715 loss 2.30851054192\n",
      "Iteration 4715 ... step 4716 loss 2.27913188934\n",
      "Iteration 4716 ... step 4717 loss 2.30212402344\n",
      "Iteration 4717 ... step 4718 loss 2.45360279083\n",
      "Iteration 4718 ... step 4719 loss 2.06444191933\n",
      "Iteration 4719 ... step 4720 loss 2.2884979248\n",
      "Iteration 4720 ... step 4721 loss 2.34246397018\n",
      "Iteration 4721 ... step 4722 loss 2.35787463188\n",
      "Iteration 4722 ... step 4723 loss 2.15810418129\n",
      "Iteration 4723 ... step 4724 loss 2.21275186539\n",
      "Iteration 4724 ... step 4725 loss 2.30020308495\n",
      "Iteration 4725 ... step 4726 loss 2.10715937614\n",
      "Iteration 4726 ... step 4727 loss 2.23468399048\n",
      "Iteration 4727 ... step 4728 loss 2.27637338638\n",
      "Iteration 4728 ... step 4729 loss 2.20955133438\n",
      "Iteration 4729 ... step 4730 loss 2.56567764282\n",
      "Iteration 4730 ... step 4731 loss 2.16009140015\n",
      "Iteration 4731 ... step 4732 loss 2.16770911217\n",
      "Iteration 4732 ... step 4733 loss 2.28918886185\n",
      "Iteration 4733 ... step 4734 loss 2.30336475372\n",
      "Iteration 4734 ... step 4735 loss 2.17837691307\n",
      "Iteration 4735 ... step 4736 loss 2.32665634155\n",
      "Iteration 4736 ... step 4737 loss 2.36016511917\n",
      "Iteration 4737 ... step 4738 loss 2.19260215759\n",
      "Iteration 4738 ... step 4739 loss 2.00798368454\n",
      "Iteration 4739 ... step 4740 loss 2.47029018402\n",
      "Iteration 4740 ... step 4741 loss 2.33773994446\n",
      "Iteration 4741 ... step 4742 loss 2.20722341537\n",
      "Iteration 4742 ... step 4743 loss 2.28301811218\n",
      "Iteration 4743 ... step 4744 loss 2.2215051651\n",
      "Iteration 4744 ... step 4745 loss 2.23895597458\n",
      "Iteration 4745 ... step 4746 loss 2.3402338028\n",
      "Iteration 4746 ... step 4747 loss 2.269354105\n",
      "Iteration 4747 ... step 4748 loss 2.30474567413\n",
      "Iteration 4748 ... step 4749 loss 2.32488775253\n",
      "Iteration 4749 ... step 4750 loss 2.22558259964\n",
      "Iteration 4750 ... step 4751 loss 2.24989318848\n",
      "Iteration 4751 ... step 4752 loss 2.32563924789\n",
      "Iteration 4752 ... step 4753 loss 2.20586490631\n",
      "Iteration 4753 ... step 4754 loss 2.2450761795\n",
      "Iteration 4754 ... step 4755 loss 2.05929517746\n",
      "Iteration 4755 ... step 4756 loss 2.27117681503\n",
      "Iteration 4756 ... step 4757 loss 2.24947309494\n",
      "Iteration 4757 ... step 4758 loss 2.12372493744\n",
      "Iteration 4758 ... step 4759 loss 2.41056489944\n",
      "Iteration 4759 ... step 4760 loss 2.26133584976\n",
      "Iteration 4760 ... step 4761 loss 2.23040723801\n",
      "Iteration 4761 ... step 4762 loss 2.28997278214\n",
      "Iteration 4762 ... step 4763 loss 2.28354287148\n",
      "Iteration 4763 ... step 4764 loss 2.39375829697\n",
      "Iteration 4764 ... step 4765 loss 2.28083896637\n",
      "Iteration 4765 ... step 4766 loss 2.30438661575\n",
      "Iteration 4766 ... step 4767 loss 2.08133077621\n",
      "Iteration 4767 ... step 4768 loss 2.14618086815\n",
      "Iteration 4768 ... step 4769 loss 2.28905057907\n",
      "Iteration 4769 ... step 4770 loss 2.16530275345\n",
      "Iteration 4770 ... step 4771 loss 2.25733542442\n",
      "Iteration 4771 ... step 4772 loss 2.23291492462\n",
      "Iteration 4772 ... step 4773 loss 2.30685257912\n",
      "Iteration 4773 ... step 4774 loss 2.14874744415\n",
      "Iteration 4774 ... step 4775 loss 2.15347957611\n",
      "Iteration 4775 ... step 4776 loss 2.171687603\n",
      "Iteration 4776 ... step 4777 loss 2.25294160843\n",
      "Iteration 4777 ... step 4778 loss 2.2448182106\n",
      "Iteration 4778 ... step 4779 loss 2.18415641785\n",
      "Iteration 4779 ... step 4780 loss 2.39877033234\n",
      "Iteration 4780 ... step 4781 loss 2.34366369247\n",
      "Iteration 4781 ... step 4782 loss 2.32557201385\n",
      "Iteration 4782 ... step 4783 loss 2.21452665329\n",
      "Iteration 4783 ... step 4784 loss 2.28101158142\n",
      "Iteration 4784 ... step 4785 loss 2.21371269226\n",
      "Iteration 4785 ... step 4786 loss 2.0953605175\n",
      "Iteration 4786 ... step 4787 loss 2.36722826958\n",
      "Iteration 4787 ... step 4788 loss 2.22459363937\n",
      "Iteration 4788 ... step 4789 loss 2.08692717552\n",
      "Iteration 4789 ... step 4790 loss 2.25897741318\n",
      "Iteration 4790 ... step 4791 loss 2.28389930725\n",
      "Iteration 4791 ... step 4792 loss 2.32276248932\n",
      "Iteration 4792 ... step 4793 loss 2.22622680664\n",
      "Iteration 4793 ... step 4794 loss 2.12902140617\n",
      "Iteration 4794 ... step 4795 loss 2.3840212822\n",
      "Iteration 4795 ... step 4796 loss 2.28189897537\n",
      "Iteration 4796 ... step 4797 loss 1.95417284966\n",
      "Iteration 4797 ... step 4798 loss 2.26619386673\n",
      "Iteration 4798 ... step 4799 loss 2.05614447594\n",
      "Iteration 4799 ... step 4800 loss 2.28623342514\n",
      "Iteration 4800 ... step 4801 loss 2.25350952148\n",
      "Iteration 4801 ... step 4802 loss 2.41644573212\n",
      "Iteration 4802 ... step 4803 loss 2.118496418\n",
      "Iteration 4803 ... step 4804 loss 2.1900305748\n",
      "Iteration 4804 ... step 4805 loss 2.41330909729\n",
      "Iteration 4805 ... step 4806 loss 2.17309904099\n",
      "Iteration 4806 ... step 4807 loss 2.59626269341\n",
      "Iteration 4807 ... step 4808 loss 2.24153375626\n",
      "Iteration 4808 ... step 4809 loss 2.16024684906\n",
      "Iteration 4809 ... step 4810 loss 2.14377570152\n",
      "Iteration 4810 ... step 4811 loss 2.22991847992\n",
      "Iteration 4811 ... step 4812 loss 2.29134082794\n",
      "Iteration 4812 ... step 4813 loss 2.10251903534\n",
      "Iteration 4813 ... step 4814 loss 2.32133173943\n",
      "Iteration 4814 ... step 4815 loss 2.36303973198\n",
      "Iteration 4815 ... step 4816 loss 2.48518514633\n",
      "Iteration 4816 ... step 4817 loss 2.25759506226\n",
      "Iteration 4817 ... step 4818 loss 2.25564908981\n",
      "Iteration 4818 ... step 4819 loss 2.24121880531\n",
      "Iteration 4819 ... step 4820 loss 2.28600263596\n",
      "Iteration 4820 ... step 4821 loss 2.25342273712\n",
      "Iteration 4821 ... step 4822 loss 2.32026815414\n",
      "Iteration 4822 ... step 4823 loss 2.45032215118\n",
      "Iteration 4823 ... step 4824 loss 2.11696743965\n",
      "Iteration 4824 ... step 4825 loss 2.376537323\n",
      "Iteration 4825 ... step 4826 loss 2.3927116394\n",
      "Iteration 4826 ... step 4827 loss 2.22126102448\n",
      "Iteration 4827 ... step 4828 loss 2.49496746063\n",
      "Iteration 4828 ... step 4829 loss 2.23842334747\n",
      "Iteration 4829 ... step 4830 loss 2.26735496521\n",
      "Iteration 4830 ... step 4831 loss 2.29920291901\n",
      "Iteration 4831 ... step 4832 loss 2.19922327995\n",
      "Iteration 4832 ... step 4833 loss 2.16028881073\n",
      "Iteration 4833 ... step 4834 loss 2.2916636467\n",
      "Iteration 4834 ... step 4835 loss 2.35500049591\n",
      "Iteration 4835 ... step 4836 loss 2.30804634094\n",
      "Iteration 4836 ... step 4837 loss 2.17733120918\n",
      "Iteration 4837 ... step 4838 loss 2.19968271255\n",
      "Iteration 4838 ... step 4839 loss 2.35279273987\n",
      "Iteration 4839 ... step 4840 loss 2.08035612106\n",
      "Iteration 4840 ... step 4841 loss 2.35396337509\n",
      "Iteration 4841 ... step 4842 loss 2.3326830864\n",
      "Iteration 4842 ... step 4843 loss 2.12941789627\n",
      "Iteration 4843 ... step 4844 loss 2.41454482079\n",
      "Iteration 4844 ... step 4845 loss 2.171646595\n",
      "Iteration 4845 ... step 4846 loss 2.06276464462\n",
      "Iteration 4846 ... step 4847 loss 2.21998548508\n",
      "Iteration 4847 ... step 4848 loss 2.15072441101\n",
      "Iteration 4848 ... step 4849 loss 2.28099489212\n",
      "Iteration 4849 ... step 4850 loss 2.25605916977\n",
      "Iteration 4850 ... step 4851 loss 2.03749084473\n",
      "Iteration 4851 ... step 4852 loss 2.09196090698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4852 ... step 4853 loss 2.03123307228\n",
      "Iteration 4853 ... step 4854 loss 1.96685433388\n",
      "Iteration 4854 ... step 4855 loss 2.34213972092\n",
      "Iteration 4855 ... step 4856 loss 2.09168863297\n",
      "Iteration 4856 ... step 4857 loss 2.06905031204\n",
      "Iteration 4857 ... step 4858 loss 2.30054092407\n",
      "Iteration 4858 ... step 4859 loss 2.28727626801\n",
      "Iteration 4859 ... step 4860 loss 2.20412755013\n",
      "Iteration 4860 ... step 4861 loss 2.09535217285\n",
      "Iteration 4861 ... step 4862 loss 2.16923737526\n",
      "Iteration 4862 ... step 4863 loss 2.21418333054\n",
      "Iteration 4863 ... step 4864 loss 2.35749006271\n",
      "Iteration 4864 ... step 4865 loss 2.30201244354\n",
      "Iteration 4865 ... step 4866 loss 2.42438077927\n",
      "Iteration 4866 ... step 4867 loss 2.14304304123\n",
      "Iteration 4867 ... step 4868 loss 2.12098550797\n",
      "Iteration 4868 ... step 4869 loss 2.23908662796\n",
      "Iteration 4869 ... step 4870 loss 2.08671069145\n",
      "Iteration 4870 ... step 4871 loss 2.30378246307\n",
      "Iteration 4871 ... step 4872 loss 2.23178005219\n",
      "Iteration 4872 ... step 4873 loss 2.27612304688\n",
      "Iteration 4873 ... step 4874 loss 2.093708992\n",
      "Iteration 4874 ... step 4875 loss 2.25119447708\n",
      "Iteration 4875 ... step 4876 loss 2.30928564072\n",
      "Iteration 4876 ... step 4877 loss 2.23302268982\n",
      "Iteration 4877 ... step 4878 loss 2.32988119125\n",
      "Iteration 4878 ... step 4879 loss 2.16572260857\n",
      "Iteration 4879 ... step 4880 loss 2.21035528183\n",
      "Iteration 4880 ... step 4881 loss 2.12860155106\n",
      "Iteration 4881 ... step 4882 loss 2.18986701965\n",
      "Iteration 4882 ... step 4883 loss 2.20200824738\n",
      "Iteration 4883 ... step 4884 loss 2.13820075989\n",
      "Iteration 4884 ... step 4885 loss 2.01879000664\n",
      "Iteration 4885 ... step 4886 loss 2.09967899323\n",
      "Iteration 4886 ... step 4887 loss 2.295835495\n",
      "Iteration 4887 ... step 4888 loss 2.09534335136\n",
      "Iteration 4888 ... step 4889 loss 2.08127713203\n",
      "Iteration 4889 ... step 4890 loss 2.13314151764\n",
      "Iteration 4890 ... step 4891 loss 2.07333445549\n",
      "Iteration 4891 ... step 4892 loss 2.28164458275\n",
      "Iteration 4892 ... step 4893 loss 2.25822210312\n",
      "Iteration 4893 ... step 4894 loss 2.21649742126\n",
      "Iteration 4894 ... step 4895 loss 2.20783472061\n",
      "Iteration 4895 ... step 4896 loss 2.24190950394\n",
      "Iteration 4896 ... step 4897 loss 2.20221710205\n",
      "Iteration 4897 ... step 4898 loss 2.19755482674\n",
      "Iteration 4898 ... step 4899 loss 2.34058380127\n",
      "Iteration 4899 ... step 4900 loss 2.21263122559\n",
      "Iteration 4900 ... step 4901 loss 2.33427524567\n",
      "Iteration 4901 ... step 4902 loss 2.385617733\n",
      "Iteration 4902 ... step 4903 loss 2.25588488579\n",
      "Iteration 4903 ... step 4904 loss 2.23237848282\n",
      "Iteration 4904 ... step 4905 loss 2.19831037521\n",
      "Iteration 4905 ... step 4906 loss 2.21260261536\n",
      "Iteration 4906 ... step 4907 loss 2.40445566177\n",
      "Iteration 4907 ... step 4908 loss 2.09319353104\n",
      "Iteration 4908 ... step 4909 loss 2.28570985794\n",
      "Iteration 4909 ... step 4910 loss 2.23129701614\n",
      "Iteration 4910 ... step 4911 loss 2.21996879578\n",
      "Iteration 4911 ... step 4912 loss 2.2073597908\n",
      "Iteration 4912 ... step 4913 loss 2.39901208878\n",
      "Iteration 4913 ... step 4914 loss 2.19699335098\n",
      "Iteration 4914 ... step 4915 loss 2.32288002968\n",
      "Iteration 4915 ... step 4916 loss 2.19696593285\n",
      "Iteration 4916 ... step 4917 loss 2.04891610146\n",
      "Iteration 4917 ... step 4918 loss 2.21990776062\n",
      "Iteration 4918 ... step 4919 loss 2.31453967094\n",
      "Iteration 4919 ... step 4920 loss 2.16901373863\n",
      "Iteration 4920 ... step 4921 loss 2.11583757401\n",
      "Iteration 4921 ... step 4922 loss 2.66402864456\n",
      "Iteration 4922 ... step 4923 loss 2.1502892971\n",
      "Iteration 4923 ... step 4924 loss 2.01878833771\n",
      "Iteration 4924 ... step 4925 loss 2.09591603279\n",
      "Iteration 4925 ... step 4926 loss 2.18511748314\n",
      "Iteration 4926 ... step 4927 loss 2.18023443222\n",
      "Iteration 4927 ... step 4928 loss 2.15934491158\n",
      "Iteration 4928 ... step 4929 loss 2.44022989273\n",
      "Iteration 4929 ... step 4930 loss 2.20820999146\n",
      "Iteration 4930 ... step 4931 loss 2.26115703583\n",
      "Iteration 4931 ... step 4932 loss 2.20516490936\n",
      "Iteration 4932 ... step 4933 loss 2.30888462067\n",
      "Iteration 4933 ... step 4934 loss 2.40208339691\n",
      "Iteration 4934 ... step 4935 loss 2.24504947662\n",
      "Iteration 4935 ... step 4936 loss 2.45345115662\n",
      "Iteration 4936 ... step 4937 loss 2.15034341812\n",
      "Iteration 4937 ... step 4938 loss 2.51941204071\n",
      "Iteration 4938 ... step 4939 loss 2.54995012283\n",
      "Iteration 4939 ... step 4940 loss 2.26570558548\n",
      "Iteration 4940 ... step 4941 loss 2.39925050735\n",
      "Iteration 4941 ... step 4942 loss 2.25777482986\n",
      "Iteration 4942 ... step 4943 loss 2.26827192307\n",
      "Iteration 4943 ... step 4944 loss 2.02193164825\n",
      "Iteration 4944 ... step 4945 loss 2.1631872654\n",
      "Iteration 4945 ... step 4946 loss 2.12267112732\n",
      "Iteration 4946 ... step 4947 loss 2.1678621769\n",
      "Iteration 4947 ... step 4948 loss 1.97673177719\n",
      "Iteration 4948 ... step 4949 loss 2.25433301926\n",
      "Iteration 4949 ... step 4950 loss 2.28994560242\n",
      "Iteration 4950 ... step 4951 loss 2.2750954628\n",
      "Iteration 4951 ... step 4952 loss 2.43483161926\n",
      "Iteration 4952 ... step 4953 loss 2.30020093918\n",
      "Iteration 4953 ... step 4954 loss 2.20248699188\n",
      "Iteration 4954 ... step 4955 loss 2.53199958801\n",
      "Iteration 4955 ... step 4956 loss 2.33460950851\n",
      "Iteration 4956 ... step 4957 loss 2.07530117035\n",
      "Iteration 4957 ... step 4958 loss 2.26267194748\n",
      "Iteration 4958 ... step 4959 loss 2.16248655319\n",
      "Iteration 4959 ... step 4960 loss 2.11434531212\n",
      "Iteration 4960 ... step 4961 loss 2.27436208725\n",
      "Iteration 4961 ... step 4962 loss 2.04994869232\n",
      "Iteration 4962 ... step 4963 loss 2.12163066864\n",
      "Iteration 4963 ... step 4964 loss 2.24025201797\n",
      "Iteration 4964 ... step 4965 loss 2.36804008484\n",
      "Iteration 4965 ... step 4966 loss 1.9479085207\n",
      "Iteration 4966 ... step 4967 loss 2.0817732811\n",
      "Iteration 4967 ... step 4968 loss 2.24538111687\n",
      "Iteration 4968 ... step 4969 loss 2.12540125847\n",
      "Iteration 4969 ... step 4970 loss 2.13496232033\n",
      "Iteration 4970 ... step 4971 loss 2.14750909805\n",
      "Iteration 4971 ... step 4972 loss 2.28569746017\n",
      "Iteration 4972 ... step 4973 loss 2.45043897629\n",
      "Iteration 4973 ... step 4974 loss 2.16932439804\n",
      "Iteration 4974 ... step 4975 loss 2.25509119034\n",
      "Iteration 4975 ... step 4976 loss 2.24607276917\n",
      "Iteration 4976 ... step 4977 loss 2.19657278061\n",
      "Iteration 4977 ... step 4978 loss 2.26926374435\n",
      "Iteration 4978 ... step 4979 loss 2.06492877007\n",
      "Iteration 4979 ... step 4980 loss 2.41391992569\n",
      "Iteration 4980 ... step 4981 loss 2.37946200371\n",
      "Iteration 4981 ... step 4982 loss 2.32765626907\n",
      "Iteration 4982 ... step 4983 loss 2.28851699829\n",
      "Iteration 4983 ... step 4984 loss 2.11314058304\n",
      "Iteration 4984 ... step 4985 loss 2.10557079315\n",
      "Iteration 4985 ... step 4986 loss 2.22018098831\n",
      "Iteration 4986 ... step 4987 loss 2.2222070694\n",
      "Iteration 4987 ... step 4988 loss 2.31153488159\n",
      "Iteration 4988 ... step 4989 loss 2.37736558914\n",
      "Iteration 4989 ... step 4990 loss 2.18632030487\n",
      "Iteration 4990 ... step 4991 loss 2.13302278519\n",
      "Iteration 4991 ... step 4992 loss 2.16587162018\n",
      "Iteration 4992 ... step 4993 loss 2.38263845444\n",
      "Iteration 4993 ... step 4994 loss 2.08879733086\n",
      "Iteration 4994 ... step 4995 loss 2.35327887535\n",
      "Iteration 4995 ... step 4996 loss 2.13478469849\n",
      "Iteration 4996 ... step 4997 loss 2.26597738266\n",
      "Iteration 4997 ... step 4998 loss 2.19648647308\n",
      "Iteration 4998 ... step 4999 loss 2.15722346306\n",
      "Iteration 4999 ... step 5000 loss 2.32742738724\n",
      "Iteration 5000 ... step 5001 loss 2.31291532516\n",
      "Minibatch loss at step 5000: 2.312915\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 73.5%\n",
      "Test accuracy: 81.1%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8facd89d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "learn_rate = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    print(\"Iteration {} ... step {} loss {}\".format(step, global_step.eval() , l))\n",
    "        \n",
    "    #Monitoring the Model\n",
    "    losses.append(l)\n",
    "    learn_rate.append(learning_rate.eval()) #TODO: Get learning rate from model.\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps), losses)\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "#line_x_range = (-4, 6)\n",
    "ax2.plot(range(0, num_steps), learn_rate)\n",
    "ax2.set_ylabel(\"Learning Rate\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

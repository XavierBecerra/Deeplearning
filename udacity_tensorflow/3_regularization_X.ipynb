{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 28, 28) (438047,)\n",
      "Validation set (15586, 28, 28) (15586,)\n",
      "Test set (13645, 28, 28) (13645,)\n"
     ]
    }
   ],
   "source": [
    "#pickle_file = 'notMNIST.pickle'\n",
    "pickle_file = 'notMNIST_noDupNorOvlp.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (438047, 784) (438047, 10)\n",
      "Validation set (15586, 784) (15586, 10)\n",
      "Test set (13645, 784) (13645, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1: \n",
    "Logistic regression with SGD\n",
    "\n",
    "Set-up Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "_BETA_REGUL = 5e-4 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  #The question here is: Do we also want to take into account biases fro regularization?\n",
    "  #\"applying weight decay to the bias units usually makes only a small difference to the final network\"\n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regularization = (tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases) )\n",
    "  #l2_regularization = tf.nn.l2_loss(weights)\n",
    "  loss += _BETA_REGUL * l2_regularization\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.104467\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 15.0%\n",
      "Minibatch loss at step 500: 2.347745\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 1000: 1.452825\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 1500: 1.589714\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 2000: 1.175524\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 2500: 0.860617\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 3000: 0.580042\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 78.3%\n",
      "Test accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to assignment 2 (Logistic regression SGD at iteration 3000:\n",
    "\n",
    "*Ass 2 no regu: Minibatch loss at step 3000: 0.578054\n",
    "Minibatch accuracy: 86.7%\n",
    "Validation accuracy: 76.1%\n",
    "Test accuracy: 83.6%*\n",
    "\n",
    "*Last L2 Regul only weights beta 5e-4: Minibatch loss at step 3000: 0.580042\n",
    "Minibatch accuracy: 89.1%\n",
    "Validation accuracy: 78.3%\n",
    "Test accuracy: 85.7%*\n",
    "\n",
    "\n",
    "\n",
    "##### Model 2:  \n",
    "1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "hidden1_nodes = 1024\n",
    "\n",
    "_BETA_REGUL = 5e-4 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h1': tf.Variable( tf.truncated_normal([image_size * image_size, hidden1_nodes]) ),\n",
    "    'out': tf.Variable( tf.truncated_normal([hidden1_nodes, num_labels]) )\n",
    "  }\n",
    "  biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_nodes])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "  \n",
    "  # Create neural network model:\n",
    "  # Hidden fully connected layer with 256 neurons.\n",
    "  layer_1 = tf.add(tf.matmul(tf_train_dataset, weights['h1']) , biases['b1'])\n",
    "  #Output layer applying relu to hiden layer\n",
    "  logits_out = tf.matmul( tf.nn.relu(layer_1), weights['out']) + biases['out']\n",
    "  #Define loss\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_out))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regularization = (tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(biases['b1'])\n",
    "                      + tf.nn.l2_loss(weights['out']) + tf.nn.l2_loss(biases['out']))\n",
    "  #l2_regularization = tf.nn.l2_loss(weights)\n",
    "  loss += _BETA_REGUL * l2_regularization\n",
    "\n",
    "    ##Needed to evaluate test and validation datasets\n",
    "  test_layer_1 = tf.matmul(tf_test_dataset, weights['h1']) + biases['b1']\n",
    "  test_logits_out = tf.matmul( tf.nn.relu(test_layer_1), weights['out']) + biases['out']\n",
    "    \n",
    "  valid_layer_1 = tf.add(tf.matmul(tf_valid_dataset, weights['h1']) , biases['b1'])\n",
    "  valid_logits_out = tf.matmul( tf.nn.relu(valid_layer_1), weights['out']) + biases['out']\n",
    "  \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_out)\n",
    "  valid_prediction = tf.nn.softmax(valid_logits_out)\n",
    "  test_prediction = tf.nn.softmax(test_logits_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Run the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 535.250305\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 25.0%\n",
      "Minibatch loss at step 500: 128.788483\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 1000: 95.687538\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1500: 73.943321\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 2000: 58.023598\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.3%\n",
      "Test accuracy: 85.6%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff25b722fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    losses.append(l)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(range(0, num_steps), losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to assignment 2 (Logistic regression SGD at iteration 3000:\n",
    "\n",
    "*Ass 2 NN: Minibatch loss at step 5000: 4.587700\n",
    "Minibatch accuracy: 78.1%\n",
    "Validation accuracy: 79.6%\n",
    "Test accuracy: 86.6%*\n",
    "\n",
    "*Last L2 Regul only weights and bias beta 5e-4: Minibatch loss at step 5000: 12.971046\n",
    "Minibatch accuracy: 82.0%\n",
    "Validation accuracy: 84.4%\n",
    "Test accuracy: 90.5%\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a few batches we will try by only running a small subset of batches. So batch size is the same, but we will only train with a small subset of batches (no going through the whole training dataset.\n",
    "\n",
    "We will implement this by making the offset a random number between 0 and 10. Only first 10 batches on the data training set are used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s run the model Using the Setup of previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 527.359375\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 36.1%\n",
      "Minibatch loss at step 500: 127.361511\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 1000: 95.485588\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1500: 74.362022\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 2000: 57.911427\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 2500: 45.099991\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 3000: 35.122944\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 3500: 27.352951\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 4000: 21.302094\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 4500: 16.589764\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 5000: 12.920185\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Test accuracy: 83.3%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34ea38c3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_steps = 5001\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the first 10 subsetes of 128 entries in dataset.\n",
    "    offset = batch_size * np.random.choice(10)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    losses.append(l)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(range(0, num_steps), losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, The Minibatch accuracy is 100% (Actually this is like training with a smaller dataset).\n",
    "However, Validation and test accuracy are lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nn.dropout is only applied in train_layer_1. \n",
    "\n",
    "We will apply a probability of 0.5 to keep/dropout an activation (values that go from one layer to the next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-dcf2a00a5606>:36: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "hidden1_nodes = 1024\n",
    "\n",
    "_BETA_REGUL = 5e-4 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h1': tf.Variable( tf.truncated_normal([image_size * image_size, hidden1_nodes]) ),\n",
    "    'out': tf.Variable( tf.truncated_normal([hidden1_nodes, num_labels]) )\n",
    "  }\n",
    "  biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_nodes])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "  \n",
    "  # Create neural network model:\n",
    "  # Hidden fully connected layer with 256 neurons.\n",
    "  train_layer_1 = tf.add(tf.matmul(tf_train_dataset, weights['h1']) , biases['b1'])\n",
    "  train_layer_1 = tf.nn.dropout(train_layer_1, 0.5)\n",
    "  #Output layer applying relu to hiden layer\n",
    "  logits_out = tf.matmul( tf.nn.relu(train_layer_1), weights['out']) + biases['out']\n",
    "  #Define loss\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_out))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regularization = (tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(biases['b1'])\n",
    "                      + tf.nn.l2_loss(weights['out']) + tf.nn.l2_loss(biases['out']))\n",
    "  #l2_regularization = tf.nn.l2_loss(weights)\n",
    "  loss += _BETA_REGUL * l2_regularization\n",
    "\n",
    "    ##Needed to evaluate test and validation datasets\n",
    "  test_layer_1 = tf.matmul(tf_test_dataset, weights['h1']) + biases['b1']\n",
    "  test_logits_out = tf.matmul( tf.nn.relu(test_layer_1), weights['out']) + biases['out']\n",
    "    \n",
    "  valid_layer_1 = tf.add(tf.matmul(tf_valid_dataset, weights['h1']) , biases['b1'])\n",
    "  valid_logits_out = tf.matmul( tf.nn.relu(valid_layer_1), weights['out']) + biases['out']\n",
    "  \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_out)\n",
    "  valid_prediction = tf.nn.softmax(valid_logits_out)\n",
    "  test_prediction = tf.nn.softmax(test_logits_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 647.199707\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 30.8%\n",
      "Minibatch loss at step 500: 141.209442\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1000: 98.687836\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 1500: 78.025322\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 2000: 62.269276\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 2500: 45.906364\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 3000: 34.820324\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 3500: 27.925144\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 4000: 21.562328\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 4500: 16.728735\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 5000: 13.389703\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.4%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3487d42350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    losses.append(l)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(range(0, num_steps), losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout technique seems to improve the model. Test accuray is now aroun 89% when for Problem 1 was around 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Set Up the Model\n",
    "###### First Approach \n",
    "Multilayer Neural Network with 4 hidden layers of same number of neurons each. \n",
    "\n",
    "Weights initialized with deviation square root of (2 / input activations to layer)\n",
    "\n",
    "RELU function for each layer activations.\n",
    "\n",
    "Dropout technique at each layer activations. Only during training.\n",
    "\n",
    "L2 Regulariation including also biases.\n",
    "\n",
    "Dynamic learning rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "num_steps = 250001\n",
    "\n",
    "MODEL_NAME = \"madel_layer1024_step250K\"\n",
    "\n",
    "hidden_layer_nodes = {\n",
    "    'h1' : 1024,\n",
    "    'h2' : 1024,\n",
    "    'h3' : 1024,\n",
    "    'h4' : 512 \n",
    "}\n",
    "\n",
    "_BETA_REGUL = 5e-5 #5e-4 Based on 3_mnist_from_scratch.ipynb. May change.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = {\n",
    "    'h1': tf.Variable( tf.truncated_normal([image_size * image_size, hidden_layer_nodes['h1'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / (image_size * image_size))) ),\n",
    "    'h2': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h1'], hidden_layer_nodes['h2'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / hidden_layer_nodes['h1'] )) ),\n",
    "    'h3': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h2'], hidden_layer_nodes['h3'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / hidden_layer_nodes['h2'] )) ),\n",
    "    'h4': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h3'], hidden_layer_nodes['h4'] ] , \n",
    "                                           stddev = np.sqrt(2.0 / hidden_layer_nodes['h3'] )) ),\n",
    "    'out': tf.Variable( tf.truncated_normal([hidden_layer_nodes['h4'], num_labels] , \n",
    "                                            stddev= np.sqrt(2.0 / hidden_layer_nodes['h4'] )) ),\n",
    "  }\n",
    "  biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden_layer_nodes['h1'] ])),\n",
    "    'b2': tf.Variable(tf.zeros([hidden_layer_nodes['h2'] ])),\n",
    "    'b3': tf.Variable(tf.zeros([hidden_layer_nodes['h3'] ])),\n",
    "    'b4': tf.Variable(tf.zeros([hidden_layer_nodes['h4'] ])),\n",
    "    'out': tf.Variable(tf.zeros([num_labels]))\n",
    "  }\n",
    "  \n",
    "  # Create neural network model:\n",
    "  train_layer_1 = tf.add(tf.matmul(tf_train_dataset, weights['h1']) , biases['b1'])\n",
    "  train_layer_1 = tf.nn.dropout(tf.nn.relu(train_layer_1), 0.5)\n",
    "    \n",
    "  train_layer_2 = tf.add(tf.matmul(train_layer_1, weights['h2']) , biases['b2'])\n",
    "  train_layer_2 = tf.nn.dropout(tf.nn.relu(train_layer_2), 0.7)\n",
    "    \n",
    "  train_layer_3 = tf.add(tf.matmul(train_layer_2, weights['h3']) , biases['b3'])\n",
    "  train_layer_3 = tf.nn.dropout(tf.nn.relu(train_layer_3), 0.7)\n",
    "    \n",
    "  train_layer_4 = tf.add(tf.matmul(train_layer_3, weights['h4']) , biases['b4'])\n",
    "  train_layer_4 = tf.nn.dropout(tf.nn.relu(train_layer_4), 0.9)\n",
    "    \n",
    "  logits_out = tf.add(tf.matmul( train_layer_4, weights['out']) , biases['out'])\n",
    "  #Define loss\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits_out))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  l2_regul = (tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(biases['b1'])\n",
    "                       + tf.nn.l2_loss(weights['h2']) + tf.nn.l2_loss(biases['b2'])\n",
    "                       + tf.nn.l2_loss(weights['h3']) + tf.nn.l2_loss(biases['b3'])\n",
    "                       + tf.nn.l2_loss(weights['h4']) + tf.nn.l2_loss(biases['b4'])\n",
    "                       + tf.nn.l2_loss(weights['out']) + tf.nn.l2_loss(biases['out']) )\n",
    "    \n",
    "  loss += _BETA_REGUL * l2_regul\n",
    "  \n",
    "  # Optimizer. Exponential decay of learning rate\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, # Base learning rate. Start big to learn fast at the beginning\n",
    "                                             global_step * batch_size,\n",
    "                                             num_steps,  # tf_train_labels.shape[0],# Decay step.\n",
    "                                             0.97,                # Decay rate.\n",
    "                                             staircase=True)\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  #optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    "  \n",
    "\n",
    "  ##Needed to evaluate test and validation datasets\n",
    "  test_layer_1 = tf.add(tf.matmul(tf_test_dataset, weights['h1']) , biases['b1'])\n",
    "  test_layer_2 = tf.add(tf.matmul(tf.nn.relu(test_layer_1), weights['h2']) , biases['b2'])\n",
    "  test_layer_3 = tf.add(tf.matmul(tf.nn.relu(test_layer_2), weights['h3']) , biases['b3'])\n",
    "  test_layer_4 = tf.add(tf.matmul(tf.nn.relu(test_layer_3), weights['h4']) , biases['b4'])\n",
    "  test_logits_out = tf.matmul( tf.nn.relu(test_layer_4), weights['out']) + biases['out']\n",
    "    \n",
    "  valid_layer_1 = tf.add(tf.matmul(tf_valid_dataset, weights['h1']) , biases['b1'])\n",
    "  valid_layer_2 = tf.add(tf.matmul(tf.nn.relu(valid_layer_1), weights['h2']) , biases['b2'])\n",
    "  valid_layer_3 = tf.add(tf.matmul(tf.nn.relu(valid_layer_2), weights['h3']) , biases['b3'])\n",
    "  valid_layer_4 = tf.add(tf.matmul(tf.nn.relu(valid_layer_3), weights['h4']) , biases['b4'])\n",
    "  valid_logits_out = tf.matmul( tf.nn.relu(valid_layer_4), weights['out']) + biases['out']\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_out)\n",
    "  valid_prediction = tf.nn.softmax(valid_logits_out)\n",
    "  test_prediction = tf.nn.softmax(test_logits_out)\n",
    "\n",
    "  #Saver in order to save a fully trained model.\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s run the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.641196\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 12.6%\n",
      "Minibatch loss at step 500: 0.641326\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1000: 0.583873\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1500: 0.719728\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2000: 0.640030\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2500: 0.641124\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3000: 0.492197\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3500: 0.606868\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4000: 0.571099\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4500: 0.589196\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 5000: 0.609385\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 5500: 0.475113\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 6000: 0.534709\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6500: 0.637693\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 7000: 0.458664\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 7500: 0.738527\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 8000: 0.466669\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8500: 0.448500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 9000: 0.403580\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 9500: 0.362576\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 10000: 0.605604\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 10500: 0.555195\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 11000: 0.463858\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 11500: 0.578486\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 12000: 0.552973\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 12500: 0.331444\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13000: 0.559635\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13500: 0.385778\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14000: 0.572367\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 14500: 0.639118\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 15000: 0.497656\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 15500: 0.445401\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 16000: 0.367167\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 16500: 0.420728\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 17000: 0.331145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 17500: 0.462322\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 18000: 0.455250\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 18500: 0.593577\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 19000: 0.453770\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 19500: 0.531973\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 20000: 0.412714\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 20500: 0.356006\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 21000: 0.516719\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 21500: 0.379665\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 22000: 0.439360\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 22500: 0.572221\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 23000: 0.432828\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 23500: 0.486902\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 24000: 0.520574\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 24500: 0.494747\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 25000: 0.614655\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 25500: 0.584166\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 26000: 0.507164\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 26500: 0.397923\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 27000: 0.431028\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 27500: 0.564681\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 28000: 0.430921\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 28500: 0.577732\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 29000: 0.409647\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 29500: 0.464585\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 30000: 0.423141\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 30500: 0.360914\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 31000: 0.424445\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 31500: 0.492336\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 32000: 0.410479\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 32500: 0.403783\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 33000: 0.329094\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 33500: 0.384754\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 34000: 0.240321\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 34500: 0.447893\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 35000: 0.474392\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 35500: 0.385682\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 36000: 0.318347\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 36500: 0.404092\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 37000: 0.255556\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 37500: 0.350873\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 38000: 0.375610\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 38500: 0.504422\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 39000: 0.421403\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 39500: 0.391859\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 40000: 0.312250\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 40500: 0.344648\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 41000: 0.408716\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 41500: 0.338636\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 42000: 0.360644\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 42500: 0.297961\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 43000: 0.373663\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 43500: 0.421973\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 44000: 0.266370\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 44500: 0.407862\n",
      "Minibatch accuracy: 88.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 45000: 0.404746\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 45500: 0.328594\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 46000: 0.379374\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 46500: 0.392736\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 47000: 0.297079\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 47500: 0.289522\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 48000: 0.453555\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 48500: 0.459894\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 49000: 0.438117\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 49500: 0.416990\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 50000: 0.356813\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 50500: 0.431863\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 51000: 0.405729\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 51500: 0.536464\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 52000: 0.359731\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 52500: 0.393961\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 53000: 0.389766\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 53500: 0.220238\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 54000: 0.361652\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 54500: 0.413221\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 55000: 0.414778\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 55500: 0.455355\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 56000: 0.233994\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 56500: 0.371489\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 57000: 0.386655\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 57500: 0.328510\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 58000: 0.410469\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 58500: 0.283688\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 59000: 0.367675\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 59500: 0.408649\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 60000: 0.267652\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 60500: 0.341907\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 61000: 0.307820\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 61500: 0.363069\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 62000: 0.402000\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 62500: 0.291006\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 63000: 0.343958\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 63500: 0.294873\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 64000: 0.421181\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 64500: 0.297125\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 65000: 0.351725\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 65500: 0.292071\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 66000: 0.386220\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 66500: 0.382759\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 67000: 0.339082\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 67500: 0.363287\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 68000: 0.247385\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 68500: 0.259703\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 69000: 0.460989\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 69500: 0.446982\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 70000: 0.217703\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 70500: 0.447751\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 71000: 0.423705\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 71500: 0.295026\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 72000: 0.264082\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 72500: 0.319098\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 73000: 0.269910\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 73500: 0.339779\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 74000: 0.345555\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 74500: 0.162427\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 75000: 0.239534\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 75500: 0.285204\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 76000: 0.398892\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 76500: 0.284558\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 77000: 0.436318\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 77500: 0.411321\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 78000: 0.334852\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 78500: 0.219438\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 79000: 0.344952\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 79500: 0.445643\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 80000: 0.345806\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 80500: 0.379299\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 81000: 0.310623\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 81500: 0.328540\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 82000: 0.470567\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 82500: 0.290763\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 83000: 0.298612\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 83500: 0.320577\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 84000: 0.209083\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 84500: 0.348296\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 85000: 0.361946\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 85500: 0.354333\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 86000: 0.379015\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 86500: 0.292083\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 87000: 0.235892\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 87500: 0.292354\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 88000: 0.325158\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 88500: 0.404989\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 89000: 0.253267\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 89500: 0.434537\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 90000: 0.226084\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 90500: 0.280629\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 91000: 0.282822\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 91500: 0.346846\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 92000: 0.358275\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 92500: 0.449130\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 93000: 0.258279\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 93500: 0.262607\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 94000: 0.501349\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 94500: 0.252963\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 95000: 0.387697\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 95500: 0.317255\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 96000: 0.308341\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 96500: 0.414679\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 97000: 0.399096\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 97500: 0.356599\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 98000: 0.174189\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 98500: 0.271597\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 99000: 0.233836\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 99500: 0.306437\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 100000: 0.296341\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 100500: 0.331669\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 101000: 0.235889\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 101500: 0.332132\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 102000: 0.259505\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 102500: 0.333790\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 103000: 0.200895\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 103500: 0.297520\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 104000: 0.255679\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 104500: 0.215328\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 105000: 0.306467\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 105500: 0.294719\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 106000: 0.191145\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 106500: 0.336002\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 107000: 0.282467\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 107500: 0.392537\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 108000: 0.336718\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 108500: 0.313663\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 109000: 0.159019\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 109500: 0.350349\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 110000: 0.307822\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 110500: 0.248222\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 111000: 0.254415\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 111500: 0.347667\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 112000: 0.272762\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 112500: 0.236245\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 113000: 0.381166\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 113500: 0.299544\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 114000: 0.224797\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 114500: 0.334796\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 115000: 0.357161\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 115500: 0.301944\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 116000: 0.258119\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 116500: 0.287664\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 117000: 0.375794\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 117500: 0.243792\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 118000: 0.240234\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 118500: 0.298212\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 119000: 0.315850\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 119500: 0.304062\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 120000: 0.376961\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 120500: 0.271795\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 121000: 0.291484\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 121500: 0.295223\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 122000: 0.241843\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 122500: 0.308875\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 123000: 0.211609\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 123500: 0.276187\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 124000: 0.289625\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 124500: 0.286170\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 125000: 0.187440\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 125500: 0.227605\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 126000: 0.185817\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 126500: 0.371465\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 127000: 0.391618\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 127500: 0.215453\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 128000: 0.235342\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 128500: 0.346981\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 129000: 0.269288\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 129500: 0.224658\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 130000: 0.212088\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 130500: 0.291562\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 131000: 0.246842\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 131500: 0.247703\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 132000: 0.272511\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 132500: 0.218129\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 133000: 0.282529\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 133500: 0.343333\n",
      "Minibatch accuracy: 88.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 134000: 0.280651\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 134500: 0.237110\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 135000: 0.209707\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 135500: 0.394420\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 136000: 0.285088\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 136500: 0.254633\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 137000: 0.322916\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 137500: 0.322835\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 138000: 0.232762\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 138500: 0.214555\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 139000: 0.183989\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 139500: 0.350959\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 140000: 0.199637\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 140500: 0.170006\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 141000: 0.334544\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 141500: 0.266098\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 142000: 0.185592\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 142500: 0.258502\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 143000: 0.300963\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 143500: 0.227309\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 144000: 0.256929\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 144500: 0.203399\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 145000: 0.390879\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 145500: 0.151586\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 146000: 0.342553\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 146500: 0.287936\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 147000: 0.227718\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 147500: 0.197375\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 148000: 0.311694\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.0%\n",
      "Minibatch loss at step 148500: 0.239677\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 149000: 0.277170\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 149500: 0.244564\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 150000: 0.239006\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 150500: 0.224327\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 151000: 0.185702\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 151500: 0.302228\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 152000: 0.288334\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 152500: 0.254223\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 153000: 0.223493\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 153500: 0.297484\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 154000: 0.317916\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 154500: 0.183556\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 155000: 0.261865\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 155500: 0.195425\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 156000: 0.240460\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 156500: 0.135489\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 157000: 0.236863\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 157500: 0.187264\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 158000: 0.351797\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 158500: 0.264768\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 159000: 0.236256\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 159500: 0.250105\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 160000: 0.248307\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 160500: 0.203412\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 161000: 0.297732\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 161500: 0.245746\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 162000: 0.268251\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 162500: 0.183920\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 163000: 0.255482\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 163500: 0.260485\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 164000: 0.266660\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 164500: 0.259004\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 165000: 0.187206\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 165500: 0.241865\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 166000: 0.271281\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 166500: 0.253114\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 167000: 0.288177\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 167500: 0.319140\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 168000: 0.259879\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 168500: 0.364633\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 169000: 0.262797\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 169500: 0.247723\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 170000: 0.251356\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 170500: 0.276886\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 171000: 0.277098\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 171500: 0.294838\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 172000: 0.251624\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 172500: 0.252216\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 173000: 0.294358\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 173500: 0.297483\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 174000: 0.213983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 174500: 0.289032\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 175000: 0.295879\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 175500: 0.198905\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 176000: 0.251596\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 176500: 0.245383\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 177000: 0.228017\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 177500: 0.197502\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 178000: 0.303861\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 178500: 0.355348\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 179000: 0.243735\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 179500: 0.214143\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 180000: 0.278917\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 180500: 0.166619\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 181000: 0.237442\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 181500: 0.193069\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 182000: 0.236562\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 182500: 0.304757\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 183000: 0.293904\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 183500: 0.257898\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 184000: 0.211447\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 184500: 0.202427\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 185000: 0.199891\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 185500: 0.396397\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 186000: 0.274652\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 186500: 0.280897\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 187000: 0.269462\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 187500: 0.275000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 188000: 0.245087\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 188500: 0.323500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 189000: 0.324935\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 189500: 0.254242\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 190000: 0.258222\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 190500: 0.211166\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 191000: 0.240498\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 191500: 0.214009\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 192000: 0.223891\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 192500: 0.261343\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 193000: 0.247757\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 193500: 0.215862\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 194000: 0.269272\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 194500: 0.082434\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 195000: 0.178059\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 195500: 0.148678\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 196000: 0.213816\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 196500: 0.176951\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 197000: 0.283190\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 197500: 0.190505\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 198000: 0.266310\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 198500: 0.318884\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 199000: 0.240974\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 199500: 0.237994\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 200000: 0.276617\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 200500: 0.164607\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 201000: 0.219642\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 201500: 0.363068\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 202000: 0.207023\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 202500: 0.180004\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 203000: 0.220294\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 203500: 0.221481\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 204000: 0.184027\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 204500: 0.186744\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 205000: 0.187305\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 205500: 0.185919\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 206000: 0.228259\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 206500: 0.209334\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 207000: 0.168656\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 207500: 0.253804\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 208000: 0.256015\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 208500: 0.174410\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 209000: 0.134701\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 209500: 0.233685\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 210000: 0.204051\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 210500: 0.192608\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 211000: 0.153195\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 211500: 0.162845\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 212000: 0.276772\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 212500: 0.199958\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 213000: 0.191760\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 213500: 0.287857\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 214000: 0.253684\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 214500: 0.128456\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 215000: 0.193375\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 215500: 0.189687\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 216000: 0.144635\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 216500: 0.241230\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 217000: 0.189464\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 217500: 0.202461\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 218000: 0.221743\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 218500: 0.229169\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 219000: 0.263946\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 219500: 0.274197\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 220000: 0.196546\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 220500: 0.245452\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 221000: 0.247408\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 221500: 0.171810\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 222000: 0.186097\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 222500: 0.242966\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 223000: 0.198406\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 223500: 0.185774\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 224000: 0.229499\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 224500: 0.256938\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 225000: 0.195450\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 225500: 0.169025\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 226000: 0.289858\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 226500: 0.299234\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 227000: 0.142107\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 227500: 0.180734\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 228000: 0.199641\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 228500: 0.262685\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 229000: 0.201292\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 229500: 0.250162\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 230000: 0.194486\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 230500: 0.196347\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 231000: 0.374942\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 231500: 0.228063\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 232000: 0.278564\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 232500: 0.261218\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 233000: 0.181421\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 233500: 0.179249\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 234000: 0.180645\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 234500: 0.230641\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 235000: 0.239437\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 235500: 0.177478\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 236000: 0.162550\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 236500: 0.231427\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 237000: 0.143653\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 237500: 0.235919\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 238000: 0.241073\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 238500: 0.274910\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 239000: 0.181200\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 239500: 0.263836\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 240000: 0.183151\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 240500: 0.297152\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 241000: 0.217674\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 241500: 0.216512\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 242000: 0.196157\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 242500: 0.230318\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 243000: 0.153953\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 243500: 0.283228\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 244000: 0.278422\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 244500: 0.236066\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 245000: 0.175834\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 245500: 0.233694\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 246000: 0.130912\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 246500: 0.207999\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 247000: 0.269230\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 247500: 0.270265\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Minibatch loss at step 248000: 0.274966\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 248500: 0.192799\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 249000: 0.190500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 249500: 0.224777\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 250000: 0.339918\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Test accuracy: 97.0%\n",
      "Model saved in path: ./saved_models/madel_layer1024_step250K.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f870dcf23d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "learn_rate = []\n",
    "valid_acc = []\n",
    "valid_acc_x = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print(\"Iteration {} ... step {} loss {}\".format(step, global_step.eval() , l))\n",
    "        \n",
    "    #Monitoring the Model\n",
    "    losses.append(l)\n",
    "    learn_rate.append(learning_rate.eval()) \n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      val_acc = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      print(\"Validation accuracy: %.1f%%\" % val_acc)\n",
    "      valid_acc.append(val_acc)\n",
    "      valid_acc_x.append(step)\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "  \n",
    "  save_path = saver.save(session, \"./saved_models/\" + MODEL_NAME + \".ckpt\")\n",
    "  print(\"Model saved in path: {}\".format(save_path))\n",
    "\n",
    "\n",
    "# Show the loss over time.\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "fig.set_size_inches(10, 4)\n",
    "ax1.plot(range(0, num_steps), losses)\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Training steps\")\n",
    "#line_x_range = (-4, 6)\n",
    "ax2.plot(range(0, num_steps), learn_rate)\n",
    "ax2.set_ylabel(\"Learning Rate\")\n",
    "ax2.set_xlabel(\"Training steps\")\n",
    "ax3.plot(valid_acc_x, valid_acc)\n",
    "ax3.set_ylabel(\"Validation Accuracy [%]\")\n",
    "ax3.set_xlabel(\"Training step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With layer size 1024 - 512 -521 - 256  and steps 250001 -> Test accuracy 96.8% !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
